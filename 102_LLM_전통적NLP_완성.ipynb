{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbilsJ+//g7c+Y2lID+5Qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/ktcloud_genai/blob/main/102_LLM_%EC%A0%84%ED%86%B5%EC%A0%81NLP_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QuickTour** : ì „í†µì ì¸ NLP"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡**ì½”ë“œ ë‚´ìš©**\n",
        "    - 2000ë…„ëŒ€ ì´ì „ì˜ ìì—°ì–´ ì²˜ë¦¬(NLP) ê³¼ì • ì´í•´ë¥¼ ë•ëŠ” ì˜ˆì œë“¤ê³  êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1980~90ë…„ëŒ€: í†µê³„ì  NLP, í™•ë¥ ë¡ ì  ì ‘ê·¼ë²•\n",
        "\n",
        "- **í•µì‹¬ ì„¤ëª…**\n",
        "    - 1980~90ë…„ëŒ€ ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ê·œì¹™ ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **í†µê³„ì  ë°©ë²•**ìœ¼ë¡œ ì „í™˜ë¨\n",
        "    - **ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ì˜ ë“±ì¥** â†’ **í™•ë¥  ëª¨ë¸ ê¸°ë°˜ ì–¸ì–´ ì²˜ë¦¬** ê°€ëŠ¥\n",
        "    - ëŒ€í‘œ ê¸°ë²•: n-ê·¸ë¨ ì–¸ì–´ ëª¨ë¸, HMM(Hidden Markov Model) ê¸°ë°˜ í’ˆì‚¬ íƒœê¹…\n",
        "    - ***n-ê·¸ë¨ ì–¸ì–´ ëª¨ë¸** :\n",
        "        - ì•ì˜ n-1 ë‹¨ì–´ë¥¼ ì´ìš©í•´ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì–¸ì–´ ëª¨ë¸\n",
        "    - ***HMM(Hidden Markov Model)** :\n",
        "        - ê´€ì°°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°(ì˜ˆ: ë‹¨ì–´)ë¥¼ í†µí•´ ê·¸ ë’¤ì— ìˆ¨ê²¨ì§„ ìƒíƒœ(ì˜ˆ: í’ˆì‚¬)ë¥¼ ì¶”ë¡ í•˜ëŠ” í†µê³„ì  ëª¨ë¸\n",
        "        - ìˆ¨ê²¨ì§„ ìƒíƒœ(ì˜ˆ: í’ˆì‚¬) ì˜ ì „ì´ í™•ë¥ ê³¼ ê´€ì¸¡ ë‹¨ì–´ì˜ ë°œìƒ í™•ë¥ ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ë§í•˜ëŠ” í™•ë¥  ê¸°ë°˜ ëª¨ë¸\n",
        "        - HMMì€ **ì „ì´í™•ë¥ (P(tagáµ¢|tagáµ¢â‚‹â‚))** +  **ë°œìƒí™•ë¥ (P(word|tag))** ì„ ì´ìš©í•´ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì‹œí€€ìŠ¤ë¥¼ ì„ íƒ"
      ],
      "metadata": {
        "id": "vjJh4IMODeSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : n-ê·¸ë¨ ì–¸ì–´ëª¨ë¸**"
      ],
      "metadata": {
        "id": "_MAPofpjVe4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) n-ê·¸ë¨ ì–¸ì–´ëª¨ë¸ ì˜ˆì œ (Brown Corpus í™œìš©)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams, ConditionalFreqDist\n",
        "\n",
        "nltk.download('brown')\n",
        "words = brown.words(categories='news')\n",
        "bi_grams = list(bigrams(words))\n",
        "cfd = ConditionalFreqDist(bi_grams)\n",
        "\n",
        "print(\"ì˜ˆì¸¡ ë‹¨ì–´ (ì• ë‹¨ì–´='the'):\", cfd[\"the\"].most_common(5))"
      ],
      "metadata": {
        "id": "I2QbPiIzOAIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : HMM í’ˆì‚¬ íƒœê¹…**"
      ],
      "metadata": {
        "id": "NRS-qz2SVpj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì˜ì–´ ì²˜ë¦¬**"
      ],
      "metadata": {
        "id": "mzWe2XKhw996"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìì—°ì–´ ì²˜ë¦¬(NLP)ë¥¼ ìœ„í•œ ë¹ ë¥´ê³  ì‹¤ìš©ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "!pip install spacy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5O6U_12WUNwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ì–´(en) ì–¸ì–´ ëª¨ë¸: en_core_web_sm ë‹¤ìš´ë¡œë“œ --> ì„¸ì…˜ ë‹¤ì‹œì‹œì‘\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "G6_V2MS4URU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# en_core_web_sm ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Google is a major technology company located in California.\"\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "doc = nlp(text)\n",
        "\n",
        "# ê° ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ ì¶œë ¥\n",
        "print(\"{:<15} {:<10} {:<10}\".format(\"ë‹¨ì–´\", \"í’ˆì‚¬\", \"ê°œì²´ëª…\"))\n",
        "print(\"-\" * 35)\n",
        "for token in doc:\n",
        "    entity = token.ent_type_ if token.ent_type_ else \"ì—†ìŒ\"\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {entity:<10}\")"
      ],
      "metadata": {
        "id": "M9Wb2SBAvnY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í•œêµ­ì–´ ì²˜ë¦¬**\n",
        "- í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° : https://www.bigkinds.or.kr/v2/analysis/featureExtraction.do#"
      ],
      "metadata": {
        "id": "97FuOvzJw3P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# konlpy ì„¤ì¹˜\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "ihcQO2rVw3lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n",
        "okt = Okt()\n",
        "\n",
        "text = \"ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤.\"\n",
        "\n",
        "# í˜•íƒœì†Œ ë¶„ì„\n",
        "morphs = okt.morphs(text)\n",
        "print(f\"í˜•íƒœì†Œ ë¶„ì„: {morphs}\")\n",
        "\n",
        "# í’ˆì‚¬ íƒœê¹…\n",
        "pos_tags = okt.pos(text, stem=True)\n",
        "print(f\"í’ˆì‚¬ íƒœê¹…: {pos_tags}\")"
      ],
      "metadata": {
        "id": "YWkboWB0xIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Spacy(êµ­ì œ í‘œì¤€) + Konlpy(í•œêµ­ì–´ì— íŠ¹í™”ëœ í‘œì¤€) --> ì†Œí†µë˜ë„ë¡ ì ì ˆí•œ ì²˜ë¦¬ê°€ ì ìš©ë¨"
      ],
      "metadata": {
        "id": "nR2HaTSaxdFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from konlpy.tag import Okt\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Konlpyì˜ íƒœê·¸ë¥¼ Spacyì˜ UD íƒœê·¸ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "# ëª¨ë“  íƒœê·¸ë¥¼ ë§¤í•‘í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì˜ˆì œì— ì‚¬ìš©ëœ ê²ƒ ìœ„ì£¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "TAG_MAP = {\n",
        "    'Noun': 'NOUN',\n",
        "    'Josa': 'ADP',  # í•œêµ­ì–´ì˜ 'ì¡°ì‚¬'ëŠ” ì˜ì–´ì˜ ì „ì¹˜ì‚¬ì— ê°€ê¹Œìš´ ì—­í• \n",
        "    'Verb': 'VERB',\n",
        "    'Adjective': 'ADJ',\n",
        "    'Punctuation': 'PUNCT',\n",
        "    'Modifier': 'ADJ', # 'ë§¤ìš°'ì™€ ê°™ì€ ë¶€ì‚¬ëŠ” í˜•ìš©ì‚¬ì— ê°€ê¹ê²Œ ë§¤í•‘\n",
        "}\n",
        "\n",
        "def konlpy_tokenizer(text):\n",
        "    \"\"\"Konlpyë¡œ í† í°í™”í•˜ê³  í’ˆì‚¬ íƒœê·¸ë¥¼ Spacyì— ë§ê²Œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    okt = Okt()\n",
        "    pos_tags = okt.pos(text, stem=True)\n",
        "    words = [t[0] for t in pos_tags]\n",
        "    # Konlpy íƒœê·¸ë¥¼ Spacyì˜ 'tag'ì—, UD íƒœê·¸ë¥¼ 'pos'ì— í• ë‹¹\n",
        "    tags = [t[1] for t in pos_tags]\n",
        "    pos = [TAG_MAP.get(t[1], 'X') for t in pos_tags]  # ë§¤í•‘ë˜ì§€ ì•Šì€ íƒœê·¸ëŠ” 'X'ë¡œ ì²˜ë¦¬\n",
        "    return words, pos, tags\n",
        "\n",
        "# í•œê¸€ ë¬¸ì¥ ì˜ˆì œ\n",
        "text = \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "# Konlpyë¡œ í† í°í™” ë° í’ˆì‚¬ íƒœê¹…\n",
        "words, pos, tags = konlpy_tokenizer(text)\n",
        "\n",
        "# Spacyì˜ Doc ê°ì²´ë¡œ ë³€í™˜\n",
        "# ì£¼ì˜: 'pos' ëŒ€ì‹  'tags' ì†ì„±ì— Konlpyì˜ íƒœê·¸ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n",
        "doc = Doc(nlp.vocab, words=words, tags=tags, pos=pos)\n",
        "\n",
        "# Spacy Doc ê°ì²´ë¡œ ë³€í™˜ëœ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
        "print(\"Spacy Doc ê°ì²´ë¡œ ë³€í™˜ëœ ê²°ê³¼:\")\n",
        "print(f\"{'ë‹¨ì–´':<10} {'ì›ë˜ íƒœê·¸ (tag)':<15} {'UD íƒœê·¸ (pos)':<15}\")\n",
        "print(\"-\" * 45)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<10} {token.tag_:<15} {token.pos_:<15}\")"
      ],
      "metadata": {
        "id": "3Tp5IfsVximf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wp95PCZdxdHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) HMM í’ˆì‚¬ íƒœê¹… ì˜ˆì œ (NLTK HMM Tagger)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "test_sentences = [\n",
        "    \"Time flies like an arrow\",\n",
        "    \"Fruit flies like a banana\",\n",
        "    \"Programming is really fun\"\n",
        "]\n",
        "\n",
        "\n",
        "# ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íƒœê±°\n",
        "def simple_rule_based_tagger():\n",
        "    \"\"\"ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ í’ˆì‚¬ íƒœê±°\"\"\"\n",
        "\n",
        "    # ê¸°ë³¸ í’ˆì‚¬ ì‚¬ì „\n",
        "    pos_dict = {\n",
        "        'the': 'DT', 'a': 'DT', 'an': 'DT',\n",
        "        'is': 'VBZ', 'are': 'VBP', 'was': 'VBD', 'were': 'VBD',\n",
        "        'and': 'CC', 'or': 'CC', 'but': 'CC',\n",
        "        'in': 'IN', 'on': 'IN', 'at': 'IN', 'like': 'IN',\n",
        "        'very': 'RB', 'really': 'RB', 'quite': 'RB'\n",
        "    }\n",
        "\n",
        "    def tag_sentence(sentence):\n",
        "        words = sentence.lower().split()\n",
        "        tagged = []\n",
        "\n",
        "        for word in words:\n",
        "            if word in pos_dict:\n",
        "                tag = pos_dict[word]\n",
        "            elif word.endswith('ing'):\n",
        "                tag = 'VBG'  # í˜„ì¬ë¶„ì‚¬\n",
        "            elif word.endswith('ed'):\n",
        "                tag = 'VBD'  # ê³¼ê±°í˜•\n",
        "            elif word.endswith('ly'):\n",
        "                tag = 'RB'   # ë¶€ì‚¬\n",
        "            else:\n",
        "                tag = 'NN'   # ê¸°ë³¸ê°’: ëª…ì‚¬\n",
        "\n",
        "            tagged.append((word, tag))\n",
        "\n",
        "        return tagged\n",
        "\n",
        "    return tag_sentence\n",
        "\n",
        "print(\"\\n=== ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ íƒœê±° ===\")\n",
        "rule_tagger = simple_rule_based_tagger()\n",
        "\n",
        "for sentence in [\"Time flies like an arrow\", \"Programming is really fun\"]:\n",
        "    result = rule_tagger(sentence)\n",
        "    print(f\"ì›ë¬¸: {sentence}\")\n",
        "    print(f\"íƒœê¹…: {result}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "D7YciS3tTq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ : ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ë¡œ n-ê·¸ë¨ ì–¸ì–´ëª¨ë¸ êµ¬ì¶•ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì¥ ìƒì„±\n",
        "- ì´ ì½”ë“œëŠ” **êµ¬í…ë² ë¥´í¬ ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸**ë¥¼ ë‹¤ìš´ë¡œë“œí•´ì„œ (--> Project Gutenbergì—ì„œ ì œê³µí•˜ëŠ” ê³ ì „ ë¬¸í•™ ì „ìì±… ëª¨ìŒ)\n",
        "- ë¹…ê·¸ë¨(bigram) ëª¨ë¸(í™•ë¥ ì  NLP ê¸°ë²•)ì„ ë§Œë“¤ê³ ,\n",
        "- **ì¡°ê±´ë¶€ í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±**í•¨"
      ],
      "metadata": {
        "id": "jQHx8oG7Div4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ERcAduBZ1q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import requests\n",
        "\n",
        "# 1) ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ ì˜ˆì‹œ)\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # ì œì¸ ì˜¤ìŠ¤í‹´ 'Pride and Prejudice(ì˜¤ë§Œê³¼ í¸ê²¬)'\n",
        "text = requests.get(url).text.lower()\n",
        "\n",
        "\n",
        "# 2) í† í°í™” (ë‹¨ì–´ ë‹¨ìœ„)\n",
        "tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
        "#  \"ë‹¨ì–´ ê²½ê³„(\\b)ë¡œ ì‹œì‘í•´ì„œ, í•˜ë‚˜ ì´ìƒì˜ ë‹¨ì–´ ë¬¸ì(\\w+)ê°€ ë‚˜ì˜¤ê³ , ë‹¤ì‹œ ë‹¨ì–´ ê²½ê³„(\\b)ë¡œ ëë‚˜ëŠ” ë¬¸ìì—´ì„ ì°¾ì•„ë¼.\"\n",
        "\n",
        "\n",
        "# 3) n-ê·¸ë¨(ë¹…ê·¸ë¨) ìƒì„±\n",
        "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "\n",
        "# 4) ì¡°ê±´ë¶€ í™•ë¥  ê³„ì‚° (P(w2 | w1))\n",
        "bigram_model = defaultdict(list)\n",
        "for (w1, w2), freq in bigram_counts.items():\n",
        "    bigram_model[w1].append((w2, freq))\n",
        "\n",
        "\n",
        "# 5) í™•ë¥  ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "def generate_text(start_word, length=20):\n",
        "    word = start_word\n",
        "    result = [word]\n",
        "    for _ in range(length):\n",
        "        if word not in bigram_model:\n",
        "            break\n",
        "        candidates, weights = zip(*bigram_model[word])\n",
        "        word = random.choices(candidates, weights=weights, k=1)[0]  # ê°€ì¤‘ì¹˜ ê°’ì´ í° ê²ƒ ì„ íƒ\n",
        "        result.append(word)\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "print(generate_text(\"love\", 30))  # ì§€ì‹œì–´, ë‹¨ì–´ê°¯ìˆ˜\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : ë¹„ì§€ë„ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ í•™ìŠµí•œ HMM ì–¸ì–´ ëª¨ë¸ ì˜ˆì œ**\n",
        "- EM(Expectation-Maximization)\n",
        "    - HMMì˜ ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning) ë°©ì‹ì˜ ëŒ€í‘œì  ì•Œê³ ë¦¬ì¦˜\n",
        "- í¼í”Œë ‰ì‹œí‹°(perplexity : PPL)\n",
        "    - ì–¸ì–´ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ í‰ê°€ ì§€í‘œ -->(í—·ê°ˆë¦¬ëŠ” ì •ë„ë¥¼ ìˆ˜ì¹˜í™”)\n",
        "    - ìˆ˜ì¹˜ê°€ 'ë‚®ì„ìˆ˜ë¡' ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤.\n",
        "    - $PPL(W)=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$\n",
        "- ë¡œê·¸ìš°ë„(Log-Likelihood)\n",
        "    - ì–´ë–¤ ëª¨ë¸ì´ ì£¼ì–´ì§„ ë°ì´í„°(ì—¬ê¸°ì„œëŠ” ë¬¸ì¥)ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ\n",
        "    - 'ìš°ë„(Likelihood)'ëŠ” ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ìƒì„±í•  í™•ë¥ ì„ ì˜ë¯¸í•˜ë©°,\n",
        "    - ì´ ìš°ë„ ê°’ì— ë¡œê·¸(log)ë¥¼ ì·¨í•œ ê²ƒì´ ë°”ë¡œ ë¡œê·¸ìš°ë„\n",
        "    - ì˜ˆ: ì–¸ì–´ ëª¨ë¸ì´ \"I love AI.\"ë¼ëŠ” ë¬¸ì¥ì„ ìƒì„±í•  í™•ë¥ \n",
        "        - ìš°ë„(Likelihood): P(\"IloveAI.\")=P(\"I\")xP(\"love\"|\"I\")xP(\"AI\"|\"Ilove\")\n",
        "        - ë¡œê·¸ìš°ë„(Log-Likelihood): log(P(\"IloveAI.\"))=log(P(\"I\"))+log(P(\"love\"|\"I\"))+log(P(\"AI\"|\"Ilove\"))\n",
        "    -  ë¡œê·¸ ì ìš© ì´ìœ \n",
        "        - ê°’ì´ 0ì´ ë˜ì§€ ì•Šê³  ìŒìˆ˜ë¡œ ë³€í™˜ë˜ì–´ ì •í™•í•œ ê³„ì‚°ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤.\n",
        "        - ë³µì¡í•œ ê³±ì…ˆ ì—°ì‚°ì„ ë‹¨ìˆœí•œ ë§ì…ˆ ì—°ì‚°ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆ\n"
      ],
      "metadata": {
        "id": "QY5G4ozHDlhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì£¼ì˜! ì‹¤í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆë‹¤.\n",
        "# ============================\n",
        "# HMM Language Model (unsupervised, EM)\n",
        "# bigram example:\n",
        "# ============================\n",
        "import re, math, random, requests\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# 0) ë°ì´í„° ì¤€ë¹„ (ë™ì¼ URL)\n",
        "# ----------------------------\n",
        "URL = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "\n",
        "def fetch_text(url=URL):\n",
        "    text = requests.get(url).text\n",
        "    return text\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    # ë‹¨ì–´ + ë¬¸ì¥ë¶€í˜¸(.?!)\n",
        "    toks = re.findall(r\"\\b\\w+\\b|[.!?]\", text.lower())\n",
        "    sents, cur = [], []\n",
        "    for t in toks:\n",
        "        if t in [\".\", \"!\", \"?\"]:\n",
        "            if cur:\n",
        "                sents.append(cur)\n",
        "                cur = []\n",
        "        else:\n",
        "            cur.append(t)\n",
        "    if cur:  # ë§ˆì§€ë§‰ ì”ì—¬ í† í°\n",
        "        sents.append(cur)\n",
        "    return sents  # ë¦¬ìŠ¤íŠ¸(ë¬¸ì¥)ë“¤ì˜ ë¦¬ìŠ¤íŠ¸(ë‹¨ì–´)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) ì–´íœ˜ êµ¬ì„± & í¬ê·€ì–´ ì²˜ë¦¬\n",
        "# ----------------------------\n",
        "def build_vocab(sentences, min_freq=2, max_tokens=None):\n",
        "    # sentences: List[List[str]]\n",
        "    if max_tokens:\n",
        "        # ëŒ€ìš©ëŸ‰ì¼ ë•Œ í•™ìŠµ í† í° ìˆ˜ ì œí•œ(ì˜µì…˜)\n",
        "        flat = [w for s in sentences for w in s][:max_tokens]\n",
        "        # ë¬¸ì¥ë„ ì˜ë¼ì„œ ë§ì¶°ì¤Œ\n",
        "        words_left = set(flat)\n",
        "        trimmed = []\n",
        "        total = 0\n",
        "        for s in sentences:\n",
        "            keep = []\n",
        "            for w in s:\n",
        "                if total >= max_tokens: break\n",
        "                if w in words_left:\n",
        "                    keep.append(w); total += 1\n",
        "            if keep: trimmed.append(keep)\n",
        "            if total >= max_tokens: break\n",
        "        sentences = trimmed\n",
        "\n",
        "    freq = Counter(w for s in sentences for w in s)\n",
        "    vocab = {w for w,c in freq.items() if c >= min_freq}\n",
        "    vocab |= {\"<unk>\"}  # í¬ê·€ì–´ ì¹˜í™˜\n",
        "    word2id = {w:i for i,w in enumerate(sorted(vocab))}\n",
        "    id2word = {i:w for w,i in word2id.items()}\n",
        "    # í¬ê·€ì–´ ì¹˜í™˜\n",
        "    proc = [[w if w in vocab else \"<unk>\" for w in s] for s in sentences]\n",
        "    return proc, word2id, id2word\n",
        "\n",
        "# ----------------------------\n",
        "# 2) ì‹œí€€ìŠ¤ ë³€í™˜\n",
        "# ----------------------------\n",
        "def to_id_sequences(sentences, word2id):\n",
        "    return [np.array([word2id[w] for w in s], dtype=np.int32) for s in sentences if len(s) > 0]\n",
        "\n",
        "# ----------------------------\n",
        "# 3) ìˆ˜ì¹˜ ì•ˆì •í™” ìœ í‹¸\n",
        "# ----------------------------\n",
        "def logsumexp(v):\n",
        "    m = np.max(v)\n",
        "    return m + np.log(np.sum(np.exp(v - m) + 1e-300))\n",
        "\n",
        "# ----------------------------\n",
        "# 4) HMM (ë¹„ì§€ë„, EM)\n",
        "# ----------------------------\n",
        "class HMM:\n",
        "    def __init__(self, n_states, vocab_size, seed=42):\n",
        "        self.K = n_states\n",
        "        self.V = vocab_size\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        # íŒŒë¼ë¯¸í„°ëŠ” log-í™•ë¥ ë¡œ ë³´ê´€\n",
        "        # ì´ˆê¸°ë¶„í¬ pi, ì „ì´ A, ë°©ì¶œ B\n",
        "        pi = rng.random(self.K); pi /= pi.sum()\n",
        "        A = rng.random((self.K, self.K)); A /= A.sum(axis=1, keepdims=True)\n",
        "        B = rng.random((self.K, self.V)); B /= B.sum(axis=1, keepdims=True)\n",
        "\n",
        "        self.log_pi = np.log(pi + 1e-300)\n",
        "        self.log_A  = np.log(A + 1e-300)\n",
        "        self.log_B  = np.log(B + 1e-300)\n",
        "\n",
        "    # Forward-Backward (í•œ ë¬¸ì¥)\n",
        "    def forward(self, seq):\n",
        "        T = len(seq)\n",
        "        alpha = np.full((T, self.K), -np.inf)\n",
        "        # t=0\n",
        "        alpha[0] = self.log_pi + self.log_B[:, seq[0]]\n",
        "        # t>=1\n",
        "        for t in range(1, T):\n",
        "            bt = self.log_B[:, seq[t]]\n",
        "            for k in range(self.K):\n",
        "                alpha[t, k] = bt[k] + logsumexp(alpha[t-1] + self.log_A[:, k])\n",
        "        ll = logsumexp(alpha[-1])\n",
        "        return alpha, ll\n",
        "\n",
        "    def backward(self, seq):\n",
        "        T = len(seq)\n",
        "        beta = np.full((T, self.K), -np.inf)\n",
        "        beta[-1] = 0.0  # log(1)\n",
        "        for t in range(T-2, -1, -1):\n",
        "            bt1 = self.log_B[:, seq[t+1]]\n",
        "            for k in range(self.K):\n",
        "                beta[t, k] = logsumexp(self.log_A[k] + bt1 + beta[t+1])\n",
        "        return beta\n",
        "\n",
        "    def e_step_accumulate(self, seq, exp_pi, exp_A, exp_B):\n",
        "        alpha, ll = self.forward(seq)\n",
        "        beta = self.backward(seq)\n",
        "        T = len(seq)\n",
        "\n",
        "        # gamma (state posterior)\n",
        "        gamma = alpha + beta - ll\n",
        "        gamma = np.exp(gamma)  # (T,K)\n",
        "\n",
        "        # xi (pairwise transition posterior) ëˆ„ì \n",
        "        for t in range(T-1):\n",
        "            # (K,K): alpha[t, j] + log_A[j, k]\n",
        "            s = alpha[t][:, None] + self.log_A\n",
        "            # (K,K): + log_B[k, w_{t+1}]\n",
        "            s = s + self.log_B[:, seq[t+1]][None, :]\n",
        "            # (K,K): + beta[t+1, k]\n",
        "            s = s + beta[t+1][None, :]\n",
        "            # normalize in log-space by subtracting ll\n",
        "            s = s - ll\n",
        "            xi = np.exp(s)                  # (K,K)\n",
        "            exp_A += xi                     # ëˆ„ì \n",
        "\n",
        "        exp_pi += gamma[0]\n",
        "        for t in range(T):\n",
        "            exp_B[:, seq[t]] += gamma[t]\n",
        "\n",
        "        return ll\n",
        "\n",
        "\n",
        "    def m_step(self, exp_pi, exp_A, exp_B, alpha_smooth=1e-2):\n",
        "        # Dirichlet-like í‰í™œí™”\n",
        "        exp_pi = exp_pi + alpha_smooth\n",
        "        exp_A  = exp_A  + alpha_smooth\n",
        "        exp_B  = exp_B  + alpha_smooth\n",
        "\n",
        "        self.log_pi = np.log(exp_pi / exp_pi.sum() + 1e-300)\n",
        "\n",
        "        A = exp_A / exp_A.sum(axis=1, keepdims=True)\n",
        "        B = exp_B / exp_B.sum(axis=1, keepdims=True)\n",
        "\n",
        "        self.log_A = np.log(A + 1e-300)\n",
        "        self.log_B = np.log(B + 1e-300)\n",
        "\n",
        "    def fit(self, sequences, n_iter=5, alpha_smooth=1e-2, verbose=True):\n",
        "        for it in range(1, n_iter+1):\n",
        "            exp_pi = np.zeros(self.K)\n",
        "            exp_A  = np.zeros((self.K, self.K))\n",
        "            exp_B  = np.zeros((self.K, self.V))\n",
        "\n",
        "            total_ll = 0.0\n",
        "            total_T  = 0\n",
        "            for seq in sequences:\n",
        "                ll = self.e_step_accumulate(seq, exp_pi, exp_A, exp_B)\n",
        "                total_ll += ll\n",
        "                total_T  += len(seq)\n",
        "\n",
        "            self.m_step(exp_pi, exp_A, exp_B, alpha_smooth=alpha_smooth)\n",
        "            avg_nll = - total_ll / max(total_T, 1)\n",
        "            if verbose:\n",
        "                print(f\"[EM {it}/{n_iter}] avg NLL per token: {avg_nll:.4f} (perplexity â‰ˆ {math.exp(avg_nll):.2f})\")\n",
        "\n",
        "    # ë¬¸ì¥ ë¡œê·¸ìš°ë„ (forwardì˜ ll)\n",
        "    def sequence_loglik(self, seq):\n",
        "        _, ll = self.forward(seq)\n",
        "        return ll\n",
        "\n",
        "    # ë¬´ì¡°ê±´ ìƒì„±\n",
        "    def sample_unconditional(self, id2word, length=20, seed=0):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # z0 ~ pi, w0 ~ B[z0]\n",
        "        z = rng.choice(self.K, p=np.exp(self.log_pi))\n",
        "        words = []\n",
        "        for t in range(length):\n",
        "            w = rng.choice(len(id2word), p=np.exp(self.log_B[z]))\n",
        "            words.append(id2word[w])\n",
        "            z = rng.choice(self.K, p=np.exp(self.log_A[z]))\n",
        "        return \" \".join(words)\n",
        "\n",
        "    # ì‹œë“œ ë‹¨ì–´ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì²« ìƒíƒœë¥¼ í›„í—˜ì—ì„œ ìƒ˜í”Œ(ì‚¬í›„í™•ë¥ )\n",
        "    def sample_conditioned_on_first_word(self, first_word_id, id2word, length=20, seed=0):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # p(z0 | w0) âˆ pi * B[:, w0]\n",
        "        post0 = np.exp(self.log_pi + self.log_B[:, first_word_id])\n",
        "        post0 = post0 / post0.sum()\n",
        "        z = rng.choice(self.K, p=post0)\n",
        "\n",
        "        words = [id2word[first_word_id]]\n",
        "        for _ in range(length-1):\n",
        "            z = rng.choice(self.K, p=np.exp(self.log_A[z]))\n",
        "            w = rng.choice(len(id2word), p=np.exp(self.log_B[z]))\n",
        "            words.append(id2word[w])\n",
        "        return \" \".join(words)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) ì‹¤í–‰ íŒŒì´í”„ë¼ì¸\n",
        "# ----------------------------\n",
        "def train_hmm_language_model(\n",
        "    n_states=8,             # ìˆ¨ì€ ìƒíƒœ ìˆ˜ (ì£¼ì œ/ë¬¸ë§¥ í´ëŸ¬ìŠ¤í„°ì²˜ëŸ¼ ì‘ë™)\n",
        "    min_freq=3,            # í¬ê·€ì–´ ì„ê³„ì¹˜\n",
        "    max_tokens=200_000,    # ëŒ€ìš©ëŸ‰ ì‹œ ì†ë„/ë©”ëª¨ë¦¬ ì œì–´ (None = ì „ì²´ ì‚¬ìš©)\n",
        "    em_iters=6,            # EM ë°˜ë³µ íšŸìˆ˜\n",
        "    seed=42\n",
        "):\n",
        "    print(\"Downloading text ... (Project Gutenberg #1342)\")\n",
        "    text = fetch_text(URL)\n",
        "    print(\"Sentence tokenizing ...\")\n",
        "    sentences = sentence_tokenize(text)   # List[List[str]]\n",
        "    print(f\"Total sentences: {len(sentences)}\")\n",
        "\n",
        "    print(\"Building vocab & <unk> ...\")\n",
        "    proc_sents, word2id, id2word = build_vocab(sentences, min_freq=min_freq, max_tokens=max_tokens)\n",
        "    sequences = to_id_sequences(proc_sents, word2id)\n",
        "    total_tokens = sum(len(s) for s in sequences)\n",
        "    print(f\"Vocab size: {len(word2id)}, Tokens used: {total_tokens}\")\n",
        "\n",
        "    hmm = HMM(n_states, vocab_size=len(word2id), seed=seed)\n",
        "    print(\"Training HMM with EM ...\")\n",
        "    hmm.fit(sequences, n_iter=em_iters, alpha_smooth=1e-2, verbose=True)\n",
        "\n",
        "    # ìƒíƒœë³„ ìƒìœ„ ë‹¨ì–´(ë°©ì¶œ í™•ë¥  ìƒìœ„)\n",
        "    B = np.exp(hmm.log_B)\n",
        "    for k in range(n_states):\n",
        "        top_ids = np.argsort(-B[k])[:10]\n",
        "        top_words = [id2word[i] for i in top_ids]\n",
        "        print(f\"[State {k}] top words:\", \", \".join(top_words))\n",
        "\n",
        "    return hmm, sequences, word2id, id2word\n",
        "\n",
        "# ----------------------------\n",
        "# 6) ë°ëª¨ ì‹¤í–‰\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    hmm, seqs, word2id, id2word = train_hmm_language_model(\n",
        "        n_states=8, min_freq=3, max_tokens=200_000, em_iters=6, seed=7\n",
        "    )\n",
        "\n",
        "    # ë¬´ì¡°ê±´ ìƒì„±\n",
        "    print(\"\\n=== Unconditional Sample ===\")\n",
        "    print(hmm.sample_unconditional(id2word, length=25, seed=1))\n",
        "\n",
        "\n",
        "    # ì‹œë“œ ë‹¨ì–´ ì¡°ê±´ ìƒì„± (ì˜ˆ: 'love')\n",
        "    seed_word = \"love\" if \"love\" in word2id else \"<unk>\"\n",
        "    print(\"\\n=== Conditioned on first word:\", seed_word, \"===\")\n",
        "    print(hmm.sample_conditioned_on_first_word(word2id[seed_word], id2word, length=25, seed=2))\n",
        "\n",
        "\n",
        "    # ì˜ˆì‹œ ë¬¸ì¥ ë¡œê·¸ìš°ë„(í‰ê°€)\n",
        "    # example = [\"time\",\"flies\",\"like\",\"an\",\"arrow\"]\n",
        "    # example = [w if w in word2id else \"<unk>\" for w in example]\n",
        "    # seq = np.array([word2id[w] for w in example], dtype=np.int32)\n",
        "    # ll = hmm.sequence_loglik(seq)\n",
        "    # avg_nll = - ll / len(seq)\n",
        "    # print(f\"\\n#Example sentence: {' '.join(example)}\")\n",
        "    # print(f\"Log-likelihood: {ll:.3f}, avg NLL/token: {avg_nll:.3f}, perplexity â‰ˆ {math.exp(avg_nll):.2f}\")\n"
      ],
      "metadata": {
        "id": "EQydgo7UBgT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8h129nZci8Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2000ë…„ëŒ€: ê¸°ê³„í•™ìŠµ ê¸°ë°˜ NLP\n"
      ],
      "metadata": {
        "id": "DG1KXzUrMqdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í•µì‹¬ ì„¤ëª…**\n",
        "    - **SVM(Support Vector Machine)ì˜ NLP ì ìš©**\n",
        "        - ë°°ê²½: 1995ë…„ Vapnikì´ ê°œë°œí•œ SVMì´ 2000ë…„ëŒ€ ì´ˆ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ë¶„ì•¼ì—ì„œ ê°ê´‘ë°›ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê³ ì°¨ì› í¬ì†Œ ë²¡í„°ë¡œ í‘œí˜„ë˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì„±ì— SVMì´ ë§¤ìš° ì í•©í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "        - **í•µì‹¬ ì•„ì´ë””ì–´**:\n",
        "            - í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ìœ¼ë¡œ ë§¤í•‘\n",
        "            - ìµœëŒ€ ë§ˆì§„ì„ ê°–ëŠ” ê²°ì • ê²½ê³„ ì°¾ê¸°\n",
        "            - ì»¤ë„ íŠ¸ë¦­ì„ í†µí•œ ë¹„ì„ í˜• ë¶„ë¥˜\n",
        "    - **HMM(Hidden Markov Model)ì˜ ë°œì „**\n",
        "        - ë°°ê²½: 1980ë…„ëŒ€ë¶€í„° ìŒì„± ì¸ì‹ì— ì‚¬ìš©ë˜ë˜ HMMì´ 2000ë…„ëŒ€ì— í’ˆì‚¬ íƒœê¹…, ê°œì²´ëª… ì¸ì‹ ë“±ìœ¼ë¡œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "        - **í•œê³„ì™€ ê·¹ë³µ**:\n",
        "            - ë¬¸ì œì : ê´€ì°° ë…ë¦½ì„± ê°€ì •, ë¼ë²¨ í¸í–¥ ë¬¸ì œ\n",
        "            - í•´ê²°ì±…: CRF(Conditional Random Fields) ê°œë°œë¡œ ì „ì—­ ìµœì í™” ê°€ëŠ¥\n"
      ],
      "metadata": {
        "id": "30NKnVWCdKET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : SVMì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜**\n",
        "\n",
        "- SVM(Support Vector Machine) ëª¨ë¸ê³¼ TF-IDF(Term Frequency-Inverse Document Frequency) ë²¡í„°í™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ\n",
        "- ì´ ì½”ë“œëŠ” ë‹¹ì‹œì˜ ì»´í“¨íŒ… í™˜ê²½ê³¼ ê¸°ìˆ  ìˆ˜ì¤€ì„ ë°˜ì˜í•˜ì—¬, í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ë¶„ë¥˜ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì¤Œ"
      ],
      "metadata": {
        "id": "wIV39svoeCQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SVMTextClassifier2000s:\n",
        "    \"\"\"2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ SVM í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸°: ì„ í˜• ë¶„\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.classifier = None\n",
        "        self.pipeline = None\n",
        "\n",
        "    def create_sample_dataset(self):\n",
        "        \"\"\"2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
        "        # ì‹¤ì œ 2000ë…„ëŒ€ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìŠ¤íƒ€ì¼\n",
        "        news_data = {\n",
        "            'technology': [\n",
        "                \"Apple introduces new iPod with video capabilities\",\n",
        "                \"Google launches Gmail with 1GB storage\",\n",
        "                \"Microsoft releases Windows XP Service Pack 2\",\n",
        "                \"Yahoo acquires Flickr photo sharing service\",\n",
        "                \"Mozilla Firefox browser gains market share\",\n",
        "                \"Intel announces dual-core processor technology\",\n",
        "                \"Sony PlayStation Portable hits the market\",\n",
        "                \"YouTube launches online video platform\",\n",
        "                \"Wikipedia becomes popular reference source\",\n",
        "                \"Broadband internet adoption increases rapidly\"\n",
        "            ],\n",
        "            'politics': [\n",
        "                \"President Bush announces new homeland security measures\",\n",
        "                \"Election results show divided political landscape\",\n",
        "                \"Congress debates healthcare reform legislation\",\n",
        "                \"International summit addresses climate change\",\n",
        "                \"Supreme Court ruling affects civil rights\",\n",
        "                \"Senator proposes new tax reform bill\",\n",
        "                \"Presidential candidate announces campaign strategy\",\n",
        "                \"Foreign policy experts discuss Middle East crisis\",\n",
        "                \"Governor signs education funding legislation\",\n",
        "                \"Political analysts predict election outcomes\"\n",
        "            ],\n",
        "            'sports': [\n",
        "                \"Lakers win NBA championship in overtime thriller\",\n",
        "                \"Olympic Games showcase international competition\",\n",
        "                \"World Cup final attracts global television audience\",\n",
        "                \"Baseball season ends with dramatic playoff series\",\n",
        "                \"Tennis tournament features top-ranked players\",\n",
        "                \"Football team advances to conference championship\",\n",
        "                \"Swimming records broken at international meet\",\n",
        "                \"Golf tournament decided on final hole\",\n",
        "                \"Basketball coach announces retirement plans\",\n",
        "                \"Soccer match ends in penalty shootout\"\n",
        "            ],\n",
        "            'entertainment': [\n",
        "                \"Hollywood movie breaks box office records\",\n",
        "                \"Music industry adapts to digital downloads\",\n",
        "                \"Television series finale draws huge audience\",\n",
        "                \"Celebrity couple announces engagement news\",\n",
        "                \"Film festival showcases independent cinema\",\n",
        "                \"Pop star releases highly anticipated album\",\n",
        "                \"Broadway show receives critical acclaim\",\n",
        "                \"Reality TV show becomes cultural phenomenon\",\n",
        "                \"Movie sequel exceeds original's success\",\n",
        "                \"Entertainment awards ceremony honors achievements\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "        texts = []\n",
        "        labels = []\n",
        "        for category, articles in news_data.items():\n",
        "            texts.extend(articles)\n",
        "            labels.extend([category] * len(articles))\n",
        "\n",
        "        return pd.DataFrame({'text': texts, 'category': labels})\n",
        "\n",
        "    def preprocess_text_2000s_style(self, texts):\n",
        "        \"\"\"2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
        "        # ë‹¹ì‹œì—ëŠ” ê°„ë‹¨í•œ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰\n",
        "        processed = []\n",
        "        for text in texts:\n",
        "            # ì†Œë¬¸ì ë³€í™˜\n",
        "            text = text.lower()\n",
        "            # ê°„ë‹¨í•œ ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì œê±°ëŠ” ìµœì†Œí™”)\n",
        "            import re\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            processed.append(text)\n",
        "        return processed\n",
        "\n",
        "    def train_svm_classifier(self, df):\n",
        "        \"\"\"SVM ë¶„ë¥˜ê¸° í›ˆë ¨ (2000ë…„ëŒ€ ë°©ì‹)\"\"\"\n",
        "        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "        df['processed_text'] = self.preprocess_text_2000s_style(df['text'])\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_text'], df['category'],\n",
        "            test_size=0.3, random_state=42, stratify=df['category']\n",
        "        )\n",
        "\n",
        "        # 2000ë…„ëŒ€ ìŠ¤íƒ€ì¼: TF-IDF + SVM\n",
        "        self.pipeline = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer(\n",
        "                max_features=1000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                max_df=0.8,\n",
        "                stop_words='english'\n",
        "            )),\n",
        "            ('classifier', SVC(\n",
        "                kernel='linear',\n",
        "                C=1.0,\n",
        "                random_state=42,\n",
        "                # ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ í™•ë¥  ì˜ˆì¸¡ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
        "                probability=True\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # ëª¨ë¸ í›ˆë ¨\n",
        "        print(\"=== 2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ SVM í…ìŠ¤íŠ¸ ë¶„ë¥˜ê¸° í›ˆë ¨ ===\")\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # ì˜ˆì¸¡ ë° í‰ê°€\n",
        "        y_pred = self.pipeline.predict(X_test)\n",
        "\n",
        "        print(\"\\në¶„ë¥˜ ì„±ëŠ¥ ë³´ê³ ì„œ:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # êµì°¨ ê²€ì¦ (2000ë…„ëŒ€ í‘œì¤€ í‰ê°€ ë°©ë²•)\n",
        "        cv_scores = cross_val_score(self.pipeline, X_train, y_train, cv=5)\n",
        "        print(f\"\\n5-Fold êµì°¨ê²€ì¦ ì •í™•ë„: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "        return X_test, y_test, y_pred\n",
        "\n",
        "    def analyze_features(self):\n",
        "        \"\"\"íŠ¹ì„± ë¶„ì„ (2000ë…„ëŒ€ ë°©ì‹)\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.\")\n",
        "            return\n",
        "\n",
        "        # íŠ¹ì„± ì´ë¦„ê³¼ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\n",
        "        vectorizer = self.pipeline.named_steps['vectorizer']\n",
        "        classifier = self.pipeline.named_steps['classifier']\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        print(\"\\n=== íŠ¹ì„± ë¶„ì„ (SVM ê°€ì¤‘ì¹˜) ===\")\n",
        "\n",
        "        # ê° í´ë˜ìŠ¤ë³„ ì¤‘ìš” íŠ¹ì„± ì¶œë ¥\n",
        "        classes = classifier.classes_\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f\"\\n'{class_name}' í´ë˜ìŠ¤ì˜ ì¤‘ìš” íŠ¹ì„±:\")\n",
        "\n",
        "            # í•´ë‹¹ í´ë˜ìŠ¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜\n",
        "            if hasattr(classifier, 'coef_'):\n",
        "                # ì˜¤ì§ ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "                # .toarray()ë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜\n",
        "                weights = classifier.coef_[i].toarray().flatten() if len(classes) > 2 else classifier.coef_[0].toarray().flatten()\n",
        "\n",
        "                # ìƒìœ„ 10ê°œ íŠ¹ì„±\n",
        "                top_indices = np.argsort(weights)[-10:][::-1]\n",
        "                for idx in top_indices:\n",
        "                    print(f\"  {feature_names[idx]}: {weights[idx]:.3f}\")\n",
        "\n",
        "# ì‹¤í–‰ ì˜ˆì œ\n",
        "print(\"=== 2000ë…„ëŒ€ SVM í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤ìŠµ ===\\n\")\n",
        "\n",
        "classifier = SVMTextClassifier2000s()\n",
        "\n",
        "# ----------------------------\n",
        "# 1. ë°ì´í„°ì…‹ ìƒì„±\n",
        "# ----------------------------\n",
        "df = classifier.create_sample_dataset()\n",
        "print(\"ìƒì„±ëœ ë°ì´í„°ì…‹:\")\n",
        "print(df.groupby('category').size())\n",
        "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
        "for category in df['category'].unique():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    sample = df[df['category'] == category]['text'].iloc[0]\n",
        "    print(f\"  {sample}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. SVM ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€\n",
        "# ----------------------------\n",
        "X_test, y_test, y_pred = classifier.train_svm_classifier(df)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. íŠ¹ì„± ë¶„ì„\n",
        "# ----------------------------\n",
        "classifier.analyze_features()\n",
        "\n",
        "# ----------------------------\n",
        "# 4. ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\n",
        "# ----------------------------\n",
        "print(\"\\n=== ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ===\")\n",
        "test_texts = [\n",
        "    \"Microsoft announces new software development kit\",\n",
        "    \"Basketball team wins championship game\",\n",
        "    \"Senator proposes new economic policy\",\n",
        "    \"Movie star wins academy award\"\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ë¶„ë¥˜\n",
        "# ----------------------------\n",
        "for text in test_texts:\n",
        "    processed = classifier.preprocess_text_2000s_style([text])\n",
        "\n",
        "    # ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬\n",
        "    prediction = classifier.pipeline.predict(processed)[0]\n",
        "\n",
        "    # decision_functionì„ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°\n",
        "    decision_scores = classifier.pipeline.decision_function(processed)[0]\n",
        "\n",
        "    # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì˜ ê²½ìš°, ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©\n",
        "    max_score = np.max(decision_scores)\n",
        "\n",
        "    print(f\"\\ní…ìŠ¤íŠ¸: {text}\")\n",
        "    print(f\"ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬: {prediction}\")\n",
        "    print(f\"ì‹ ë¢°ë„ ì ìˆ˜ (decision_function): {max_score:.3f}\")\n",
        "\n",
        "# ì‹ ë¢°ë„ ì ìˆ˜ (decision_function): ì˜ˆì¸¡ëœ ìƒ˜í”Œì´ SVMì˜ ê²°ì • ê²½ê³„(decision boundary)ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’\n",
        "#  ì ˆëŒ€ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ë¶„ë¥˜ ê²°ê³¼ì— ëŒ€í•œ ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ë†’ë‹¤ëŠ” ì˜ë¯¸"
      ],
      "metadata": {
        "id": "uaF0NK4BeElX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ : SVMì„ ì´ìš©í•œ ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜"
      ],
      "metadata": {
        "id": "AMOS08iulX0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EmailFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"ìˆ˜ì •ëœ 2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ ì´ë©”ì¼ íŠ¹ì„± ì¶”ì¶œê¸°\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.spam_keywords = [\n",
        "            'free', 'money', 'win', 'winner', 'cash', 'prize', 'offer',\n",
        "            'click', 'buy', 'sale', 'discount', 'viagra', 'cialis',\n",
        "            'mortgage', 'loan', 'credit', 'debt', 'investment',\n",
        "            'guarantee', 'urgent', 'act now', 'limited time'\n",
        "        ]\n",
        "\n",
        "        self.spam_patterns = [\n",
        "            r'\\$+', r'!{2,}', r'\\*+', r'#{2,}', r'%+',\n",
        "            r'[A-Z]{3,}', r'\\d+%', r'www\\.', r'http://',\n",
        "            r'\\.com', r'\\.net', r'\\.org'\n",
        "        ]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"ì´ë©”ì¼ì—ì„œ ìˆ˜ì¹˜ íŠ¹ì„± ì¶”ì¶œ\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for email in X:\n",
        "            email_features = []\n",
        "            email_lower = email.lower()\n",
        "\n",
        "            # ì•ˆì „í•œ íŠ¹ì„± ê³„ì‚°\n",
        "            try:\n",
        "                # 1. ìŠ¤íŒ¸ í‚¤ì›Œë“œ ë¹ˆë„\n",
        "                spam_keyword_count = sum(email_lower.count(keyword) for keyword in self.spam_keywords)\n",
        "                email_features.append(float(spam_keyword_count))\n",
        "\n",
        "                # 2. ëŒ€ë¬¸ì ë¹„ìœ¨\n",
        "                if len(email) > 0:\n",
        "                    upper_ratio = sum(1 for c in email if c.isupper()) / len(email)\n",
        "                else:\n",
        "                    upper_ratio = 0.0\n",
        "                email_features.append(float(upper_ratio))\n",
        "\n",
        "                # 3. íŠ¹ìˆ˜ë¬¸ì íŒ¨í„´ ê°œìˆ˜\n",
        "                special_pattern_count = sum(len(re.findall(pattern, email)) for pattern in self.spam_patterns)\n",
        "                email_features.append(float(special_pattern_count))\n",
        "\n",
        "                # 4. ìˆ«ì ë¹„ìœ¨\n",
        "                if len(email) > 0:\n",
        "                    digit_ratio = sum(1 for c in email if c.isdigit()) / len(email)\n",
        "                else:\n",
        "                    digit_ratio = 0.0\n",
        "                email_features.append(float(digit_ratio))\n",
        "\n",
        "                # 5. í‰ê·  ë‹¨ì–´ ê¸¸ì´\n",
        "                words = email_lower.split()\n",
        "                if len(words) > 0:\n",
        "                    avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "                else:\n",
        "                    avg_word_length = 0.0\n",
        "                email_features.append(float(avg_word_length))\n",
        "\n",
        "                # 6. ëŠë‚Œí‘œ ê°œìˆ˜\n",
        "                exclamation_count = email.count('!')\n",
        "                email_features.append(float(exclamation_count))\n",
        "\n",
        "                # 7. URL ê°œìˆ˜\n",
        "                url_count = len(re.findall(r'http[s]?://|www\\.', email_lower))\n",
        "                email_features.append(float(url_count))\n",
        "\n",
        "                # 8. ì´ë©”ì¼ ì£¼ì†Œ ê°œìˆ˜\n",
        "                email_count = len(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', email))\n",
        "                email_features.append(float(email_count))\n",
        "\n",
        "            except Exception as e:\n",
        "                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •\n",
        "                email_features = [0.0] * 8\n",
        "\n",
        "            features.append(email_features)\n",
        "\n",
        "        # numpy ë°°ì—´ë¡œ ë³€í™˜ (í¬ì†Œ í–‰ë ¬ ë¬¸ì œ ë°©ì§€)\n",
        "        return np.array(features, dtype=np.float64)\n",
        "\n",
        "class FixedSVMSpamFilter2000s:\n",
        "    \"\"\"ìˆ˜ì •ëœ 2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ SVM ìŠ¤íŒ¸ í•„í„°\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.text_feature_names = None\n",
        "        self.custom_feature_names = [\n",
        "            'spam_keywords', 'upper_ratio', 'special_patterns',\n",
        "            'digit_ratio', 'avg_word_length', 'exclamation_count',\n",
        "            'url_count', 'email_count'\n",
        "        ]\n",
        "\n",
        "    def create_2000s_email_dataset(self):\n",
        "        \"\"\"2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ ì´ë©”ì¼ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
        "\n",
        "        ham_emails = [\n",
        "            \"Hi John, hope you're doing well. Let's meet for coffee this weekend to discuss the project.\",\n",
        "            \"The quarterly report is ready for review. Please find the attached document with detailed analysis.\",\n",
        "            \"Thank you for your presentation yesterday. The team found it very informative and useful.\",\n",
        "            \"Reminder: Staff meeting tomorrow at 10 AM in the conference room. Agenda attached.\",\n",
        "            \"Great job on the client presentation! The feedback has been overwhelmingly positive.\",\n",
        "            \"Please review the contract terms and let me know if you have any questions or concerns.\",\n",
        "            \"The software update is scheduled for this weekend. Expect brief downtime on Sunday morning.\",\n",
        "            \"Congratulations on your promotion! Well deserved after all your hard work this year.\",\n",
        "            \"Can you send me the latest version of the budget spreadsheet when you have a moment?\",\n",
        "            \"The training session was very helpful. Looking forward to implementing these new techniques.\",\n",
        "            \"Happy birthday! Hope you have a wonderful day celebrating with family and friends.\",\n",
        "            \"The conference next month looks interesting. Would you like to attend together?\",\n",
        "            \"Please confirm your attendance for the team dinner on Friday evening at 7 PM.\",\n",
        "            \"The new hire orientation is scheduled for Monday. HR will send details soon.\",\n",
        "            \"Thanks for covering my shift yesterday. I really appreciate your help and flexibility.\"\n",
        "        ]\n",
        "\n",
        "        spam_emails = [\n",
        "            \"CONGRATULATIONS!!! You've WON $1,000,000!!! Click HERE NOW to claim your PRIZE!!!\",\n",
        "            \"URGENT: Your account will be closed! Click www.fakebank.com to verify information NOW!\",\n",
        "            \"FREE MONEY! Get $500 cash instantly! No credit check required! Act now!!!\",\n",
        "            \"VIAGRA 50% OFF! Best prices guaranteed! Order now and save BIG money!!!\",\n",
        "            \"You've been selected for a special offer! Buy now and get 90% discount!!!\",\n",
        "            \"MAKE $5000 A WEEK working from home! No experience needed! Start TODAY!!!\",\n",
        "            \"WARNING: Your computer is infected! Download our FREE antivirus software NOW!\",\n",
        "            \"Lose 20 pounds in 10 days! GUARANTEED results! Order our miracle pills now!\",\n",
        "            \"CREDIT PROBLEMS? No problem! Get approved for any loan instantly! Apply now!\",\n",
        "            \"FREE iPod! Just pay shipping and handling! Limited time offer - ACT FAST!!!\",\n",
        "            \"HOT SINGLES in your area want to meet you! Click here for instant access!\",\n",
        "            \"Your mortgage can be cut in HALF! Refinance now and save thousands!!!\",\n",
        "            \"FANTASTIC INVESTMENT OPPORTUNITY! Double your money in 30 days! Risk-free!\",\n",
        "            \"URGENT: You have unclaimed inheritance of $2.5 million! Contact us immediately!\",\n",
        "            \"CASINO BONUS: $200 FREE! No deposit required! Play now and win BIG!!!\"\n",
        "        ]\n",
        "\n",
        "        emails = ham_emails + spam_emails\n",
        "        labels = ['ham'] * len(ham_emails) + ['spam'] * len(spam_emails)\n",
        "\n",
        "        return pd.DataFrame({'email': emails, 'label': labels})\n",
        "\n",
        "    def preprocess_email(self, emails):\n",
        "        \"\"\"ì´ë©”ì¼ ì „ì²˜ë¦¬\"\"\"\n",
        "        processed = []\n",
        "        for email in emails:\n",
        "            email = re.sub(r'[^\\w\\s@.-]', ' ', email)\n",
        "            email = re.sub(r'\\s+', ' ', email)\n",
        "            processed.append(email.strip())\n",
        "        return processed\n",
        "\n",
        "    def build_2000s_pipeline(self):\n",
        "        \"\"\"ìˆ˜ì •ëœ íŒŒì´í”„ë¼ì¸ êµ¬ì„±\"\"\"\n",
        "\n",
        "        # í…ìŠ¤íŠ¸ íŠ¹ì„± íŒŒì´í”„ë¼ì¸\n",
        "        text_pipeline = TfidfVectorizer(\n",
        "            max_features=500,  # íŠ¹ì„± ìˆ˜ ì¤„ì„\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=1,\n",
        "            max_df=0.9,\n",
        "            stop_words='english',\n",
        "            lowercase=True\n",
        "        )\n",
        "\n",
        "        # ì „ì²´ íŒŒì´í”„ë¼ì¸ (FeatureUnion ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "        self.pipeline = Pipeline([\n",
        "            ('tfidf', text_pipeline),\n",
        "            ('classifier', SVC(\n",
        "                kernel='linear',\n",
        "                C=1.0,\n",
        "                probability=True,\n",
        "                class_weight='balanced',\n",
        "                random_state=42\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def train_spam_filter(self, df):\n",
        "        \"\"\"ìŠ¤íŒ¸ í•„í„° í›ˆë ¨\"\"\"\n",
        "        print(\"=== ìˆ˜ì •ëœ 2000ë…„ëŒ€ SVM ìŠ¤íŒ¸ í•„í„° í›ˆë ¨ ===\")\n",
        "\n",
        "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
        "        df['processed_email'] = self.preprocess_email(df['email'])\n",
        "\n",
        "        # ë°ì´í„° ë¶„í• \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_email'], df['label'],\n",
        "            test_size=0.3, random_state=42, stratify=df['label']\n",
        "        )\n",
        "\n",
        "        # íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° í›ˆë ¨\n",
        "        self.build_2000s_pipeline()\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # íŠ¹ì„± ì´ë¦„ ì €ì¥\n",
        "        self.text_feature_names = self.pipeline.named_steps['tfidf'].get_feature_names_out()\n",
        "\n",
        "        # ì„±ëŠ¥ í‰ê°€\n",
        "        y_pred = self.pipeline.predict(X_test)\n",
        "        y_prob = self.pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        print(f\"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ\")\n",
        "        print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ\")\n",
        "        print(f\"ìŠ¤íŒ¸ ë¹„ìœ¨: {(y_train == 'spam').mean():.2f}\")\n",
        "\n",
        "        print(\"\\n=== ì„±ëŠ¥ í‰ê°€ ===\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # ROC AUC\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_test == 'spam', y_prob)\n",
        "            print(f\"ROC AUC: {roc_auc:.3f}\")\n",
        "        except:\n",
        "            print(\"ROC AUC ê³„ì‚° ì‹¤íŒ¨\")\n",
        "\n",
        "        # êµì°¨ ê²€ì¦\n",
        "        cv_scores = cross_val_score(self.pipeline, X_train, y_train, cv=5, scoring='f1_macro')\n",
        "        print(f\"5-Fold CV F1-Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "        return X_test, y_test, y_pred, y_prob\n",
        "\n",
        "    def analyze_spam_features_fixed(self):\n",
        "        \"\"\"ìˆ˜ì •ëœ ìŠ¤íŒ¸ íƒì§€ íŠ¹ì„± ë¶„ì„\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== ìˆ˜ì •ëœ ìŠ¤íŒ¸ íƒì§€ ì¤‘ìš” íŠ¹ì„± ë¶„ì„ ===\")\n",
        "\n",
        "        try:\n",
        "            # SVM ë¶„ë¥˜ê¸° ê°€ì ¸ì˜¤ê¸°\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "\n",
        "            if not hasattr(classifier, 'coef_'):\n",
        "                print(\"ì„ í˜• SVMì´ ì•„ë‹ˆë¯€ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "                return\n",
        "\n",
        "            # ê°€ì¤‘ì¹˜ ì¶”ì¶œ ë° numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "            coef_matrix = classifier.coef_\n",
        "            if hasattr(coef_matrix, 'toarray'):\n",
        "                weights = coef_matrix.toarray()[0]\n",
        "            else:\n",
        "                weights = np.array(coef_matrix[0]).flatten()\n",
        "\n",
        "            feature_names = self.text_feature_names\n",
        "\n",
        "            print(f\"ì´ íŠ¹ì„± ìˆ˜: {len(weights)}\")\n",
        "            print(f\"íŠ¹ì„± ì´ë¦„ ìˆ˜: {len(feature_names)}\")\n",
        "\n",
        "            # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì²˜ë¦¬\n",
        "            max_features = min(len(weights), len(feature_names))\n",
        "\n",
        "            if max_features == 0:\n",
        "                print(\"ë¶„ì„í•  íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "                return\n",
        "\n",
        "            # ìŠ¤íŒ¸ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒìœ„ íŠ¹ì„± (ì–‘ìˆ˜ ê°€ì¤‘ì¹˜)\n",
        "            positive_indices = []\n",
        "            negative_indices = []\n",
        "\n",
        "            for i in range(max_features):\n",
        "                weight_val = float(weights[i])  # ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜\n",
        "                if weight_val > 0:\n",
        "                    positive_indices.append((i, weight_val))\n",
        "                elif weight_val < 0:\n",
        "                    negative_indices.append((i, weight_val))\n",
        "\n",
        "            # ì •ë ¬\n",
        "            positive_indices.sort(key=lambda x: x[1], reverse=True)\n",
        "            negative_indices.sort(key=lambda x: x[1])\n",
        "\n",
        "            print(\"\\nìŠ¤íŒ¸ì„ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± (ìƒìœ„ 10ê°œ):\")\n",
        "            for i, (idx, weight) in enumerate(positive_indices[:10]):\n",
        "                if idx < len(feature_names):\n",
        "                    print(f\"  {i+1}. {feature_names[idx]}: {weight:.3f}\")\n",
        "\n",
        "            print(\"\\nì •ìƒ ì´ë©”ì¼ì„ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„± (ìƒìœ„ 10ê°œ):\")\n",
        "            for i, (idx, weight) in enumerate(negative_indices[:10]):\n",
        "                if idx < len(feature_names):\n",
        "                    print(f\"  {i+1}. {feature_names[idx]}: {weight:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"íŠ¹ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            print(\"ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "            # ê¸°ë³¸ í†µê³„ ì •ë³´\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "            print(f\"SVM í´ë˜ìŠ¤: {classifier.classes_}\")\n",
        "            print(f\"ì„œí¬íŠ¸ ë²¡í„° ìˆ˜: {classifier.n_support_}\")\n",
        "\n",
        "    def test_new_emails_fixed(self):\n",
        "        \"\"\"ìˆ˜ì •ëœ ìƒˆë¡œìš´ ì´ë©”ì¼ í…ŒìŠ¤íŠ¸\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼œì£¼ì„¸ìš”.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== ìƒˆë¡œìš´ ì´ë©”ì¼ ìŠ¤íŒ¸ íƒì§€ í…ŒìŠ¤íŠ¸ ===\")\n",
        "\n",
        "        test_emails = [\n",
        "            \"Hi Sarah, can we reschedule our meeting to Thursday afternoon? Thanks!\",\n",
        "            \"FREE CASH NOW!!! Click here to get $1000 instantly! NO QUESTIONS ASKED!!!\",\n",
        "            \"The project deadline has been moved to next Friday. Please update your schedules.\",\n",
        "            \"URGENT! Your account expires TODAY! Verify now at www.scamsite.com or lose access!\",\n",
        "            \"Lunch at the new Italian restaurant was great. Highly recommend their pasta dishes.\",\n",
        "            \"VIAGRA CIALIS 80% OFF! Order now and save BIG! Discreet shipping guaranteed!\",\n",
        "            \"Please find attached the updated employee handbook. HR policy changes included.\",\n",
        "            \"WIN A FREE IPHONE! Just pay $19.95 shipping! Limited time offer - Act NOW!\"\n",
        "        ]\n",
        "\n",
        "        for i, email in enumerate(test_emails, 1):\n",
        "            try:\n",
        "                processed = self.preprocess_email([email])\n",
        "                prediction = self.pipeline.predict(processed)[0]\n",
        "                probabilities = self.pipeline.predict_proba(processed)[0]\n",
        "\n",
        "                # í´ë˜ìŠ¤ ìˆœì„œ í™•ì¸\n",
        "                classes = self.pipeline.named_steps['classifier'].classes_\n",
        "                spam_idx = list(classes).index('spam') if 'spam' in classes else 1\n",
        "                spam_prob = probabilities[spam_idx]\n",
        "\n",
        "                print(f\"\\n{i}. ì´ë©”ì¼: {email[:60]}...\")\n",
        "                print(f\"   ì˜ˆì¸¡: {prediction.upper()}\")\n",
        "                print(f\"   ìŠ¤íŒ¸ í™•ë¥ : {spam_prob:.3f}\")\n",
        "\n",
        "                confidence = \"ë†’ìŒ\" if max(probabilities) > 0.8 else \"ë³´í†µ\" if max(probabilities) > 0.6 else \"ë‚®ìŒ\"\n",
        "                print(f\"   ì‹ ë¢°ë„: {confidence}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n{i}. ì´ë©”ì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    def simple_performance_analysis(self, df):\n",
        "        \"\"\"ê°„ë‹¨í•œ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
        "        print(\"\\n=== ê°„ë‹¨í•œ ì„±ëŠ¥ ë¶„ì„ ===\")\n",
        "\n",
        "        try:\n",
        "            # ê¸°ë³¸ í†µê³„\n",
        "            print(f\"ë°ì´í„°ì…‹ í¬ê¸°: {len(df)}\")\n",
        "            print(f\"ìŠ¤íŒ¸ ë¹„ìœ¨: {(df['label'] == 'spam').mean():.2f}\")\n",
        "\n",
        "            # ëª¨ë¸ ì •ë³´\n",
        "            if self.pipeline:\n",
        "                classifier = self.pipeline.named_steps['classifier']\n",
        "                print(f\"SVM ì»¤ë„: {classifier.kernel}\")\n",
        "                print(f\"C íŒŒë¼ë¯¸í„°: {classifier.C}\")\n",
        "                print(f\"í´ë˜ìŠ¤: {classifier.classes_}\")\n",
        "\n",
        "                if hasattr(classifier, 'n_support_'):\n",
        "                    print(f\"ì„œí¬íŠ¸ ë²¡í„° ìˆ˜: {classifier.n_support_}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# ìˆ˜ì •ëœ ì‹¤í–‰ ì˜ˆì œ\n",
        "print(\"=== ìˆ˜ì •ëœ 2000ë…„ëŒ€ SVM ìŠ¤íŒ¸ ì´ë©”ì¼ í•„í„° ===\\n\")\n",
        "\n",
        "# 1. ìŠ¤íŒ¸ í•„í„° ìƒì„±\n",
        "spam_filter = FixedSVMSpamFilter2000s()\n",
        "\n",
        "# 2. ë°ì´í„°ì…‹ ìƒì„±\n",
        "email_df = spam_filter.create_2000s_email_dataset()\n",
        "print(\"ì´ë©”ì¼ ë°ì´í„°ì…‹:\")\n",
        "print(email_df['label'].value_counts())\n",
        "\n",
        "print(\"\\nìƒ˜í”Œ ì´ë©”ì¼:\")\n",
        "ham_sample = email_df[email_df['label'] == 'ham']['email'].iloc[0]\n",
        "spam_sample = email_df[email_df['label'] == 'spam']['email'].iloc[0]\n",
        "print(f\"HAM: {ham_sample[:80]}...\")\n",
        "print(f\"SPAM: {spam_sample[:80]}...\")\n",
        "\n",
        "# 3. ìŠ¤íŒ¸ í•„í„° í›ˆë ¨\n",
        "try:\n",
        "    X_test, y_test, y_pred, y_prob = spam_filter.train_spam_filter(email_df)\n",
        "    print(\"\\nëª¨ë¸ í›ˆë ¨ ì„±ê³µ!\")\n",
        "except Exception as e:\n",
        "    print(f\"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# 4. í˜¼ë™ í–‰ë ¬\n",
        "try:\n",
        "    print(\"\\n=== í˜¼ë™ í–‰ë ¬ ===\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"í˜¼ë™ í–‰ë ¬:\")\n",
        "    print(f\"        ì˜ˆì¸¡\")\n",
        "    print(f\"ì‹¤ì œ    HAM  SPAM\")\n",
        "    print(f\"HAM     {cm[0,0]:3d}   {cm[0,1]:3d}\")\n",
        "    print(f\"SPAM    {cm[1,0]:3d}   {cm[1,1]:3d}\")\n",
        "except Exception as e:\n",
        "    print(f\"í˜¼ë™ í–‰ë ¬ ìƒì„± ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# 5. ìˆ˜ì •ëœ íŠ¹ì„± ë¶„ì„\n",
        "spam_filter.analyze_spam_features_fixed()\n",
        "\n",
        "# 6. ìˆ˜ì •ëœ ì´ë©”ì¼ í…ŒìŠ¤íŠ¸\n",
        "spam_filter.test_new_emails_fixed()\n",
        "\n",
        "# 7. ê°„ë‹¨í•œ ì„±ëŠ¥ ë¶„ì„\n",
        "spam_filter.simple_performance_analysis(email_df)\n",
        "\n",
        "# 8. 2000ë…„ëŒ€ ê¸°ìˆ ì  íŠ¹ì§• ìš”ì•½\n",
        "print(\"\\n=== 2000ë…„ëŒ€ SVM ìŠ¤íŒ¸ í•„í„° ê¸°ìˆ ì  íŠ¹ì§• ===\")\n",
        "print(\"ì¥ì :\")\n",
        "print(\"  â€¢ ê³ ì°¨ì› í…ìŠ¤íŠ¸ ë°ì´í„°ì— íš¨ê³¼ì \")\n",
        "print(\"  â€¢ ë§ˆì§„ ìµœëŒ€í™”ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜\")\n",
        "print(\"  â€¢ í™•ë¥  ê¸°ë°˜ ì‹ ë¢°ë„ ì œê³µ\")\n",
        "print(\"  â€¢ ë¹„êµì  ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©\")\n",
        "\n",
        "print(\"\\ní•œê³„ì :\")\n",
        "print(\"  â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì†ë„ ëŠë¦¼\")\n",
        "print(\"  â€¢ íŠ¹ì„± ê³µí•™ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´\")\n",
        "print(\"  â€¢ ìˆœì°¨ ì •ë³´ ì†ì‹¤\")\n",
        "print(\"  â€¢ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”\")\n",
        "\n",
        "print(\"\\në‹¹ì‹œ ì‹¤ì œ ì„±ëŠ¥:\")\n",
        "print(\"  â€¢ ì •í™•ë„: 95-98%\")\n",
        "print(\"  â€¢ ì˜¤íƒë¥ : 1-3%\")\n",
        "print(\"  â€¢ ì²˜ë¦¬ì†ë„: 1000+ ì´ë©”ì¼/ì´ˆ\")"
      ],
      "metadata": {
        "id": "Ynsev-Zdldx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ : HMMì„ ì´ìš©í•œ í’ˆì‚¬ íƒœê¹…"
      ],
      "metadata": {
        "id": "tmp4RxUSeM2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "try:\n",
        "    nltk.data.find('corpora/treebank')\n",
        "except LookupError:\n",
        "    nltk.download('treebank', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "class ImprovedHMM2000s:\n",
        "    \"\"\"2000ë…„ëŒ€ ê°œì„ ëœ HMM í’ˆì‚¬ íƒœê±°\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing_parameter=0.01):\n",
        "        self.smoothing = smoothing_parameter\n",
        "        self.states = set()  # í’ˆì‚¬ íƒœê·¸ë“¤\n",
        "        self.observations = set()  # ë‹¨ì–´ë“¤\n",
        "        self.initial_prob = defaultdict(float)\n",
        "        self.transition_prob = defaultdict(lambda: defaultdict(float))\n",
        "        self.emission_prob = defaultdict(lambda: defaultdict(float))\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.tag_counts = defaultdict(int)\n",
        "        self.tag_word_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def create_2000s_training_data(self):\n",
        "        \"\"\"2000ë…„ëŒ€ ìŠ¤íƒ€ì¼ í›ˆë ¨ ë°ì´í„° ìƒì„±\"\"\"\n",
        "        from nltk.corpus import treebank\n",
        "\n",
        "        # Treebank ì½”í¼ìŠ¤ ì‚¬ìš© (2000ë…„ëŒ€ í‘œì¤€)\n",
        "        tagged_sentences = treebank.tagged_sents()\n",
        "\n",
        "        # ë°ì´í„° ì „ì²˜ë¦¬ (2000ë…„ëŒ€ ë°©ì‹)\n",
        "        processed_sentences = []\n",
        "        for sentence in tagged_sentences[:1000]:  # ì œí•œëœ ë°ì´í„°\n",
        "            processed_sent = []\n",
        "            for word, tag in sentence:\n",
        "                # ê°„ë‹¨í•œ ì •ê·œí™”\n",
        "                word = word.lower()\n",
        "                # ìˆ«ìëŠ” íŠ¹ë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬\n",
        "                if word.isdigit():\n",
        "                    word = '<NUM>'\n",
        "                # í¬ì†Œí•œ ë‹¨ì–´ëŠ” <UNK>ë¡œ ì²˜ë¦¬\n",
        "                elif len(word) == 1 and not word.isalpha():\n",
        "                    word = '<PUNCT>'\n",
        "\n",
        "                processed_sent.append((word, tag))\n",
        "\n",
        "            if processed_sent:  # ë¹ˆ ë¬¸ì¥ ì œì™¸\n",
        "                processed_sentences.append(processed_sent)\n",
        "\n",
        "        return processed_sentences\n",
        "\n",
        "    def train(self, tagged_sentences):\n",
        "        \"\"\"HMM ëª¨ë¸ í›ˆë ¨ (ìŠ¤ë¬´ë”© ì ìš©)\"\"\"\n",
        "        print(\"=== 2000ë…„ëŒ€ ê°œì„ ëœ HMM ëª¨ë¸ í›ˆë ¨ ===\")\n",
        "\n",
        "        # 1. í†µê³„ ìˆ˜ì§‘\n",
        "        for sentence in tagged_sentences:\n",
        "            prev_tag = '<START>'\n",
        "\n",
        "            for word, tag in sentence:\n",
        "                # ê´€ì°° ë° ìƒíƒœ ì§‘í•© êµ¬ì¶•\n",
        "                self.observations.add(word)\n",
        "                self.states.add(tag)\n",
        "\n",
        "                # ì¹´ìš´íŠ¸ ìˆ˜ì§‘\n",
        "                self.word_counts[word] += 1\n",
        "                self.tag_counts[tag] += 1\n",
        "                self.tag_word_counts[tag][word] += 1\n",
        "\n",
        "                # ì´ˆê¸° í™•ë¥  (ë¬¸ì¥ ì²« ë‹¨ì–´)\n",
        "                if prev_tag == '<START>':\n",
        "                    self.initial_prob[tag] += 1\n",
        "\n",
        "                # ì „ì´ í™•ë¥ \n",
        "                self.transition_prob[prev_tag][tag] += 1\n",
        "\n",
        "                prev_tag = tag\n",
        "\n",
        "            # ë¬¸ì¥ ë ì „ì´\n",
        "            self.transition_prob[prev_tag]['<END>'] += 1\n",
        "\n",
        "        # 2. í™•ë¥  ì •ê·œí™” (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©)\n",
        "        self._normalize_probabilities()\n",
        "\n",
        "        print(f\"í›ˆë ¨ ì™„ë£Œ:\")\n",
        "        print(f\"  - ìƒíƒœ(íƒœê·¸) ìˆ˜: {len(self.states)}\")\n",
        "        print(f\"  - ê´€ì°°(ë‹¨ì–´) ìˆ˜: {len(self.observations)}\")\n",
        "        print(f\"  - í›ˆë ¨ ë¬¸ì¥ ìˆ˜: {len(tagged_sentences)}\")\n",
        "\n",
        "    def _normalize_probabilities(self):\n",
        "        \"\"\"í™•ë¥  ì •ê·œí™” ë° ìŠ¤ë¬´ë”©\"\"\"\n",
        "        # ì´ˆê¸° í™•ë¥  ì •ê·œí™”\n",
        "        total_initial = sum(self.initial_prob.values())\n",
        "        for tag in self.initial_prob:\n",
        "            self.initial_prob[tag] /= total_initial\n",
        "\n",
        "        # ì „ì´ í™•ë¥  ì •ê·œí™” (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©)\n",
        "        for prev_tag in self.transition_prob:\n",
        "            total_transitions = sum(self.transition_prob[prev_tag].values())\n",
        "            vocab_size = len(self.states) + 1  # +1 for <END>\n",
        "\n",
        "            for next_tag in self.transition_prob[prev_tag]:\n",
        "                count = self.transition_prob[prev_tag][next_tag]\n",
        "                # ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©\n",
        "                self.transition_prob[prev_tag][next_tag] = \\\n",
        "                    (count + self.smoothing) / (total_transitions + self.smoothing * vocab_size)\n",
        "\n",
        "        # ë°©ì¶œ í™•ë¥  ì •ê·œí™” (ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©)\n",
        "        for tag in self.tag_word_counts:\n",
        "            total_emissions = sum(self.tag_word_counts[tag].values())\n",
        "            vocab_size = len(self.observations)\n",
        "\n",
        "            for word in self.tag_word_counts[tag]:\n",
        "                count = self.tag_word_counts[tag][word]\n",
        "                # ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©\n",
        "                self.emission_prob[tag][word] = \\\n",
        "                    (count + self.smoothing) / (total_emissions + self.smoothing * vocab_size)\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        \"\"\"ë¹„í„°ë¹„ ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë””ì½”ë”©\"\"\"\n",
        "        words = [word.lower() for word in sentence]\n",
        "        n = len(words)\n",
        "\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        # ë¯¸ì§€ ë‹¨ì–´ ì²˜ë¦¬\n",
        "        processed_words = []\n",
        "        for word in words:\n",
        "            if word not in self.observations:\n",
        "                if word.isdigit():\n",
        "                    processed_words.append('<NUM>')\n",
        "                elif len(word) == 1 and not word.isalpha():\n",
        "                    processed_words.append('<PUNCT>')\n",
        "                else:\n",
        "                    processed_words.append('<UNK>')\n",
        "            else:\n",
        "                processed_words.append(word)\n",
        "\n",
        "        # ë™ì  ê³„íšë²• í…Œì´ë¸”\n",
        "        dp = defaultdict(lambda: defaultdict(float))\n",
        "        backpointer = defaultdict(lambda: defaultdict(str))\n",
        "\n",
        "        # ì´ˆê¸°í™”\n",
        "        for tag in self.states:\n",
        "            init_prob = self.initial_prob.get(tag, self.smoothing)\n",
        "            emission_prob = self._get_emission_prob(tag, processed_words[0])\n",
        "            dp[0][tag] = np.log(init_prob) + np.log(emission_prob)\n",
        "\n",
        "        # ìˆœë°©í–¥ ë‹¨ê³„\n",
        "        for t in range(1, n):\n",
        "            for curr_tag in self.states:\n",
        "                max_prob = float('-inf')\n",
        "                best_prev_tag = None\n",
        "\n",
        "                for prev_tag in self.states:\n",
        "                    trans_prob = self._get_transition_prob(prev_tag, curr_tag)\n",
        "                    emission_prob = self._get_emission_prob(curr_tag, processed_words[t])\n",
        "\n",
        "                    prob = dp[t-1][prev_tag] + np.log(trans_prob) + np.log(emission_prob)\n",
        "\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        best_prev_tag = prev_tag\n",
        "\n",
        "                dp[t][curr_tag] = max_prob\n",
        "                backpointer[t][curr_tag] = best_prev_tag\n",
        "\n",
        "        # ìµœì  ê²½ë¡œ ì—­ì¶”ì \n",
        "        best_path = [''] * n\n",
        "\n",
        "        # ë§ˆì§€ë§‰ íƒœê·¸ ì°¾ê¸°\n",
        "        max_prob = float('-inf')\n",
        "        for tag in self.states:\n",
        "            if dp[n-1][tag] > max_prob:\n",
        "                max_prob = dp[n-1][tag]\n",
        "                best_path[n-1] = tag\n",
        "\n",
        "        # ì—­ë°©í–¥ ì¶”ì \n",
        "        for t in range(n-2, -1, -1):\n",
        "            best_path[t] = backpointer[t+1][best_path[t+1]]\n",
        "\n",
        "        return list(zip(words, best_path))\n",
        "\n",
        "    def _get_transition_prob(self, prev_tag, curr_tag):\n",
        "        \"\"\"ì „ì´ í™•ë¥  ì¡°íšŒ (ìŠ¤ë¬´ë”© ì ìš©)\"\"\"\n",
        "        if prev_tag in self.transition_prob and curr_tag in self.transition_prob[prev_tag]:\n",
        "            return self.transition_prob[prev_tag][curr_tag]\n",
        "        else:\n",
        "            # ë¯¸ê´€ì°° ì „ì´ì— ëŒ€í•œ ìŠ¤ë¬´ë”©\n",
        "            total_transitions = sum(self.transition_prob[prev_tag].values()) if prev_tag in self.transition_prob else 0\n",
        "            vocab_size = len(self.states)\n",
        "            return self.smoothing / (total_transitions + self.smoothing * vocab_size)\n",
        "\n",
        "    def _get_emission_prob(self, tag, word):\n",
        "        \"\"\"ë°©ì¶œ í™•ë¥  ì¡°íšŒ (ë¯¸ì§€ ë‹¨ì–´ ì²˜ë¦¬)\"\"\"\n",
        "        if tag in self.emission_prob and word in self.emission_prob[tag]:\n",
        "            return self.emission_prob[tag][word]\n",
        "        else:\n",
        "            # ë¯¸ê´€ì°° ë‹¨ì–´ì— ëŒ€í•œ ìŠ¤ë¬´ë”©\n",
        "            total_emissions = sum(self.tag_word_counts[tag].values()) if tag in self.tag_word_counts else 1\n",
        "            vocab_size = len(self.observations)\n",
        "            return self.smoothing / (total_emissions + self.smoothing * vocab_size)\n",
        "\n",
        "    def evaluate(self, test_sentences):\n",
        "        \"\"\"ëª¨ë¸ í‰ê°€\"\"\"\n",
        "        all_true_tags = []\n",
        "        all_pred_tags = []\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for sentence in test_sentences:\n",
        "            words = [word for word, tag in sentence]\n",
        "            true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "            pred_result = self.viterbi_decode(words)\n",
        "            pred_tags = [tag for word, tag in pred_result]\n",
        "\n",
        "            all_true_tags.extend(true_tags)\n",
        "            all_pred_tags.extend(pred_tags)\n",
        "\n",
        "            # ë¬¸ì¥ë³„ ì •í™•ë„\n",
        "            for true_tag, pred_tag in zip(true_tags, pred_tags):\n",
        "                if true_tag == pred_tag:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "        print(f\"\\n=== HMM ëª¨ë¸ í‰ê°€ ê²°ê³¼ ===\")\n",
        "        print(f\"ì „ì²´ ì •í™•ë„: {accuracy:.4f}\")\n",
        "        print(f\"ì˜¬ë°”ë¥¸ ì˜ˆì¸¡: {correct}/{total}\")\n",
        "\n",
        "        return accuracy, all_true_tags, all_pred_tags\n",
        "\n",
        "# ì‹¤í–‰ ì˜ˆì œ\n",
        "print(\"=== 2000ë…„ëŒ€ ê°œì„ ëœ HMM í’ˆì‚¬ íƒœê¹… ì‹¤ìŠµ ===\\n\")\n",
        "\n",
        "# 1. HMM ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
        "hmm_model = ImprovedHMM2000s(smoothing_parameter=0.01)\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
        "training_data = hmm_model.create_2000s_training_data()\n",
        "print(f\"í›ˆë ¨ ë°ì´í„°: {len(training_data)}ê°œ ë¬¸ì¥\")\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "hmm_model.train(training_data[:800])  # 80% í›ˆë ¨ìš©\n",
        "\n",
        "# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
        "test_data = training_data[800:]  # 20% í…ŒìŠ¤íŠ¸ìš©\n",
        "accuracy, true_tags, pred_tags = hmm_model.evaluate(test_data)\n",
        "\n",
        "# 3. ì‹¤ì œ ë¬¸ì¥ íƒœê¹… í…ŒìŠ¤íŠ¸\n",
        "print(\"\\n=== ì‹¤ì œ ë¬¸ì¥ íƒœê¹… í…ŒìŠ¤íŠ¸ ===\")\n",
        "test_sentences = [\n",
        "    [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"],\n",
        "    [\"Machine\", \"learning\", \"is\", \"very\", \"interesting\"],\n",
        "    [\"I\", \"am\", \"studying\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"Google\", \"released\", \"a\", \"new\", \"algorithm\"]\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result = hmm_model.viterbi_decode(sentence)\n",
        "    print(f\"\\në¬¸ì¥: {' '.join(sentence)}\")\n",
        "    print(\"íƒœê¹… ê²°ê³¼:\")\n",
        "    for word, tag in result:\n",
        "        print(f\"  {word} â†’ {tag}\")\n",
        "\n",
        "# 4. ì„±ëŠ¥ ë¹„êµ (NLTK ê¸°ë³¸ íƒœê±°ì™€ ë¹„êµ)\n",
        "print(\"\\n=== ì„±ëŠ¥ ë¹„êµ (NLTK vs ê°œì„ ëœ HMM) ===\")\n",
        "sample_sentence = [\"The\", \"company\", \"announced\", \"new\", \"product\", \"development\"]\n",
        "\n",
        "# NLTK ê¸°ë³¸ íƒœê±°\n",
        "try:\n",
        "    nltk_result = nltk.pos_tag(sample_sentence)\n",
        "    print(f\"NLTK ê²°ê³¼: {nltk_result}\")\n",
        "except:\n",
        "    print(\"NLTK íƒœê±° ì‚¬ìš© ë¶ˆê°€\")\n",
        "\n",
        "# ê°œì„ ëœ HMM ê²°ê³¼\n",
        "hmm_result = hmm_model.viterbi_decode(sample_sentence)\n",
        "print(f\"ê°œì„ ëœ HMM: {hmm_result}\")"
      ],
      "metadata": {
        "id": "9rla1jF7ePQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ: SVMë¥¼ ì´ìš©í•œ ë¬¸ì¥ ìƒì„±\n",
        "- SVMì€ ìƒì„± ëª¨ë¸ì´ ì•„ë‹ˆë¼ íŒë³„ ëª¨ë¸,\n",
        "- ì´ì „ ë‹¨ì–´ë“¤(ì»¨í…ìŠ¤íŠ¸) â†’ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ ë’¤,\n",
        "- ì˜ˆì¸¡ì„ ë°˜ë³µí•˜ì—¬ ë¬¸ì¥ì„ ìƒì„±\n",
        "- **[ì£¼ì˜!] ì‹¤í–‰ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ --> ì½”ë©ì—ì„œëŠ” RAMì„ ëª¨ë‘ ì‚¬ìš© í›„ ì„¸ì…˜ì´ ë‹¤ìš´ ë  ìˆ˜ ìˆìŒ**"
      ],
      "metadata": {
        "id": "62yvo0u6bIw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì£¼ì˜! ì‹¤í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆë‹¤.\n",
        "# SVM(LinearSVC)ë¡œ \"ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡\"ì„ ë°˜ë³µí•˜ì—¬ ë¬¸ì¥ ìƒì„±\n",
        "# ë°ì´í„°: Project Gutenberg #1342 (Pride and Prejudice)\n",
        "\n",
        "import re, requests, numpy as np, math, random\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import softmax\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "# -----------------------------\n",
        "# 1) ë°ì´í„° ìˆ˜ì§‘/ì „ì²˜ë¦¬\n",
        "# -----------------------------\n",
        "URL = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "\n",
        "def fetch_text(url=URL):\n",
        "    return requests.get(url).text\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    # ë‹¨ì–´ + ë¬¸ì¥ë¶€í˜¸(.?!)\n",
        "    toks = re.findall(r\"\\b\\w+\\b|[.!?]\", text.lower())\n",
        "    sents, cur = [], []\n",
        "    for t in toks:\n",
        "        if t in [\".\", \"!\", \"?\"]:\n",
        "            if cur:\n",
        "                sents.append(cur)\n",
        "                cur = []\n",
        "        else:\n",
        "            cur.append(t)\n",
        "    if cur:\n",
        "        sents.append(cur)\n",
        "    return sents\n",
        "\n",
        "def build_vocab(sentences, top_k=20000, min_freq=2):\n",
        "    freq = Counter(w for s in sentences for w in s)\n",
        "    # ë¹ˆë„ ê¸°ì¤€ í•„í„° â†’ ìƒìœ„ top_k\n",
        "    candidates = [w for w, c in freq.items() if c >= min_freq]\n",
        "    candidates.sort(key=lambda w: freq[w], reverse=True)\n",
        "    vocab = set(candidates[:top_k])\n",
        "    vocab |= {\"<unk>\", \"<s>\"}            # íŠ¹ìˆ˜ í† í°\n",
        "    return vocab, freq\n",
        "\n",
        "def apply_unk(sentences, vocab):\n",
        "    return [[w if w in vocab else \"<unk>\" for w in s] for s in sentences]\n",
        "\n",
        "def add_sentence_markers(sentences):\n",
        "    # ê° ë¬¸ì¥ ì•ì— ì‹œì‘ í† í° <s>ë¥¼ ë‘ ë²ˆ ë¶™ì—¬ ì»¨í…ìŠ¤íŠ¸ 2ê°œ í™•ë³´\n",
        "    return [[\"<s>\", \"<s>\"] + s for s in sentences if len(s) > 0]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) ë°ì´í„°ì…‹ êµ¬ì„±: (w-2, w-1) -> w0\n",
        "# -----------------------------\n",
        "def make_dataset(sentences):\n",
        "    X_feats = []\n",
        "    y_words = []\n",
        "    for s in sentences:\n",
        "        # s: [\"<s>\",\"<s>\", w0, w1, ...]\n",
        "        for i in range(2, len(s)):\n",
        "            w_2, w_1, w0 = s[i-2], s[i-1], s[i]\n",
        "            # íŠ¹ì§•ì€ dictë¡œ êµ¬ì„± (í•´ì‹±ìœ¼ë¡œ ë³€í™˜ ì˜ˆì •)\n",
        "            feats = {\n",
        "                f\"w-2={w_2}\": 1,\n",
        "                f\"w-1={w_1}\": 1,\n",
        "                f\"bigram={w_2}|{w_1}\": 1,\n",
        "            }\n",
        "            X_feats.append(feats)\n",
        "            y_words.append(w0)\n",
        "    return X_feats, y_words\n",
        "\n",
        "# -----------------------------\n",
        "# 3) í•™ìŠµ íŒŒì´í”„ë¼ì¸\n",
        "# -----------------------------\n",
        "def train_svm_nextword(\n",
        "    top_k_vocab=20000,\n",
        "    min_freq=2,\n",
        "    n_features=2**18,  # FeatureHasher ì°¨ì›\n",
        "    test_size=0.05,\n",
        "    random_state=42\n",
        "):\n",
        "    print(\"Downloading text ...\")\n",
        "    text = fetch_text(URL)\n",
        "\n",
        "    print(\"Sentence tokenizing ...\")\n",
        "    sents = sentence_tokenize(text)\n",
        "    print(f\"Total sentences: {len(sents)}\")\n",
        "\n",
        "    print(\"Building vocab ...\")\n",
        "    vocab, freq = build_vocab(sents, top_k=top_k_vocab, min_freq=min_freq)\n",
        "\n",
        "    print(\"Applying <unk> and adding <s> markers ...\")\n",
        "    sents = apply_unk(sents, vocab)\n",
        "    sents = add_sentence_markers(sents)\n",
        "\n",
        "    print(\"Building (context -> next word) dataset ...\")\n",
        "    X_feats, y_words = make_dataset(sents)\n",
        "\n",
        "    # í•´ì‹± íŠ¹ì§• â†’ í¬ì†Œí–‰ë ¬\n",
        "    hasher = FeatureHasher(n_features=n_features, input_type=\"dict\")\n",
        "    X = hasher.transform(X_feats)\n",
        "\n",
        "    # ë ˆì´ë¸” ì¸ì½”ë”©\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_words)\n",
        "\n",
        "    # ë°ì´í„° ë¶„í• \n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"Training LinearSVC (multi-class) ...\")\n",
        "    clf = LinearSVC(random_state=random_state, dual=\"auto\")\n",
        "    clf.fit(X_tr, y_tr)\n",
        "\n",
        "    acc = clf.score(X_te, y_te)\n",
        "    print(f\"Test accuracy (next-word, top-1): {acc:.4f}\")\n",
        "    return clf, hasher, le, vocab\n",
        "\n",
        "# -----------------------------\n",
        "# 4) ë¬¸ì¥ ìƒì„±\n",
        "# -----------------------------\n",
        "def context_to_features(w_2, w_1):\n",
        "    return {f\"w-2={w_2}\": 1, f\"w-1={w_1}\": 1, f\"bigram={w_2}|{w_1}\": 1}\n",
        "\n",
        "def generate_with_svm(\n",
        "    clf, hasher, le, vocab,\n",
        "    seed_word=\"love\",\n",
        "    max_len=30,\n",
        "    temperature=0.8,   # ë‚®ì¶œìˆ˜ë¡ ê²°ì •ì , ë†’ì¼ìˆ˜ë¡ ëœë¤\n",
        "    top_k=10,          # ìƒìœ„ k í›„ë³´ì—ì„œ ìƒ˜í”Œ\n",
        "    end_tokens=(\".\", \"!\", \"?\")\n",
        "):\n",
        "    # ì‹œì‘ ì»¨í…ìŠ¤íŠ¸: <s>, seed\n",
        "    w_2, w_1 = \"<s>\", (seed_word if seed_word in vocab else \"<unk>\")\n",
        "    result = [w_1]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        feats = context_to_features(w_2, w_1)\n",
        "        X = hasher.transform([feats])\n",
        "        scores = clf.decision_function(X)           # shape: (1, n_classes)\n",
        "        scores = np.asarray(scores).ravel()\n",
        "\n",
        "        # ìƒìœ„ k í›„ë³´\n",
        "        top_idx = np.argsort(-scores)[:top_k]\n",
        "        top_scores = scores[top_idx]\n",
        "\n",
        "        # SVM ì ìˆ˜ â†’ í™•ë¥  ë¹„ìŠ·í•˜ê²Œ ìƒ˜í”Œë§ (softmax with temperature)\n",
        "        probs = softmax(top_scores / max(temperature, 1e-6))\n",
        "        next_label = np.random.choice(top_idx, p=probs)\n",
        "        next_word = le.inverse_transform([next_label])[0]\n",
        "\n",
        "        result.append(next_word)\n",
        "\n",
        "        # ì¢…ë£Œì¡°ê±´: ë¬¸ì¥ë¶€í˜¸ ì˜ˆì¸¡ ì‹œ ì¢…ë£Œ\n",
        "        if next_word in end_tokens:\n",
        "            break\n",
        "\n",
        "        # ì»¨í…ìŠ¤íŠ¸ ì´ë™\n",
        "        w_2, w_1 = w_1, next_word\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) ì‹¤í–‰\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    clf, hasher, le, vocab = train_svm_nextword(\n",
        "        top_k_vocab=20000, min_freq=2, n_features=2**18, test_size=0.05\n",
        "    )\n",
        "\n",
        "    # ì œì‹œì–´: love\n",
        "    for i in range(3):\n",
        "        s = generate_with_svm(\n",
        "            clf, hasher, le, vocab,\n",
        "            seed_word=\"love\",\n",
        "            max_len=30,\n",
        "            temperature=0.9,\n",
        "            top_k=8\n",
        "        )\n",
        "        print(f\"[Gen {i+1}] {s}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "90w0EqVCZF3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}