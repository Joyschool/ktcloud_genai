{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbilsJ+//g7c+Y2lID+5Qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/ktcloud_genai/blob/main/102_LLM_%EC%A0%84%ED%86%B5%EC%A0%81NLP_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QuickTour** : 전통적인 NLP"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡**코드 내용**\n",
        "    - 2000년대 이전의 자연어 처리(NLP) 과정 이해를 돕는 예제들고 구성되었습니다."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1980~90년대: 통계적 NLP, 확률론적 접근법\n",
        "\n",
        "- **핵심 설명**\n",
        "    - 1980~90년대 자연어처리(NLP)는 규칙 기반 접근법의 한계를 극복하기 위해 **통계적 방법**으로 전환됨\n",
        "    - **대용량 텍스트 코퍼스의 등장** → **확률 모델 기반 언어 처리** 가능\n",
        "    - 대표 기법: n-그램 언어 모델, HMM(Hidden Markov Model) 기반 품사 태깅\n",
        "    - ***n-그램 언어 모델** :\n",
        "        - 앞의 n-1 단어를 이용해 다음 단어의 확률을 추정하는 확률적 언어 모델\n",
        "    - ***HMM(Hidden Markov Model)** :\n",
        "        - 관찰할 수 있는 데이터(예: 단어)를 통해 그 뒤에 숨겨진 상태(예: 품사)를 추론하는 통계적 모델\n",
        "        - 숨겨진 상태(예: 품사) 의 전이 확률과 관측 단어의 발생 확률을 함께 고려하여 텍스트 시퀀스를 모델링하는 확률 기반 모델\n",
        "        - HMM은 **전이확률(P(tagᵢ|tagᵢ₋₁))** +  **발생확률(P(word|tag))** 을 이용해 가장 가능성 높은 시퀀스를 선택"
      ],
      "metadata": {
        "id": "vjJh4IMODeSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : n-그램 언어모델**"
      ],
      "metadata": {
        "id": "_MAPofpjVe4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) n-그램 언어모델 예제 (Brown Corpus 활용)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams, ConditionalFreqDist\n",
        "\n",
        "nltk.download('brown')\n",
        "words = brown.words(categories='news')\n",
        "bi_grams = list(bigrams(words))\n",
        "cfd = ConditionalFreqDist(bi_grams)\n",
        "\n",
        "print(\"예측 단어 (앞 단어='the'):\", cfd[\"the\"].most_common(5))"
      ],
      "metadata": {
        "id": "I2QbPiIzOAIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : HMM 품사 태깅**"
      ],
      "metadata": {
        "id": "NRS-qz2SVpj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **영어 처리**"
      ],
      "metadata": {
        "id": "mzWe2XKhw996"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 자연어 처리(NLP)를 위한 빠르고 실용적인 오픈소스 라이브러리\n",
        "!pip install spacy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5O6U_12WUNwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어(en) 언어 모델: en_core_web_sm 다운로드 --> 세션 다시시작\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "G6_V2MS4URU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# en_core_web_sm 모델을 로드합니다.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Google is a major technology company located in California.\"\n",
        "\n",
        "# 텍스트를 모델로 처리합니다.\n",
        "doc = nlp(text)\n",
        "\n",
        "# 각 단어에 대한 정보 출력\n",
        "print(\"{:<15} {:<10} {:<10}\".format(\"단어\", \"품사\", \"개체명\"))\n",
        "print(\"-\" * 35)\n",
        "for token in doc:\n",
        "    entity = token.ent_type_ if token.ent_type_ else \"없음\"\n",
        "    print(f\"{token.text:<15} {token.pos_:<10} {entity:<10}\")"
      ],
      "metadata": {
        "id": "M9Wb2SBAvnY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **한국어 처리**\n",
        "- 한국어 형태소 분석기 : https://www.bigkinds.or.kr/v2/analysis/featureExtraction.do#"
      ],
      "metadata": {
        "id": "97FuOvzJw3P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# konlpy 설치\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "ihcQO2rVw3lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "# Okt 형태소 분석기 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "text = \"아버지가 방에 들어가신다.\"\n",
        "\n",
        "# 형태소 분석\n",
        "morphs = okt.morphs(text)\n",
        "print(f\"형태소 분석: {morphs}\")\n",
        "\n",
        "# 품사 태깅\n",
        "pos_tags = okt.pos(text, stem=True)\n",
        "print(f\"품사 태깅: {pos_tags}\")"
      ],
      "metadata": {
        "id": "YWkboWB0xIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Spacy(국제 표준) + Konlpy(한국어에 특화된 표준) --> 소통되도록 적절한 처리가 적용됨"
      ],
      "metadata": {
        "id": "nR2HaTSaxdFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from konlpy.tag import Okt\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Konlpy의 태그를 Spacy의 UD 태그로 매핑하는 딕셔너리\n",
        "# 모든 태그를 매핑할 필요는 없지만, 예제에 사용된 것 위주로 정의합니다.\n",
        "TAG_MAP = {\n",
        "    'Noun': 'NOUN',\n",
        "    'Josa': 'ADP',  # 한국어의 '조사'는 영어의 전치사에 가까운 역할\n",
        "    'Verb': 'VERB',\n",
        "    'Adjective': 'ADJ',\n",
        "    'Punctuation': 'PUNCT',\n",
        "    'Modifier': 'ADJ', # '매우'와 같은 부사는 형용사에 가깝게 매핑\n",
        "}\n",
        "\n",
        "def konlpy_tokenizer(text):\n",
        "    \"\"\"Konlpy로 토큰화하고 품사 태그를 Spacy에 맞게 반환하는 함수\"\"\"\n",
        "    okt = Okt()\n",
        "    pos_tags = okt.pos(text, stem=True)\n",
        "    words = [t[0] for t in pos_tags]\n",
        "    # Konlpy 태그를 Spacy의 'tag'에, UD 태그를 'pos'에 할당\n",
        "    tags = [t[1] for t in pos_tags]\n",
        "    pos = [TAG_MAP.get(t[1], 'X') for t in pos_tags]  # 매핑되지 않은 태그는 'X'로 처리\n",
        "    return words, pos, tags\n",
        "\n",
        "# 한글 문장 예제\n",
        "text = \"오늘 날씨가 매우 좋습니다.\"\n",
        "\n",
        "# Konlpy로 토큰화 및 품사 태깅\n",
        "words, pos, tags = konlpy_tokenizer(text)\n",
        "\n",
        "# Spacy의 Doc 객체로 변환\n",
        "# 주의: 'pos' 대신 'tags' 속성에 Konlpy의 태그를 할당합니다.\n",
        "doc = Doc(nlp.vocab, words=words, tags=tags, pos=pos)\n",
        "\n",
        "# Spacy Doc 객체로 변환된 결과를 출력\n",
        "print(\"Spacy Doc 객체로 변환된 결과:\")\n",
        "print(f\"{'단어':<10} {'원래 태그 (tag)':<15} {'UD 태그 (pos)':<15}\")\n",
        "print(\"-\" * 45)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<10} {token.tag_:<15} {token.pos_:<15}\")"
      ],
      "metadata": {
        "id": "3Tp5IfsVximf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wp95PCZdxdHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) HMM 품사 태깅 예제 (NLTK HMM Tagger)\n",
        "\n",
        "# 테스트 문장들\n",
        "test_sentences = [\n",
        "    \"Time flies like an arrow\",\n",
        "    \"Fruit flies like a banana\",\n",
        "    \"Programming is really fun\"\n",
        "]\n",
        "\n",
        "\n",
        "# 간단한 규칙 기반 태거\n",
        "def simple_rule_based_tagger():\n",
        "    \"\"\"간단한 규칙 기반 품사 태거\"\"\"\n",
        "\n",
        "    # 기본 품사 사전\n",
        "    pos_dict = {\n",
        "        'the': 'DT', 'a': 'DT', 'an': 'DT',\n",
        "        'is': 'VBZ', 'are': 'VBP', 'was': 'VBD', 'were': 'VBD',\n",
        "        'and': 'CC', 'or': 'CC', 'but': 'CC',\n",
        "        'in': 'IN', 'on': 'IN', 'at': 'IN', 'like': 'IN',\n",
        "        'very': 'RB', 'really': 'RB', 'quite': 'RB'\n",
        "    }\n",
        "\n",
        "    def tag_sentence(sentence):\n",
        "        words = sentence.lower().split()\n",
        "        tagged = []\n",
        "\n",
        "        for word in words:\n",
        "            if word in pos_dict:\n",
        "                tag = pos_dict[word]\n",
        "            elif word.endswith('ing'):\n",
        "                tag = 'VBG'  # 현재분사\n",
        "            elif word.endswith('ed'):\n",
        "                tag = 'VBD'  # 과거형\n",
        "            elif word.endswith('ly'):\n",
        "                tag = 'RB'   # 부사\n",
        "            else:\n",
        "                tag = 'NN'   # 기본값: 명사\n",
        "\n",
        "            tagged.append((word, tag))\n",
        "\n",
        "        return tagged\n",
        "\n",
        "    return tag_sentence\n",
        "\n",
        "print(\"\\n=== 간단한 규칙 기반 태거 ===\")\n",
        "rule_tagger = simple_rule_based_tagger()\n",
        "\n",
        "for sentence in [\"Time flies like an arrow\", \"Programming is really fun\"]:\n",
        "    result = rule_tagger(sentence)\n",
        "    print(f\"원문: {sentence}\")\n",
        "    print(f\"태깅: {result}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "D7YciS3tTq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 : 대용량 텍스트로 n-그램 언어모델 구축으로 새로운 문장 생성\n",
        "- 이 코드는 **구텐베르크 대용량 텍스트**를 다운로드해서 (--> Project Gutenberg에서 제공하는 고전 문학 전자책 모음)\n",
        "- 빅그램(bigram) 모델(확률적 NLP 기법)을 만들고,\n",
        "- **조건부 확률 기반으로 새로운 문장을 생성**함"
      ],
      "metadata": {
        "id": "jQHx8oG7Div4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ERcAduBZ1q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import requests\n",
        "\n",
        "# 1) 대용량 텍스트 데이터 다운로드 (위키피디아 문서 예시)\n",
        "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"  # 제인 오스틴 'Pride and Prejudice(오만과 편견)'\n",
        "text = requests.get(url).text.lower()\n",
        "\n",
        "\n",
        "# 2) 토큰화 (단어 단위)\n",
        "tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
        "#  \"단어 경계(\\b)로 시작해서, 하나 이상의 단어 문자(\\w+)가 나오고, 다시 단어 경계(\\b)로 끝나는 문자열을 찾아라.\"\n",
        "\n",
        "\n",
        "# 3) n-그램(빅그램) 생성\n",
        "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "\n",
        "# 4) 조건부 확률 계산 (P(w2 | w1))\n",
        "bigram_model = defaultdict(list)\n",
        "for (w1, w2), freq in bigram_counts.items():\n",
        "    bigram_model[w1].append((w2, freq))\n",
        "\n",
        "\n",
        "# 5) 확률 기반 텍스트 생성\n",
        "def generate_text(start_word, length=20):\n",
        "    word = start_word\n",
        "    result = [word]\n",
        "    for _ in range(length):\n",
        "        if word not in bigram_model:\n",
        "            break\n",
        "        candidates, weights = zip(*bigram_model[word])\n",
        "        word = random.choices(candidates, weights=weights, k=1)[0]  # 가중치 값이 큰 것 선택\n",
        "        result.append(word)\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "print(generate_text(\"love\", 30))  # 지시어, 단어갯수\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 비지도 학습 방법으로 학습한 HMM 언어 모델 예제**\n",
        "- EM(Expectation-Maximization)\n",
        "    - HMM의 비지도 학습(unsupervised learning) 방식의 대표적 알고리즘\n",
        "- 퍼플렉시티(perplexity : PPL)\n",
        "    - 언어 모델을 평가하기 위한 평가 지표 -->(헷갈리는 정도를 수치화)\n",
        "    - 수치가 '낮을수록' 언어 모델의 성능이 좋다.\n",
        "    - $PPL(W)=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{i-1})}}$\n",
        "- 로그우도(Log-Likelihood)\n",
        "    - 어떤 모델이 주어진 데이터(여기서는 문장)를 얼마나 잘 설명하는지를 나타내는 지표\n",
        "    - '우도(Likelihood)'는 모델이 데이터를 생성할 확률을 의미하며,\n",
        "    - 이 우도 값에 로그(log)를 취한 것이 바로 로그우도\n",
        "    - 예: 언어 모델이 \"I love AI.\"라는 문장을 생성할 확률\n",
        "        - 우도(Likelihood): P(\"IloveAI.\")=P(\"I\")xP(\"love\"|\"I\")xP(\"AI\"|\"Ilove\")\n",
        "        - 로그우도(Log-Likelihood): log(P(\"IloveAI.\"))=log(P(\"I\"))+log(P(\"love\"|\"I\"))+log(P(\"AI\"|\"Ilove\"))\n",
        "    -  로그 적용 이유\n",
        "        - 값이 0이 되지 않고 음수로 변환되어 정확한 계산을 유지할 수 있다.\n",
        "        - 복잡한 곱셈 연산을 단순한 덧셈 연산으로 바꿀 수 있\n"
      ],
      "metadata": {
        "id": "QY5G4ozHDlhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주의! 실행 시간이 오래 걸릴 수 있다.\n",
        "# ============================\n",
        "# HMM Language Model (unsupervised, EM)\n",
        "# bigram example:\n",
        "# ============================\n",
        "import re, math, random, requests\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------\n",
        "# 0) 데이터 준비 (동일 URL)\n",
        "# ----------------------------\n",
        "URL = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "\n",
        "def fetch_text(url=URL):\n",
        "    text = requests.get(url).text\n",
        "    return text\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    # 단어 + 문장부호(.?!)\n",
        "    toks = re.findall(r\"\\b\\w+\\b|[.!?]\", text.lower())\n",
        "    sents, cur = [], []\n",
        "    for t in toks:\n",
        "        if t in [\".\", \"!\", \"?\"]:\n",
        "            if cur:\n",
        "                sents.append(cur)\n",
        "                cur = []\n",
        "        else:\n",
        "            cur.append(t)\n",
        "    if cur:  # 마지막 잔여 토큰\n",
        "        sents.append(cur)\n",
        "    return sents  # 리스트(문장)들의 리스트(단어)\n",
        "\n",
        "# ----------------------------\n",
        "# 1) 어휘 구성 & 희귀어 처리\n",
        "# ----------------------------\n",
        "def build_vocab(sentences, min_freq=2, max_tokens=None):\n",
        "    # sentences: List[List[str]]\n",
        "    if max_tokens:\n",
        "        # 대용량일 때 학습 토큰 수 제한(옵션)\n",
        "        flat = [w for s in sentences for w in s][:max_tokens]\n",
        "        # 문장도 잘라서 맞춰줌\n",
        "        words_left = set(flat)\n",
        "        trimmed = []\n",
        "        total = 0\n",
        "        for s in sentences:\n",
        "            keep = []\n",
        "            for w in s:\n",
        "                if total >= max_tokens: break\n",
        "                if w in words_left:\n",
        "                    keep.append(w); total += 1\n",
        "            if keep: trimmed.append(keep)\n",
        "            if total >= max_tokens: break\n",
        "        sentences = trimmed\n",
        "\n",
        "    freq = Counter(w for s in sentences for w in s)\n",
        "    vocab = {w for w,c in freq.items() if c >= min_freq}\n",
        "    vocab |= {\"<unk>\"}  # 희귀어 치환\n",
        "    word2id = {w:i for i,w in enumerate(sorted(vocab))}\n",
        "    id2word = {i:w for w,i in word2id.items()}\n",
        "    # 희귀어 치환\n",
        "    proc = [[w if w in vocab else \"<unk>\" for w in s] for s in sentences]\n",
        "    return proc, word2id, id2word\n",
        "\n",
        "# ----------------------------\n",
        "# 2) 시퀀스 변환\n",
        "# ----------------------------\n",
        "def to_id_sequences(sentences, word2id):\n",
        "    return [np.array([word2id[w] for w in s], dtype=np.int32) for s in sentences if len(s) > 0]\n",
        "\n",
        "# ----------------------------\n",
        "# 3) 수치 안정화 유틸\n",
        "# ----------------------------\n",
        "def logsumexp(v):\n",
        "    m = np.max(v)\n",
        "    return m + np.log(np.sum(np.exp(v - m) + 1e-300))\n",
        "\n",
        "# ----------------------------\n",
        "# 4) HMM (비지도, EM)\n",
        "# ----------------------------\n",
        "class HMM:\n",
        "    def __init__(self, n_states, vocab_size, seed=42):\n",
        "        self.K = n_states\n",
        "        self.V = vocab_size\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        # 파라미터는 log-확률로 보관\n",
        "        # 초기분포 pi, 전이 A, 방출 B\n",
        "        pi = rng.random(self.K); pi /= pi.sum()\n",
        "        A = rng.random((self.K, self.K)); A /= A.sum(axis=1, keepdims=True)\n",
        "        B = rng.random((self.K, self.V)); B /= B.sum(axis=1, keepdims=True)\n",
        "\n",
        "        self.log_pi = np.log(pi + 1e-300)\n",
        "        self.log_A  = np.log(A + 1e-300)\n",
        "        self.log_B  = np.log(B + 1e-300)\n",
        "\n",
        "    # Forward-Backward (한 문장)\n",
        "    def forward(self, seq):\n",
        "        T = len(seq)\n",
        "        alpha = np.full((T, self.K), -np.inf)\n",
        "        # t=0\n",
        "        alpha[0] = self.log_pi + self.log_B[:, seq[0]]\n",
        "        # t>=1\n",
        "        for t in range(1, T):\n",
        "            bt = self.log_B[:, seq[t]]\n",
        "            for k in range(self.K):\n",
        "                alpha[t, k] = bt[k] + logsumexp(alpha[t-1] + self.log_A[:, k])\n",
        "        ll = logsumexp(alpha[-1])\n",
        "        return alpha, ll\n",
        "\n",
        "    def backward(self, seq):\n",
        "        T = len(seq)\n",
        "        beta = np.full((T, self.K), -np.inf)\n",
        "        beta[-1] = 0.0  # log(1)\n",
        "        for t in range(T-2, -1, -1):\n",
        "            bt1 = self.log_B[:, seq[t+1]]\n",
        "            for k in range(self.K):\n",
        "                beta[t, k] = logsumexp(self.log_A[k] + bt1 + beta[t+1])\n",
        "        return beta\n",
        "\n",
        "    def e_step_accumulate(self, seq, exp_pi, exp_A, exp_B):\n",
        "        alpha, ll = self.forward(seq)\n",
        "        beta = self.backward(seq)\n",
        "        T = len(seq)\n",
        "\n",
        "        # gamma (state posterior)\n",
        "        gamma = alpha + beta - ll\n",
        "        gamma = np.exp(gamma)  # (T,K)\n",
        "\n",
        "        # xi (pairwise transition posterior) 누적\n",
        "        for t in range(T-1):\n",
        "            # (K,K): alpha[t, j] + log_A[j, k]\n",
        "            s = alpha[t][:, None] + self.log_A\n",
        "            # (K,K): + log_B[k, w_{t+1}]\n",
        "            s = s + self.log_B[:, seq[t+1]][None, :]\n",
        "            # (K,K): + beta[t+1, k]\n",
        "            s = s + beta[t+1][None, :]\n",
        "            # normalize in log-space by subtracting ll\n",
        "            s = s - ll\n",
        "            xi = np.exp(s)                  # (K,K)\n",
        "            exp_A += xi                     # 누적\n",
        "\n",
        "        exp_pi += gamma[0]\n",
        "        for t in range(T):\n",
        "            exp_B[:, seq[t]] += gamma[t]\n",
        "\n",
        "        return ll\n",
        "\n",
        "\n",
        "    def m_step(self, exp_pi, exp_A, exp_B, alpha_smooth=1e-2):\n",
        "        # Dirichlet-like 평활화\n",
        "        exp_pi = exp_pi + alpha_smooth\n",
        "        exp_A  = exp_A  + alpha_smooth\n",
        "        exp_B  = exp_B  + alpha_smooth\n",
        "\n",
        "        self.log_pi = np.log(exp_pi / exp_pi.sum() + 1e-300)\n",
        "\n",
        "        A = exp_A / exp_A.sum(axis=1, keepdims=True)\n",
        "        B = exp_B / exp_B.sum(axis=1, keepdims=True)\n",
        "\n",
        "        self.log_A = np.log(A + 1e-300)\n",
        "        self.log_B = np.log(B + 1e-300)\n",
        "\n",
        "    def fit(self, sequences, n_iter=5, alpha_smooth=1e-2, verbose=True):\n",
        "        for it in range(1, n_iter+1):\n",
        "            exp_pi = np.zeros(self.K)\n",
        "            exp_A  = np.zeros((self.K, self.K))\n",
        "            exp_B  = np.zeros((self.K, self.V))\n",
        "\n",
        "            total_ll = 0.0\n",
        "            total_T  = 0\n",
        "            for seq in sequences:\n",
        "                ll = self.e_step_accumulate(seq, exp_pi, exp_A, exp_B)\n",
        "                total_ll += ll\n",
        "                total_T  += len(seq)\n",
        "\n",
        "            self.m_step(exp_pi, exp_A, exp_B, alpha_smooth=alpha_smooth)\n",
        "            avg_nll = - total_ll / max(total_T, 1)\n",
        "            if verbose:\n",
        "                print(f\"[EM {it}/{n_iter}] avg NLL per token: {avg_nll:.4f} (perplexity ≈ {math.exp(avg_nll):.2f})\")\n",
        "\n",
        "    # 문장 로그우도 (forward의 ll)\n",
        "    def sequence_loglik(self, seq):\n",
        "        _, ll = self.forward(seq)\n",
        "        return ll\n",
        "\n",
        "    # 무조건 생성\n",
        "    def sample_unconditional(self, id2word, length=20, seed=0):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # z0 ~ pi, w0 ~ B[z0]\n",
        "        z = rng.choice(self.K, p=np.exp(self.log_pi))\n",
        "        words = []\n",
        "        for t in range(length):\n",
        "            w = rng.choice(len(id2word), p=np.exp(self.log_B[z]))\n",
        "            words.append(id2word[w])\n",
        "            z = rng.choice(self.K, p=np.exp(self.log_A[z]))\n",
        "        return \" \".join(words)\n",
        "\n",
        "    # 시드 단어를 조건으로 첫 상태를 후험에서 샘플(사후확률)\n",
        "    def sample_conditioned_on_first_word(self, first_word_id, id2word, length=20, seed=0):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        # p(z0 | w0) ∝ pi * B[:, w0]\n",
        "        post0 = np.exp(self.log_pi + self.log_B[:, first_word_id])\n",
        "        post0 = post0 / post0.sum()\n",
        "        z = rng.choice(self.K, p=post0)\n",
        "\n",
        "        words = [id2word[first_word_id]]\n",
        "        for _ in range(length-1):\n",
        "            z = rng.choice(self.K, p=np.exp(self.log_A[z]))\n",
        "            w = rng.choice(len(id2word), p=np.exp(self.log_B[z]))\n",
        "            words.append(id2word[w])\n",
        "        return \" \".join(words)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) 실행 파이프라인\n",
        "# ----------------------------\n",
        "def train_hmm_language_model(\n",
        "    n_states=8,             # 숨은 상태 수 (주제/문맥 클러스터처럼 작동)\n",
        "    min_freq=3,            # 희귀어 임계치\n",
        "    max_tokens=200_000,    # 대용량 시 속도/메모리 제어 (None = 전체 사용)\n",
        "    em_iters=6,            # EM 반복 횟수\n",
        "    seed=42\n",
        "):\n",
        "    print(\"Downloading text ... (Project Gutenberg #1342)\")\n",
        "    text = fetch_text(URL)\n",
        "    print(\"Sentence tokenizing ...\")\n",
        "    sentences = sentence_tokenize(text)   # List[List[str]]\n",
        "    print(f\"Total sentences: {len(sentences)}\")\n",
        "\n",
        "    print(\"Building vocab & <unk> ...\")\n",
        "    proc_sents, word2id, id2word = build_vocab(sentences, min_freq=min_freq, max_tokens=max_tokens)\n",
        "    sequences = to_id_sequences(proc_sents, word2id)\n",
        "    total_tokens = sum(len(s) for s in sequences)\n",
        "    print(f\"Vocab size: {len(word2id)}, Tokens used: {total_tokens}\")\n",
        "\n",
        "    hmm = HMM(n_states, vocab_size=len(word2id), seed=seed)\n",
        "    print(\"Training HMM with EM ...\")\n",
        "    hmm.fit(sequences, n_iter=em_iters, alpha_smooth=1e-2, verbose=True)\n",
        "\n",
        "    # 상태별 상위 단어(방출 확률 상위)\n",
        "    B = np.exp(hmm.log_B)\n",
        "    for k in range(n_states):\n",
        "        top_ids = np.argsort(-B[k])[:10]\n",
        "        top_words = [id2word[i] for i in top_ids]\n",
        "        print(f\"[State {k}] top words:\", \", \".join(top_words))\n",
        "\n",
        "    return hmm, sequences, word2id, id2word\n",
        "\n",
        "# ----------------------------\n",
        "# 6) 데모 실행\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    hmm, seqs, word2id, id2word = train_hmm_language_model(\n",
        "        n_states=8, min_freq=3, max_tokens=200_000, em_iters=6, seed=7\n",
        "    )\n",
        "\n",
        "    # 무조건 생성\n",
        "    print(\"\\n=== Unconditional Sample ===\")\n",
        "    print(hmm.sample_unconditional(id2word, length=25, seed=1))\n",
        "\n",
        "\n",
        "    # 시드 단어 조건 생성 (예: 'love')\n",
        "    seed_word = \"love\" if \"love\" in word2id else \"<unk>\"\n",
        "    print(\"\\n=== Conditioned on first word:\", seed_word, \"===\")\n",
        "    print(hmm.sample_conditioned_on_first_word(word2id[seed_word], id2word, length=25, seed=2))\n",
        "\n",
        "\n",
        "    # 예시 문장 로그우도(평가)\n",
        "    # example = [\"time\",\"flies\",\"like\",\"an\",\"arrow\"]\n",
        "    # example = [w if w in word2id else \"<unk>\" for w in example]\n",
        "    # seq = np.array([word2id[w] for w in example], dtype=np.int32)\n",
        "    # ll = hmm.sequence_loglik(seq)\n",
        "    # avg_nll = - ll / len(seq)\n",
        "    # print(f\"\\n#Example sentence: {' '.join(example)}\")\n",
        "    # print(f\"Log-likelihood: {ll:.3f}, avg NLL/token: {avg_nll:.3f}, perplexity ≈ {math.exp(avg_nll):.2f}\")\n"
      ],
      "metadata": {
        "id": "EQydgo7UBgT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8h129nZci8Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2000년대: 기계학습 기반 NLP\n"
      ],
      "metadata": {
        "id": "DG1KXzUrMqdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **핵심 설명**\n",
        "    - **SVM(Support Vector Machine)의 NLP 적용**\n",
        "        - 배경: 1995년 Vapnik이 개발한 SVM이 2000년대 초 텍스트 마이닝 분야에서 각광받기 시작했습니다. 고차원 희소 벡터로 표현되는 텍스트 데이터의 특성에 SVM이 매우 적합했기 때문입니다.\n",
        "        - **핵심 아이디어**:\n",
        "            - 텍스트를 고차원 벡터 공간으로 매핑\n",
        "            - 최대 마진을 갖는 결정 경계 찾기\n",
        "            - 커널 트릭을 통한 비선형 분류\n",
        "    - **HMM(Hidden Markov Model)의 발전**\n",
        "        - 배경: 1980년대부터 음성 인식에 사용되던 HMM이 2000년대에 품사 태깅, 개체명 인식 등으로 확장되었습니다.\n",
        "        - **한계와 극복**:\n",
        "            - 문제점: 관찰 독립성 가정, 라벨 편향 문제\n",
        "            - 해결책: CRF(Conditional Random Fields) 개발로 전역 최적화 가능\n"
      ],
      "metadata": {
        "id": "30NKnVWCdKET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : SVM을 이용한 텍스트 분류**\n",
        "\n",
        "- SVM(Support Vector Machine) 모델과 TF-IDF(Term Frequency-Inverse Document Frequency) 벡터화 기법을 사용하여 텍스트를 분류하는 과정을 보여주는 예제\n",
        "- 이 코드는 당시의 컴퓨팅 환경과 기술 수준을 반영하여, 텍스트 데이터의 특징을 추출하고 분류 모델을 훈련하는 과정을 보여줌"
      ],
      "metadata": {
        "id": "wIV39svoeCQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SVMTextClassifier2000s:\n",
        "    \"\"\"2000년대 스타일 SVM 텍스트 분류기: 선형 분\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.classifier = None\n",
        "        self.pipeline = None\n",
        "\n",
        "    def create_sample_dataset(self):\n",
        "        \"\"\"2000년대 스타일 뉴스 데이터셋 생성\"\"\"\n",
        "        # 실제 2000년대 뉴스 헤드라인 스타일\n",
        "        news_data = {\n",
        "            'technology': [\n",
        "                \"Apple introduces new iPod with video capabilities\",\n",
        "                \"Google launches Gmail with 1GB storage\",\n",
        "                \"Microsoft releases Windows XP Service Pack 2\",\n",
        "                \"Yahoo acquires Flickr photo sharing service\",\n",
        "                \"Mozilla Firefox browser gains market share\",\n",
        "                \"Intel announces dual-core processor technology\",\n",
        "                \"Sony PlayStation Portable hits the market\",\n",
        "                \"YouTube launches online video platform\",\n",
        "                \"Wikipedia becomes popular reference source\",\n",
        "                \"Broadband internet adoption increases rapidly\"\n",
        "            ],\n",
        "            'politics': [\n",
        "                \"President Bush announces new homeland security measures\",\n",
        "                \"Election results show divided political landscape\",\n",
        "                \"Congress debates healthcare reform legislation\",\n",
        "                \"International summit addresses climate change\",\n",
        "                \"Supreme Court ruling affects civil rights\",\n",
        "                \"Senator proposes new tax reform bill\",\n",
        "                \"Presidential candidate announces campaign strategy\",\n",
        "                \"Foreign policy experts discuss Middle East crisis\",\n",
        "                \"Governor signs education funding legislation\",\n",
        "                \"Political analysts predict election outcomes\"\n",
        "            ],\n",
        "            'sports': [\n",
        "                \"Lakers win NBA championship in overtime thriller\",\n",
        "                \"Olympic Games showcase international competition\",\n",
        "                \"World Cup final attracts global television audience\",\n",
        "                \"Baseball season ends with dramatic playoff series\",\n",
        "                \"Tennis tournament features top-ranked players\",\n",
        "                \"Football team advances to conference championship\",\n",
        "                \"Swimming records broken at international meet\",\n",
        "                \"Golf tournament decided on final hole\",\n",
        "                \"Basketball coach announces retirement plans\",\n",
        "                \"Soccer match ends in penalty shootout\"\n",
        "            ],\n",
        "            'entertainment': [\n",
        "                \"Hollywood movie breaks box office records\",\n",
        "                \"Music industry adapts to digital downloads\",\n",
        "                \"Television series finale draws huge audience\",\n",
        "                \"Celebrity couple announces engagement news\",\n",
        "                \"Film festival showcases independent cinema\",\n",
        "                \"Pop star releases highly anticipated album\",\n",
        "                \"Broadway show receives critical acclaim\",\n",
        "                \"Reality TV show becomes cultural phenomenon\",\n",
        "                \"Movie sequel exceeds original's success\",\n",
        "                \"Entertainment awards ceremony honors achievements\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # 데이터프레임 생성\n",
        "        texts = []\n",
        "        labels = []\n",
        "        for category, articles in news_data.items():\n",
        "            texts.extend(articles)\n",
        "            labels.extend([category] * len(articles))\n",
        "\n",
        "        return pd.DataFrame({'text': texts, 'category': labels})\n",
        "\n",
        "    def preprocess_text_2000s_style(self, texts):\n",
        "        \"\"\"2000년대 스타일 텍스트 전처리\"\"\"\n",
        "        # 당시에는 간단한 전처리만 수행\n",
        "        processed = []\n",
        "        for text in texts:\n",
        "            # 소문자 변환\n",
        "            text = text.lower()\n",
        "            # 간단한 정제 (특수문자 제거는 최소화)\n",
        "            import re\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            processed.append(text)\n",
        "        return processed\n",
        "\n",
        "    def train_svm_classifier(self, df):\n",
        "        \"\"\"SVM 분류기 훈련 (2000년대 방식)\"\"\"\n",
        "        # 텍스트 전처리\n",
        "        df['processed_text'] = self.preprocess_text_2000s_style(df['text'])\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_text'], df['category'],\n",
        "            test_size=0.3, random_state=42, stratify=df['category']\n",
        "        )\n",
        "\n",
        "        # 2000년대 스타일: TF-IDF + SVM\n",
        "        self.pipeline = Pipeline([\n",
        "            ('vectorizer', TfidfVectorizer(\n",
        "                max_features=1000,\n",
        "                ngram_range=(1, 2),\n",
        "                min_df=2,\n",
        "                max_df=0.8,\n",
        "                stop_words='english'\n",
        "            )),\n",
        "            ('classifier', SVC(\n",
        "                kernel='linear',\n",
        "                C=1.0,\n",
        "                random_state=42,\n",
        "                # 이 부분을 수정하여 확률 예측을 활성화합니다.\n",
        "                probability=True\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        # 모델 훈련\n",
        "        print(\"=== 2000년대 스타일 SVM 텍스트 분류기 훈련 ===\")\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # 예측 및 평가\n",
        "        y_pred = self.pipeline.predict(X_test)\n",
        "\n",
        "        print(\"\\n분류 성능 보고서:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # 교차 검증 (2000년대 표준 평가 방법)\n",
        "        cv_scores = cross_val_score(self.pipeline, X_train, y_train, cv=5)\n",
        "        print(f\"\\n5-Fold 교차검증 정확도: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "        return X_test, y_test, y_pred\n",
        "\n",
        "    def analyze_features(self):\n",
        "        \"\"\"특성 분석 (2000년대 방식)\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"먼저 모델을 훈련시켜주세요.\")\n",
        "            return\n",
        "\n",
        "        # 특성 이름과 가중치 추출\n",
        "        vectorizer = self.pipeline.named_steps['vectorizer']\n",
        "        classifier = self.pipeline.named_steps['classifier']\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        print(\"\\n=== 특성 분석 (SVM 가중치) ===\")\n",
        "\n",
        "        # 각 클래스별 중요 특성 출력\n",
        "        classes = classifier.classes_\n",
        "        for i, class_name in enumerate(classes):\n",
        "            print(f\"\\n'{class_name}' 클래스의 중요 특성:\")\n",
        "\n",
        "            # 해당 클래스에 대한 가중치\n",
        "            if hasattr(classifier, 'coef_'):\n",
        "                # 오직 이 부분만 수정되었습니다.\n",
        "                # .toarray()를 사용하여 희소 행렬을 밀집 행렬로 변환\n",
        "                weights = classifier.coef_[i].toarray().flatten() if len(classes) > 2 else classifier.coef_[0].toarray().flatten()\n",
        "\n",
        "                # 상위 10개 특성\n",
        "                top_indices = np.argsort(weights)[-10:][::-1]\n",
        "                for idx in top_indices:\n",
        "                    print(f\"  {feature_names[idx]}: {weights[idx]:.3f}\")\n",
        "\n",
        "# 실행 예제\n",
        "print(\"=== 2000년대 SVM 텍스트 분류 실습 ===\\n\")\n",
        "\n",
        "classifier = SVMTextClassifier2000s()\n",
        "\n",
        "# ----------------------------\n",
        "# 1. 데이터셋 생성\n",
        "# ----------------------------\n",
        "df = classifier.create_sample_dataset()\n",
        "print(\"생성된 데이터셋:\")\n",
        "print(df.groupby('category').size())\n",
        "print(\"\\n샘플 데이터:\")\n",
        "for category in df['category'].unique():\n",
        "    print(f\"\\n{category.upper()}:\")\n",
        "    sample = df[df['category'] == category]['text'].iloc[0]\n",
        "    print(f\"  {sample}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. SVM 모델 훈련 및 평가\n",
        "# ----------------------------\n",
        "X_test, y_test, y_pred = classifier.train_svm_classifier(df)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. 특성 분석\n",
        "# ----------------------------\n",
        "classifier.analyze_features()\n",
        "\n",
        "# ----------------------------\n",
        "# 4. 새로운 텍스트 분류 테스트\n",
        "# ----------------------------\n",
        "print(\"\\n=== 새로운 텍스트 분류 테스트 ===\")\n",
        "test_texts = [\n",
        "    \"Microsoft announces new software development kit\",\n",
        "    \"Basketball team wins championship game\",\n",
        "    \"Senator proposes new economic policy\",\n",
        "    \"Movie star wins academy award\"\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# 새로운 텍스트 분류\n",
        "# ----------------------------\n",
        "for text in test_texts:\n",
        "    processed = classifier.preprocess_text_2000s_style([text])\n",
        "\n",
        "    # 예측 카테고리\n",
        "    prediction = classifier.pipeline.predict(processed)[0]\n",
        "\n",
        "    # decision_function을 사용하여 신뢰도 점수 계산\n",
        "    decision_scores = classifier.pipeline.decision_function(processed)[0]\n",
        "\n",
        "    # 다중 클래스 분류의 경우, 가장 높은 점수를 신뢰도로 사용\n",
        "    max_score = np.max(decision_scores)\n",
        "\n",
        "    print(f\"\\n텍스트: {text}\")\n",
        "    print(f\"예측 카테고리: {prediction}\")\n",
        "    print(f\"신뢰도 점수 (decision_function): {max_score:.3f}\")\n",
        "\n",
        "# 신뢰도 점수 (decision_function): 예측된 샘플이 SVM의 결정 경계(decision boundary)로부터 얼마나 멀리 떨어져 있는지를 나타내는 값\n",
        "#  절대값이 클수록 해당 분류 결과에 대한 모델의 신뢰도가 높다는 의미"
      ],
      "metadata": {
        "id": "uaF0NK4BeElX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 : SVM을 이용한 스팸메일 분류"
      ],
      "metadata": {
        "id": "AMOS08iulX0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EmailFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"수정된 2000년대 스타일 이메일 특성 추출기\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.spam_keywords = [\n",
        "            'free', 'money', 'win', 'winner', 'cash', 'prize', 'offer',\n",
        "            'click', 'buy', 'sale', 'discount', 'viagra', 'cialis',\n",
        "            'mortgage', 'loan', 'credit', 'debt', 'investment',\n",
        "            'guarantee', 'urgent', 'act now', 'limited time'\n",
        "        ]\n",
        "\n",
        "        self.spam_patterns = [\n",
        "            r'\\$+', r'!{2,}', r'\\*+', r'#{2,}', r'%+',\n",
        "            r'[A-Z]{3,}', r'\\d+%', r'www\\.', r'http://',\n",
        "            r'\\.com', r'\\.net', r'\\.org'\n",
        "        ]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"이메일에서 수치 특성 추출\"\"\"\n",
        "        features = []\n",
        "\n",
        "        for email in X:\n",
        "            email_features = []\n",
        "            email_lower = email.lower()\n",
        "\n",
        "            # 안전한 특성 계산\n",
        "            try:\n",
        "                # 1. 스팸 키워드 빈도\n",
        "                spam_keyword_count = sum(email_lower.count(keyword) for keyword in self.spam_keywords)\n",
        "                email_features.append(float(spam_keyword_count))\n",
        "\n",
        "                # 2. 대문자 비율\n",
        "                if len(email) > 0:\n",
        "                    upper_ratio = sum(1 for c in email if c.isupper()) / len(email)\n",
        "                else:\n",
        "                    upper_ratio = 0.0\n",
        "                email_features.append(float(upper_ratio))\n",
        "\n",
        "                # 3. 특수문자 패턴 개수\n",
        "                special_pattern_count = sum(len(re.findall(pattern, email)) for pattern in self.spam_patterns)\n",
        "                email_features.append(float(special_pattern_count))\n",
        "\n",
        "                # 4. 숫자 비율\n",
        "                if len(email) > 0:\n",
        "                    digit_ratio = sum(1 for c in email if c.isdigit()) / len(email)\n",
        "                else:\n",
        "                    digit_ratio = 0.0\n",
        "                email_features.append(float(digit_ratio))\n",
        "\n",
        "                # 5. 평균 단어 길이\n",
        "                words = email_lower.split()\n",
        "                if len(words) > 0:\n",
        "                    avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "                else:\n",
        "                    avg_word_length = 0.0\n",
        "                email_features.append(float(avg_word_length))\n",
        "\n",
        "                # 6. 느낌표 개수\n",
        "                exclamation_count = email.count('!')\n",
        "                email_features.append(float(exclamation_count))\n",
        "\n",
        "                # 7. URL 개수\n",
        "                url_count = len(re.findall(r'http[s]?://|www\\.', email_lower))\n",
        "                email_features.append(float(url_count))\n",
        "\n",
        "                # 8. 이메일 주소 개수\n",
        "                email_count = len(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', email))\n",
        "                email_features.append(float(email_count))\n",
        "\n",
        "            except Exception as e:\n",
        "                # 오류 발생 시 기본값으로 설정\n",
        "                email_features = [0.0] * 8\n",
        "\n",
        "            features.append(email_features)\n",
        "\n",
        "        # numpy 배열로 변환 (희소 행렬 문제 방지)\n",
        "        return np.array(features, dtype=np.float64)\n",
        "\n",
        "class FixedSVMSpamFilter2000s:\n",
        "    \"\"\"수정된 2000년대 스타일 SVM 스팸 필터\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.text_feature_names = None\n",
        "        self.custom_feature_names = [\n",
        "            'spam_keywords', 'upper_ratio', 'special_patterns',\n",
        "            'digit_ratio', 'avg_word_length', 'exclamation_count',\n",
        "            'url_count', 'email_count'\n",
        "        ]\n",
        "\n",
        "    def create_2000s_email_dataset(self):\n",
        "        \"\"\"2000년대 스타일 이메일 데이터셋 생성\"\"\"\n",
        "\n",
        "        ham_emails = [\n",
        "            \"Hi John, hope you're doing well. Let's meet for coffee this weekend to discuss the project.\",\n",
        "            \"The quarterly report is ready for review. Please find the attached document with detailed analysis.\",\n",
        "            \"Thank you for your presentation yesterday. The team found it very informative and useful.\",\n",
        "            \"Reminder: Staff meeting tomorrow at 10 AM in the conference room. Agenda attached.\",\n",
        "            \"Great job on the client presentation! The feedback has been overwhelmingly positive.\",\n",
        "            \"Please review the contract terms and let me know if you have any questions or concerns.\",\n",
        "            \"The software update is scheduled for this weekend. Expect brief downtime on Sunday morning.\",\n",
        "            \"Congratulations on your promotion! Well deserved after all your hard work this year.\",\n",
        "            \"Can you send me the latest version of the budget spreadsheet when you have a moment?\",\n",
        "            \"The training session was very helpful. Looking forward to implementing these new techniques.\",\n",
        "            \"Happy birthday! Hope you have a wonderful day celebrating with family and friends.\",\n",
        "            \"The conference next month looks interesting. Would you like to attend together?\",\n",
        "            \"Please confirm your attendance for the team dinner on Friday evening at 7 PM.\",\n",
        "            \"The new hire orientation is scheduled for Monday. HR will send details soon.\",\n",
        "            \"Thanks for covering my shift yesterday. I really appreciate your help and flexibility.\"\n",
        "        ]\n",
        "\n",
        "        spam_emails = [\n",
        "            \"CONGRATULATIONS!!! You've WON $1,000,000!!! Click HERE NOW to claim your PRIZE!!!\",\n",
        "            \"URGENT: Your account will be closed! Click www.fakebank.com to verify information NOW!\",\n",
        "            \"FREE MONEY! Get $500 cash instantly! No credit check required! Act now!!!\",\n",
        "            \"VIAGRA 50% OFF! Best prices guaranteed! Order now and save BIG money!!!\",\n",
        "            \"You've been selected for a special offer! Buy now and get 90% discount!!!\",\n",
        "            \"MAKE $5000 A WEEK working from home! No experience needed! Start TODAY!!!\",\n",
        "            \"WARNING: Your computer is infected! Download our FREE antivirus software NOW!\",\n",
        "            \"Lose 20 pounds in 10 days! GUARANTEED results! Order our miracle pills now!\",\n",
        "            \"CREDIT PROBLEMS? No problem! Get approved for any loan instantly! Apply now!\",\n",
        "            \"FREE iPod! Just pay shipping and handling! Limited time offer - ACT FAST!!!\",\n",
        "            \"HOT SINGLES in your area want to meet you! Click here for instant access!\",\n",
        "            \"Your mortgage can be cut in HALF! Refinance now and save thousands!!!\",\n",
        "            \"FANTASTIC INVESTMENT OPPORTUNITY! Double your money in 30 days! Risk-free!\",\n",
        "            \"URGENT: You have unclaimed inheritance of $2.5 million! Contact us immediately!\",\n",
        "            \"CASINO BONUS: $200 FREE! No deposit required! Play now and win BIG!!!\"\n",
        "        ]\n",
        "\n",
        "        emails = ham_emails + spam_emails\n",
        "        labels = ['ham'] * len(ham_emails) + ['spam'] * len(spam_emails)\n",
        "\n",
        "        return pd.DataFrame({'email': emails, 'label': labels})\n",
        "\n",
        "    def preprocess_email(self, emails):\n",
        "        \"\"\"이메일 전처리\"\"\"\n",
        "        processed = []\n",
        "        for email in emails:\n",
        "            email = re.sub(r'[^\\w\\s@.-]', ' ', email)\n",
        "            email = re.sub(r'\\s+', ' ', email)\n",
        "            processed.append(email.strip())\n",
        "        return processed\n",
        "\n",
        "    def build_2000s_pipeline(self):\n",
        "        \"\"\"수정된 파이프라인 구성\"\"\"\n",
        "\n",
        "        # 텍스트 특성 파이프라인\n",
        "        text_pipeline = TfidfVectorizer(\n",
        "            max_features=500,  # 특성 수 줄임\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=1,\n",
        "            max_df=0.9,\n",
        "            stop_words='english',\n",
        "            lowercase=True\n",
        "        )\n",
        "\n",
        "        # 전체 파이프라인 (FeatureUnion 사용하지 않음)\n",
        "        self.pipeline = Pipeline([\n",
        "            ('tfidf', text_pipeline),\n",
        "            ('classifier', SVC(\n",
        "                kernel='linear',\n",
        "                C=1.0,\n",
        "                probability=True,\n",
        "                class_weight='balanced',\n",
        "                random_state=42\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def train_spam_filter(self, df):\n",
        "        \"\"\"스팸 필터 훈련\"\"\"\n",
        "        print(\"=== 수정된 2000년대 SVM 스팸 필터 훈련 ===\")\n",
        "\n",
        "        # 데이터 전처리\n",
        "        df['processed_email'] = self.preprocess_email(df['email'])\n",
        "\n",
        "        # 데이터 분할\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_email'], df['label'],\n",
        "            test_size=0.3, random_state=42, stratify=df['label']\n",
        "        )\n",
        "\n",
        "        # 파이프라인 구성 및 훈련\n",
        "        self.build_2000s_pipeline()\n",
        "        self.pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # 특성 이름 저장\n",
        "        self.text_feature_names = self.pipeline.named_steps['tfidf'].get_feature_names_out()\n",
        "\n",
        "        # 성능 평가\n",
        "        y_pred = self.pipeline.predict(X_test)\n",
        "        y_prob = self.pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        print(f\"훈련 데이터: {len(X_train)}개\")\n",
        "        print(f\"테스트 데이터: {len(X_test)}개\")\n",
        "        print(f\"스팸 비율: {(y_train == 'spam').mean():.2f}\")\n",
        "\n",
        "        print(\"\\n=== 성능 평가 ===\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # ROC AUC\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_test == 'spam', y_prob)\n",
        "            print(f\"ROC AUC: {roc_auc:.3f}\")\n",
        "        except:\n",
        "            print(\"ROC AUC 계산 실패\")\n",
        "\n",
        "        # 교차 검증\n",
        "        cv_scores = cross_val_score(self.pipeline, X_train, y_train, cv=5, scoring='f1_macro')\n",
        "        print(f\"5-Fold CV F1-Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "        return X_test, y_test, y_pred, y_prob\n",
        "\n",
        "    def analyze_spam_features_fixed(self):\n",
        "        \"\"\"수정된 스팸 탐지 특성 분석\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"먼저 모델을 훈련시켜주세요.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== 수정된 스팸 탐지 중요 특성 분석 ===\")\n",
        "\n",
        "        try:\n",
        "            # SVM 분류기 가져오기\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "\n",
        "            if not hasattr(classifier, 'coef_'):\n",
        "                print(\"선형 SVM이 아니므로 가중치를 분석할 수 없습니다.\")\n",
        "                return\n",
        "\n",
        "            # 가중치 추출 및 numpy 배열로 변환\n",
        "            coef_matrix = classifier.coef_\n",
        "            if hasattr(coef_matrix, 'toarray'):\n",
        "                weights = coef_matrix.toarray()[0]\n",
        "            else:\n",
        "                weights = np.array(coef_matrix[0]).flatten()\n",
        "\n",
        "            feature_names = self.text_feature_names\n",
        "\n",
        "            print(f\"총 특성 수: {len(weights)}\")\n",
        "            print(f\"특성 이름 수: {len(feature_names)}\")\n",
        "\n",
        "            # 안전한 인덱스 처리\n",
        "            max_features = min(len(weights), len(feature_names))\n",
        "\n",
        "            if max_features == 0:\n",
        "                print(\"분석할 특성이 없습니다.\")\n",
        "                return\n",
        "\n",
        "            # 스팸을 나타내는 상위 특성 (양수 가중치)\n",
        "            positive_indices = []\n",
        "            negative_indices = []\n",
        "\n",
        "            for i in range(max_features):\n",
        "                weight_val = float(weights[i])  # 스칼라로 변환\n",
        "                if weight_val > 0:\n",
        "                    positive_indices.append((i, weight_val))\n",
        "                elif weight_val < 0:\n",
        "                    negative_indices.append((i, weight_val))\n",
        "\n",
        "            # 정렬\n",
        "            positive_indices.sort(key=lambda x: x[1], reverse=True)\n",
        "            negative_indices.sort(key=lambda x: x[1])\n",
        "\n",
        "            print(\"\\n스팸을 강하게 나타내는 특성 (상위 10개):\")\n",
        "            for i, (idx, weight) in enumerate(positive_indices[:10]):\n",
        "                if idx < len(feature_names):\n",
        "                    print(f\"  {i+1}. {feature_names[idx]}: {weight:.3f}\")\n",
        "\n",
        "            print(\"\\n정상 이메일을 강하게 나타내는 특성 (상위 10개):\")\n",
        "            for i, (idx, weight) in enumerate(negative_indices[:10]):\n",
        "                if idx < len(feature_names):\n",
        "                    print(f\"  {i+1}. {feature_names[idx]}: {weight:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"특성 분석 중 오류 발생: {e}\")\n",
        "            print(\"기본 분석을 수행합니다.\")\n",
        "\n",
        "            # 기본 통계 정보\n",
        "            classifier = self.pipeline.named_steps['classifier']\n",
        "            print(f\"SVM 클래스: {classifier.classes_}\")\n",
        "            print(f\"서포트 벡터 수: {classifier.n_support_}\")\n",
        "\n",
        "    def test_new_emails_fixed(self):\n",
        "        \"\"\"수정된 새로운 이메일 테스트\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            print(\"먼저 모델을 훈련시켜주세요.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== 새로운 이메일 스팸 탐지 테스트 ===\")\n",
        "\n",
        "        test_emails = [\n",
        "            \"Hi Sarah, can we reschedule our meeting to Thursday afternoon? Thanks!\",\n",
        "            \"FREE CASH NOW!!! Click here to get $1000 instantly! NO QUESTIONS ASKED!!!\",\n",
        "            \"The project deadline has been moved to next Friday. Please update your schedules.\",\n",
        "            \"URGENT! Your account expires TODAY! Verify now at www.scamsite.com or lose access!\",\n",
        "            \"Lunch at the new Italian restaurant was great. Highly recommend their pasta dishes.\",\n",
        "            \"VIAGRA CIALIS 80% OFF! Order now and save BIG! Discreet shipping guaranteed!\",\n",
        "            \"Please find attached the updated employee handbook. HR policy changes included.\",\n",
        "            \"WIN A FREE IPHONE! Just pay $19.95 shipping! Limited time offer - Act NOW!\"\n",
        "        ]\n",
        "\n",
        "        for i, email in enumerate(test_emails, 1):\n",
        "            try:\n",
        "                processed = self.preprocess_email([email])\n",
        "                prediction = self.pipeline.predict(processed)[0]\n",
        "                probabilities = self.pipeline.predict_proba(processed)[0]\n",
        "\n",
        "                # 클래스 순서 확인\n",
        "                classes = self.pipeline.named_steps['classifier'].classes_\n",
        "                spam_idx = list(classes).index('spam') if 'spam' in classes else 1\n",
        "                spam_prob = probabilities[spam_idx]\n",
        "\n",
        "                print(f\"\\n{i}. 이메일: {email[:60]}...\")\n",
        "                print(f\"   예측: {prediction.upper()}\")\n",
        "                print(f\"   스팸 확률: {spam_prob:.3f}\")\n",
        "\n",
        "                confidence = \"높음\" if max(probabilities) > 0.8 else \"보통\" if max(probabilities) > 0.6 else \"낮음\"\n",
        "                print(f\"   신뢰도: {confidence}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n{i}. 이메일 처리 중 오류: {e}\")\n",
        "\n",
        "    def simple_performance_analysis(self, df):\n",
        "        \"\"\"간단한 성능 분석\"\"\"\n",
        "        print(\"\\n=== 간단한 성능 분석 ===\")\n",
        "\n",
        "        try:\n",
        "            # 기본 통계\n",
        "            print(f\"데이터셋 크기: {len(df)}\")\n",
        "            print(f\"스팸 비율: {(df['label'] == 'spam').mean():.2f}\")\n",
        "\n",
        "            # 모델 정보\n",
        "            if self.pipeline:\n",
        "                classifier = self.pipeline.named_steps['classifier']\n",
        "                print(f\"SVM 커널: {classifier.kernel}\")\n",
        "                print(f\"C 파라미터: {classifier.C}\")\n",
        "                print(f\"클래스: {classifier.classes_}\")\n",
        "\n",
        "                if hasattr(classifier, 'n_support_'):\n",
        "                    print(f\"서포트 벡터 수: {classifier.n_support_}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"분석 중 오류: {e}\")\n",
        "\n",
        "# 수정된 실행 예제\n",
        "print(\"=== 수정된 2000년대 SVM 스팸 이메일 필터 ===\\n\")\n",
        "\n",
        "# 1. 스팸 필터 생성\n",
        "spam_filter = FixedSVMSpamFilter2000s()\n",
        "\n",
        "# 2. 데이터셋 생성\n",
        "email_df = spam_filter.create_2000s_email_dataset()\n",
        "print(\"이메일 데이터셋:\")\n",
        "print(email_df['label'].value_counts())\n",
        "\n",
        "print(\"\\n샘플 이메일:\")\n",
        "ham_sample = email_df[email_df['label'] == 'ham']['email'].iloc[0]\n",
        "spam_sample = email_df[email_df['label'] == 'spam']['email'].iloc[0]\n",
        "print(f\"HAM: {ham_sample[:80]}...\")\n",
        "print(f\"SPAM: {spam_sample[:80]}...\")\n",
        "\n",
        "# 3. 스팸 필터 훈련\n",
        "try:\n",
        "    X_test, y_test, y_pred, y_prob = spam_filter.train_spam_filter(email_df)\n",
        "    print(\"\\n모델 훈련 성공!\")\n",
        "except Exception as e:\n",
        "    print(f\"훈련 중 오류: {e}\")\n",
        "\n",
        "# 4. 혼동 행렬\n",
        "try:\n",
        "    print(\"\\n=== 혼동 행렬 ===\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"혼동 행렬:\")\n",
        "    print(f\"        예측\")\n",
        "    print(f\"실제    HAM  SPAM\")\n",
        "    print(f\"HAM     {cm[0,0]:3d}   {cm[0,1]:3d}\")\n",
        "    print(f\"SPAM    {cm[1,0]:3d}   {cm[1,1]:3d}\")\n",
        "except Exception as e:\n",
        "    print(f\"혼동 행렬 생성 오류: {e}\")\n",
        "\n",
        "# 5. 수정된 특성 분석\n",
        "spam_filter.analyze_spam_features_fixed()\n",
        "\n",
        "# 6. 수정된 이메일 테스트\n",
        "spam_filter.test_new_emails_fixed()\n",
        "\n",
        "# 7. 간단한 성능 분석\n",
        "spam_filter.simple_performance_analysis(email_df)\n",
        "\n",
        "# 8. 2000년대 기술적 특징 요약\n",
        "print(\"\\n=== 2000년대 SVM 스팸 필터 기술적 특징 ===\")\n",
        "print(\"장점:\")\n",
        "print(\"  • 고차원 텍스트 데이터에 효과적\")\n",
        "print(\"  • 마진 최대화로 일반화 성능 우수\")\n",
        "print(\"  • 확률 기반 신뢰도 제공\")\n",
        "print(\"  • 비교적 적은 메모리 사용\")\n",
        "\n",
        "print(\"\\n한계점:\")\n",
        "print(\"  • 대용량 데이터 처리 속도 느림\")\n",
        "print(\"  • 특성 공학에 과도하게 의존\")\n",
        "print(\"  • 순차 정보 손실\")\n",
        "print(\"  • 하이퍼파라미터 튜닝 필요\")\n",
        "\n",
        "print(\"\\n당시 실제 성능:\")\n",
        "print(\"  • 정확도: 95-98%\")\n",
        "print(\"  • 오탐률: 1-3%\")\n",
        "print(\"  • 처리속도: 1000+ 이메일/초\")"
      ],
      "metadata": {
        "id": "Ynsev-Zdldx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 : HMM을 이용한 품사 태깅"
      ],
      "metadata": {
        "id": "tmp4RxUSeM2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 필요한 NLTK 데이터 다운로드\n",
        "try:\n",
        "    nltk.data.find('corpora/treebank')\n",
        "except LookupError:\n",
        "    nltk.download('treebank', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "class ImprovedHMM2000s:\n",
        "    \"\"\"2000년대 개선된 HMM 품사 태거\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing_parameter=0.01):\n",
        "        self.smoothing = smoothing_parameter\n",
        "        self.states = set()  # 품사 태그들\n",
        "        self.observations = set()  # 단어들\n",
        "        self.initial_prob = defaultdict(float)\n",
        "        self.transition_prob = defaultdict(lambda: defaultdict(float))\n",
        "        self.emission_prob = defaultdict(lambda: defaultdict(float))\n",
        "        self.word_counts = defaultdict(int)\n",
        "        self.tag_counts = defaultdict(int)\n",
        "        self.tag_word_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def create_2000s_training_data(self):\n",
        "        \"\"\"2000년대 스타일 훈련 데이터 생성\"\"\"\n",
        "        from nltk.corpus import treebank\n",
        "\n",
        "        # Treebank 코퍼스 사용 (2000년대 표준)\n",
        "        tagged_sentences = treebank.tagged_sents()\n",
        "\n",
        "        # 데이터 전처리 (2000년대 방식)\n",
        "        processed_sentences = []\n",
        "        for sentence in tagged_sentences[:1000]:  # 제한된 데이터\n",
        "            processed_sent = []\n",
        "            for word, tag in sentence:\n",
        "                # 간단한 정규화\n",
        "                word = word.lower()\n",
        "                # 숫자는 특별 토큰으로 처리\n",
        "                if word.isdigit():\n",
        "                    word = '<NUM>'\n",
        "                # 희소한 단어는 <UNK>로 처리\n",
        "                elif len(word) == 1 and not word.isalpha():\n",
        "                    word = '<PUNCT>'\n",
        "\n",
        "                processed_sent.append((word, tag))\n",
        "\n",
        "            if processed_sent:  # 빈 문장 제외\n",
        "                processed_sentences.append(processed_sent)\n",
        "\n",
        "        return processed_sentences\n",
        "\n",
        "    def train(self, tagged_sentences):\n",
        "        \"\"\"HMM 모델 훈련 (스무딩 적용)\"\"\"\n",
        "        print(\"=== 2000년대 개선된 HMM 모델 훈련 ===\")\n",
        "\n",
        "        # 1. 통계 수집\n",
        "        for sentence in tagged_sentences:\n",
        "            prev_tag = '<START>'\n",
        "\n",
        "            for word, tag in sentence:\n",
        "                # 관찰 및 상태 집합 구축\n",
        "                self.observations.add(word)\n",
        "                self.states.add(tag)\n",
        "\n",
        "                # 카운트 수집\n",
        "                self.word_counts[word] += 1\n",
        "                self.tag_counts[tag] += 1\n",
        "                self.tag_word_counts[tag][word] += 1\n",
        "\n",
        "                # 초기 확률 (문장 첫 단어)\n",
        "                if prev_tag == '<START>':\n",
        "                    self.initial_prob[tag] += 1\n",
        "\n",
        "                # 전이 확률\n",
        "                self.transition_prob[prev_tag][tag] += 1\n",
        "\n",
        "                prev_tag = tag\n",
        "\n",
        "            # 문장 끝 전이\n",
        "            self.transition_prob[prev_tag]['<END>'] += 1\n",
        "\n",
        "        # 2. 확률 정규화 (라플라스 스무딩 적용)\n",
        "        self._normalize_probabilities()\n",
        "\n",
        "        print(f\"훈련 완료:\")\n",
        "        print(f\"  - 상태(태그) 수: {len(self.states)}\")\n",
        "        print(f\"  - 관찰(단어) 수: {len(self.observations)}\")\n",
        "        print(f\"  - 훈련 문장 수: {len(tagged_sentences)}\")\n",
        "\n",
        "    def _normalize_probabilities(self):\n",
        "        \"\"\"확률 정규화 및 스무딩\"\"\"\n",
        "        # 초기 확률 정규화\n",
        "        total_initial = sum(self.initial_prob.values())\n",
        "        for tag in self.initial_prob:\n",
        "            self.initial_prob[tag] /= total_initial\n",
        "\n",
        "        # 전이 확률 정규화 (라플라스 스무딩)\n",
        "        for prev_tag in self.transition_prob:\n",
        "            total_transitions = sum(self.transition_prob[prev_tag].values())\n",
        "            vocab_size = len(self.states) + 1  # +1 for <END>\n",
        "\n",
        "            for next_tag in self.transition_prob[prev_tag]:\n",
        "                count = self.transition_prob[prev_tag][next_tag]\n",
        "                # 라플라스 스무딩 적용\n",
        "                self.transition_prob[prev_tag][next_tag] = \\\n",
        "                    (count + self.smoothing) / (total_transitions + self.smoothing * vocab_size)\n",
        "\n",
        "        # 방출 확률 정규화 (라플라스 스무딩)\n",
        "        for tag in self.tag_word_counts:\n",
        "            total_emissions = sum(self.tag_word_counts[tag].values())\n",
        "            vocab_size = len(self.observations)\n",
        "\n",
        "            for word in self.tag_word_counts[tag]:\n",
        "                count = self.tag_word_counts[tag][word]\n",
        "                # 라플라스 스무딩 적용\n",
        "                self.emission_prob[tag][word] = \\\n",
        "                    (count + self.smoothing) / (total_emissions + self.smoothing * vocab_size)\n",
        "\n",
        "    def viterbi_decode(self, sentence):\n",
        "        \"\"\"비터비 알고리즘을 이용한 디코딩\"\"\"\n",
        "        words = [word.lower() for word in sentence]\n",
        "        n = len(words)\n",
        "\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        # 미지 단어 처리\n",
        "        processed_words = []\n",
        "        for word in words:\n",
        "            if word not in self.observations:\n",
        "                if word.isdigit():\n",
        "                    processed_words.append('<NUM>')\n",
        "                elif len(word) == 1 and not word.isalpha():\n",
        "                    processed_words.append('<PUNCT>')\n",
        "                else:\n",
        "                    processed_words.append('<UNK>')\n",
        "            else:\n",
        "                processed_words.append(word)\n",
        "\n",
        "        # 동적 계획법 테이블\n",
        "        dp = defaultdict(lambda: defaultdict(float))\n",
        "        backpointer = defaultdict(lambda: defaultdict(str))\n",
        "\n",
        "        # 초기화\n",
        "        for tag in self.states:\n",
        "            init_prob = self.initial_prob.get(tag, self.smoothing)\n",
        "            emission_prob = self._get_emission_prob(tag, processed_words[0])\n",
        "            dp[0][tag] = np.log(init_prob) + np.log(emission_prob)\n",
        "\n",
        "        # 순방향 단계\n",
        "        for t in range(1, n):\n",
        "            for curr_tag in self.states:\n",
        "                max_prob = float('-inf')\n",
        "                best_prev_tag = None\n",
        "\n",
        "                for prev_tag in self.states:\n",
        "                    trans_prob = self._get_transition_prob(prev_tag, curr_tag)\n",
        "                    emission_prob = self._get_emission_prob(curr_tag, processed_words[t])\n",
        "\n",
        "                    prob = dp[t-1][prev_tag] + np.log(trans_prob) + np.log(emission_prob)\n",
        "\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        best_prev_tag = prev_tag\n",
        "\n",
        "                dp[t][curr_tag] = max_prob\n",
        "                backpointer[t][curr_tag] = best_prev_tag\n",
        "\n",
        "        # 최적 경로 역추적\n",
        "        best_path = [''] * n\n",
        "\n",
        "        # 마지막 태그 찾기\n",
        "        max_prob = float('-inf')\n",
        "        for tag in self.states:\n",
        "            if dp[n-1][tag] > max_prob:\n",
        "                max_prob = dp[n-1][tag]\n",
        "                best_path[n-1] = tag\n",
        "\n",
        "        # 역방향 추적\n",
        "        for t in range(n-2, -1, -1):\n",
        "            best_path[t] = backpointer[t+1][best_path[t+1]]\n",
        "\n",
        "        return list(zip(words, best_path))\n",
        "\n",
        "    def _get_transition_prob(self, prev_tag, curr_tag):\n",
        "        \"\"\"전이 확률 조회 (스무딩 적용)\"\"\"\n",
        "        if prev_tag in self.transition_prob and curr_tag in self.transition_prob[prev_tag]:\n",
        "            return self.transition_prob[prev_tag][curr_tag]\n",
        "        else:\n",
        "            # 미관찰 전이에 대한 스무딩\n",
        "            total_transitions = sum(self.transition_prob[prev_tag].values()) if prev_tag in self.transition_prob else 0\n",
        "            vocab_size = len(self.states)\n",
        "            return self.smoothing / (total_transitions + self.smoothing * vocab_size)\n",
        "\n",
        "    def _get_emission_prob(self, tag, word):\n",
        "        \"\"\"방출 확률 조회 (미지 단어 처리)\"\"\"\n",
        "        if tag in self.emission_prob and word in self.emission_prob[tag]:\n",
        "            return self.emission_prob[tag][word]\n",
        "        else:\n",
        "            # 미관찰 단어에 대한 스무딩\n",
        "            total_emissions = sum(self.tag_word_counts[tag].values()) if tag in self.tag_word_counts else 1\n",
        "            vocab_size = len(self.observations)\n",
        "            return self.smoothing / (total_emissions + self.smoothing * vocab_size)\n",
        "\n",
        "    def evaluate(self, test_sentences):\n",
        "        \"\"\"모델 평가\"\"\"\n",
        "        all_true_tags = []\n",
        "        all_pred_tags = []\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for sentence in test_sentences:\n",
        "            words = [word for word, tag in sentence]\n",
        "            true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "            pred_result = self.viterbi_decode(words)\n",
        "            pred_tags = [tag for word, tag in pred_result]\n",
        "\n",
        "            all_true_tags.extend(true_tags)\n",
        "            all_pred_tags.extend(pred_tags)\n",
        "\n",
        "            # 문장별 정확도\n",
        "            for true_tag, pred_tag in zip(true_tags, pred_tags):\n",
        "                if true_tag == pred_tag:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "        print(f\"\\n=== HMM 모델 평가 결과 ===\")\n",
        "        print(f\"전체 정확도: {accuracy:.4f}\")\n",
        "        print(f\"올바른 예측: {correct}/{total}\")\n",
        "\n",
        "        return accuracy, all_true_tags, all_pred_tags\n",
        "\n",
        "# 실행 예제\n",
        "print(\"=== 2000년대 개선된 HMM 품사 태깅 실습 ===\\n\")\n",
        "\n",
        "# 1. HMM 모델 생성 및 훈련\n",
        "hmm_model = ImprovedHMM2000s(smoothing_parameter=0.01)\n",
        "\n",
        "# 훈련 데이터 준비\n",
        "training_data = hmm_model.create_2000s_training_data()\n",
        "print(f\"훈련 데이터: {len(training_data)}개 문장\")\n",
        "\n",
        "# 모델 훈련\n",
        "hmm_model.train(training_data[:800])  # 80% 훈련용\n",
        "\n",
        "# 2. 테스트 데이터로 평가\n",
        "test_data = training_data[800:]  # 20% 테스트용\n",
        "accuracy, true_tags, pred_tags = hmm_model.evaluate(test_data)\n",
        "\n",
        "# 3. 실제 문장 태깅 테스트\n",
        "print(\"\\n=== 실제 문장 태깅 테스트 ===\")\n",
        "test_sentences = [\n",
        "    [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"],\n",
        "    [\"Machine\", \"learning\", \"is\", \"very\", \"interesting\"],\n",
        "    [\"I\", \"am\", \"studying\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"Google\", \"released\", \"a\", \"new\", \"algorithm\"]\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result = hmm_model.viterbi_decode(sentence)\n",
        "    print(f\"\\n문장: {' '.join(sentence)}\")\n",
        "    print(\"태깅 결과:\")\n",
        "    for word, tag in result:\n",
        "        print(f\"  {word} → {tag}\")\n",
        "\n",
        "# 4. 성능 비교 (NLTK 기본 태거와 비교)\n",
        "print(\"\\n=== 성능 비교 (NLTK vs 개선된 HMM) ===\")\n",
        "sample_sentence = [\"The\", \"company\", \"announced\", \"new\", \"product\", \"development\"]\n",
        "\n",
        "# NLTK 기본 태거\n",
        "try:\n",
        "    nltk_result = nltk.pos_tag(sample_sentence)\n",
        "    print(f\"NLTK 결과: {nltk_result}\")\n",
        "except:\n",
        "    print(\"NLTK 태거 사용 불가\")\n",
        "\n",
        "# 개선된 HMM 결과\n",
        "hmm_result = hmm_model.viterbi_decode(sample_sentence)\n",
        "print(f\"개선된 HMM: {hmm_result}\")"
      ],
      "metadata": {
        "id": "9rla1jF7ePQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제: SVM를 이용한 문장 생성\n",
        "- SVM은 생성 모델이 아니라 판별 모델,\n",
        "- 이전 단어들(컨텍스트) → 다음 단어를 분류하는 방식으로 학습한 뒤,\n",
        "- 예측을 반복하여 문장을 생성\n",
        "- **[주의!] 실행시간이 오래 걸림 --> 코랩에서는 RAM을 모두 사용 후 세션이 다운 될 수 있음**"
      ],
      "metadata": {
        "id": "62yvo0u6bIw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주의! 실행 시간이 오래 걸릴 수 있다.\n",
        "# SVM(LinearSVC)로 \"다음 단어 예측\"을 반복하여 문장 생성\n",
        "# 데이터: Project Gutenberg #1342 (Pride and Prejudice)\n",
        "\n",
        "import re, requests, numpy as np, math, random\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import softmax\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 데이터 수집/전처리\n",
        "# -----------------------------\n",
        "URL = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
        "\n",
        "def fetch_text(url=URL):\n",
        "    return requests.get(url).text\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    # 단어 + 문장부호(.?!)\n",
        "    toks = re.findall(r\"\\b\\w+\\b|[.!?]\", text.lower())\n",
        "    sents, cur = [], []\n",
        "    for t in toks:\n",
        "        if t in [\".\", \"!\", \"?\"]:\n",
        "            if cur:\n",
        "                sents.append(cur)\n",
        "                cur = []\n",
        "        else:\n",
        "            cur.append(t)\n",
        "    if cur:\n",
        "        sents.append(cur)\n",
        "    return sents\n",
        "\n",
        "def build_vocab(sentences, top_k=20000, min_freq=2):\n",
        "    freq = Counter(w for s in sentences for w in s)\n",
        "    # 빈도 기준 필터 → 상위 top_k\n",
        "    candidates = [w for w, c in freq.items() if c >= min_freq]\n",
        "    candidates.sort(key=lambda w: freq[w], reverse=True)\n",
        "    vocab = set(candidates[:top_k])\n",
        "    vocab |= {\"<unk>\", \"<s>\"}            # 특수 토큰\n",
        "    return vocab, freq\n",
        "\n",
        "def apply_unk(sentences, vocab):\n",
        "    return [[w if w in vocab else \"<unk>\" for w in s] for s in sentences]\n",
        "\n",
        "def add_sentence_markers(sentences):\n",
        "    # 각 문장 앞에 시작 토큰 <s>를 두 번 붙여 컨텍스트 2개 확보\n",
        "    return [[\"<s>\", \"<s>\"] + s for s in sentences if len(s) > 0]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) 데이터셋 구성: (w-2, w-1) -> w0\n",
        "# -----------------------------\n",
        "def make_dataset(sentences):\n",
        "    X_feats = []\n",
        "    y_words = []\n",
        "    for s in sentences:\n",
        "        # s: [\"<s>\",\"<s>\", w0, w1, ...]\n",
        "        for i in range(2, len(s)):\n",
        "            w_2, w_1, w0 = s[i-2], s[i-1], s[i]\n",
        "            # 특징은 dict로 구성 (해싱으로 변환 예정)\n",
        "            feats = {\n",
        "                f\"w-2={w_2}\": 1,\n",
        "                f\"w-1={w_1}\": 1,\n",
        "                f\"bigram={w_2}|{w_1}\": 1,\n",
        "            }\n",
        "            X_feats.append(feats)\n",
        "            y_words.append(w0)\n",
        "    return X_feats, y_words\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 학습 파이프라인\n",
        "# -----------------------------\n",
        "def train_svm_nextword(\n",
        "    top_k_vocab=20000,\n",
        "    min_freq=2,\n",
        "    n_features=2**18,  # FeatureHasher 차원\n",
        "    test_size=0.05,\n",
        "    random_state=42\n",
        "):\n",
        "    print(\"Downloading text ...\")\n",
        "    text = fetch_text(URL)\n",
        "\n",
        "    print(\"Sentence tokenizing ...\")\n",
        "    sents = sentence_tokenize(text)\n",
        "    print(f\"Total sentences: {len(sents)}\")\n",
        "\n",
        "    print(\"Building vocab ...\")\n",
        "    vocab, freq = build_vocab(sents, top_k=top_k_vocab, min_freq=min_freq)\n",
        "\n",
        "    print(\"Applying <unk> and adding <s> markers ...\")\n",
        "    sents = apply_unk(sents, vocab)\n",
        "    sents = add_sentence_markers(sents)\n",
        "\n",
        "    print(\"Building (context -> next word) dataset ...\")\n",
        "    X_feats, y_words = make_dataset(sents)\n",
        "\n",
        "    # 해싱 특징 → 희소행렬\n",
        "    hasher = FeatureHasher(n_features=n_features, input_type=\"dict\")\n",
        "    X = hasher.transform(X_feats)\n",
        "\n",
        "    # 레이블 인코딩\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_words)\n",
        "\n",
        "    # 데이터 분할\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"Training LinearSVC (multi-class) ...\")\n",
        "    clf = LinearSVC(random_state=random_state, dual=\"auto\")\n",
        "    clf.fit(X_tr, y_tr)\n",
        "\n",
        "    acc = clf.score(X_te, y_te)\n",
        "    print(f\"Test accuracy (next-word, top-1): {acc:.4f}\")\n",
        "    return clf, hasher, le, vocab\n",
        "\n",
        "# -----------------------------\n",
        "# 4) 문장 생성\n",
        "# -----------------------------\n",
        "def context_to_features(w_2, w_1):\n",
        "    return {f\"w-2={w_2}\": 1, f\"w-1={w_1}\": 1, f\"bigram={w_2}|{w_1}\": 1}\n",
        "\n",
        "def generate_with_svm(\n",
        "    clf, hasher, le, vocab,\n",
        "    seed_word=\"love\",\n",
        "    max_len=30,\n",
        "    temperature=0.8,   # 낮출수록 결정적, 높일수록 랜덤\n",
        "    top_k=10,          # 상위 k 후보에서 샘플\n",
        "    end_tokens=(\".\", \"!\", \"?\")\n",
        "):\n",
        "    # 시작 컨텍스트: <s>, seed\n",
        "    w_2, w_1 = \"<s>\", (seed_word if seed_word in vocab else \"<unk>\")\n",
        "    result = [w_1]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        feats = context_to_features(w_2, w_1)\n",
        "        X = hasher.transform([feats])\n",
        "        scores = clf.decision_function(X)           # shape: (1, n_classes)\n",
        "        scores = np.asarray(scores).ravel()\n",
        "\n",
        "        # 상위 k 후보\n",
        "        top_idx = np.argsort(-scores)[:top_k]\n",
        "        top_scores = scores[top_idx]\n",
        "\n",
        "        # SVM 점수 → 확률 비슷하게 샘플링 (softmax with temperature)\n",
        "        probs = softmax(top_scores / max(temperature, 1e-6))\n",
        "        next_label = np.random.choice(top_idx, p=probs)\n",
        "        next_word = le.inverse_transform([next_label])[0]\n",
        "\n",
        "        result.append(next_word)\n",
        "\n",
        "        # 종료조건: 문장부호 예측 시 종료\n",
        "        if next_word in end_tokens:\n",
        "            break\n",
        "\n",
        "        # 컨텍스트 이동\n",
        "        w_2, w_1 = w_1, next_word\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) 실행\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    clf, hasher, le, vocab = train_svm_nextword(\n",
        "        top_k_vocab=20000, min_freq=2, n_features=2**18, test_size=0.05\n",
        "    )\n",
        "\n",
        "    # 제시어: love\n",
        "    for i in range(3):\n",
        "        s = generate_with_svm(\n",
        "            clf, hasher, le, vocab,\n",
        "            seed_word=\"love\",\n",
        "            max_len=30,\n",
        "            temperature=0.9,\n",
        "            top_k=8\n",
        "        )\n",
        "        print(f\"[Gen {i+1}] {s}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "90w0EqVCZF3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}