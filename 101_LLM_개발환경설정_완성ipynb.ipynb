{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "5ADhlYgBTKT8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM 개발 환경설정"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡**참고 교재**\n",
        "    - 핸즈온LLM https://www.hanbit.co.kr/store/books/look.php?p_code=B2599445240\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#파이썬 버전 확인"
      ],
      "metadata": {
        "id": "vjJh4IMODeSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Colab"
      ],
      "metadata": {
        "id": "jQHx8oG7Div4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ERcAduBZ1q"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (Local) win/mac"
      ],
      "metadata": {
        "id": "QY5G4ozHDlhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "py --version"
      ],
      "metadata": {
        "id": "EQydgo7UBgT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "py --list   # 설치된 모든 파이썬 버전 확인"
      ],
      "metadata": {
        "id": "SFT0IbQzD8HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "py -3.12    # 특정 버전 실행 : Python 3.12 실행"
      ],
      "metadata": {
        "id": "sT7pbhyvEERL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if sys.version_info < (3, 9):\n",
        "    print(\"Python 3.9 미만 버전이네요. 업그레이드 필요!\")\n",
        "else:\n",
        "    print(\"최신 버전이네요!\")\n"
      ],
      "metadata": {
        "id": "YXkIjui2ThB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "print(f\"현재 파이썬 버전은 🚀 {platform.python_version()} 🚀 입니다\")\n"
      ],
      "metadata": {
        "id": "TqkF-qErTcPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8h129nZci8Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 대표 라이브러리 설치\n"
      ],
      "metadata": {
        "id": "DG1KXzUrMqdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **transformers**\n",
        "\n",
        "\n",
        "- **Hugging Face가 개발한 오픈소스 라이브러리(Transformer 모델 라이브러리)**\n",
        "- 자연어 처리, 컴퓨터 비전, 음성 등 다양한 분야의 최신 Transformer 기반 사전 학습된(pretrained) 모델을 쉽고 빠르게 사용할 수 있도록 지원함\n",
        "- 3개의 딥러닝 라이브러리 지원 :  PyTorch, TensorFlow, Jax\n",
        "- 상세설명 : https://pypi.org/project/transformers/\n"
      ],
      "metadata": {
        "id": "cCx9g4beNOpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **지원 플랫폼 / 요구사항**\n",
        "    - Python 3.9 이상, PyTorch 2.1+, TensorFlow 2.6+, Flax 0.4.1+\n",
        "- **용도 / 특징**\n",
        "    - 텍스트, 이미지, 오디오, 멀티모달 모델들처럼 다양한 영역의 pretrained 모델을 간단한 API(pipeline)로 빠르게 사용 가능\n",
        "- **추천 사용 케이스**\n",
        "    - 빠르고 쉽게 NLP, 컴퓨터 비전, 음성 등의 사전학습 모델을 사용하거나 fine-tuning 하고자 할 때"
      ],
      "metadata": {
        "id": "2SIij8mNSmaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **설치 방법**"
      ],
      "metadata": {
        "id": "_9dDQ0-2SDH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치된 transformers 버전 확인  -->(ex: Version: 4.55.4)\n",
        "!pip show transformers | grep Version"
      ],
      "metadata": {
        "id": "YUQOtVxdYnhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gioKeW2nK2zX"
      },
      "outputs": [],
      "source": [
        "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 설치된 PyTorch 버전 확인  -->(ex: Version: 2.8.0+cu126 (+cu126-->CUDA 12.6 버전 지원))\n",
        "!pip show torch | grep Version\n",
        "\n",
        "# pip install torch==2.8.0"
      ],
      "metadata": {
        "id": "iBbbJcLvujq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **QuickTour**"
      ],
      "metadata": {
        "id": "lNBhIvnzPGYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **감정분석**"
      ],
      "metadata": {
        "id": "I-yv_vjYTEn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정 분석 파이프라인\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "print(classifier('We are very happy to introduce pipeline to the transformers repository.'))"
      ],
      "metadata": {
        "id": "kbODkmM0PL28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "print(nlp(\"이 강의는 정말 재미있어요!\"))\n"
      ],
      "metadata": {
        "id": "2Dqi289eTARx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **이미지 객체 감지**"
      ],
      "metadata": {
        "id": "5ADhlYgBTKT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import pipeline\n",
        "\n",
        "# 이미지 다운로드\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
        "image_data = requests.get(url, stream=True).raw\n",
        "image = Image.open(image_data)\n",
        "\n",
        "# 객체 감지 파이프라인\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "results = object_detector(image)\n",
        "\n",
        "# 원본 이미지에 바운딩 박스 그리기\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for result in results:\n",
        "    box = result[\"box\"]\n",
        "    label = result[\"label\"]\n",
        "    score = result[\"score\"]\n",
        "\n",
        "    # 박스 좌표\n",
        "    x1, y1, x2, y2 = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
        "\n",
        "    # 박스 그리기\n",
        "    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\", width=3)\n",
        "    # 레이블 + 점수 표시\n",
        "    draw.text((x1, y1 - 10), f\"{label} {score:.2f}\", fill=\"yellow\")\n",
        "\n",
        "# 결과 출력 (Colab 환경에서 시각화)\n",
        "image.show()   # 로컬 환경이면 새 창\n",
        "display(image) # Colab/Jupyter 환경이면 노트북 안에 표시\n"
      ],
      "metadata": {
        "id": "8GYQrUE5pBmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **기계 번역**\n",
        "- 💡주의: 모델 다운로드시 오래 걸릴 수 있다."
      ],
      "metadata": {
        "id": "BEJAb5wotaKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Access Token 없이 공개 모델 다운로드\n"
      ],
      "metadata": {
        "id": "0D3tsNJNulQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인증이 필요하지 않은 대체 모델들\n",
        "from transformers import pipeline\n",
        "\n",
        "# 옵션 1: Facebook의 NLLB 모델 (추천)\n",
        "translator = pipeline(\n",
        "    'translation',\n",
        "    model='facebook/nllb-200-distilled-600M',\n",
        "    src_lang='eng_Latn',\n",
        "    tgt_lang='kor_Hang',\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "text = \"Hello, how are you today?\"\n",
        "result = translator(text)\n",
        "print(f\"원문: {text}\")\n",
        "print(f\"번역: {result[0]['translation_text']}\")"
      ],
      "metadata": {
        "id": "5-h39UC3uZXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import builtins\n",
        "input = builtins.input  # input 함수를 원래대로 복구\n",
        "\n",
        "while True:\n",
        "    text = input('한글로 번역할 문장 입력(\"q\"): ')\n",
        "    if (text == 'q') :\n",
        "        break\n",
        "    result = translator(text)\n",
        "    print(f\"원문: {text}\")\n",
        "    print(f\"번역: {result[0]['translation_text']}\")\n",
        "    print( )\n",
        "print('번역 종료!')"
      ],
      "metadata": {
        "id": "a1JcGyQgwTM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Access Token 넣고 비공개 모델 접근"
      ],
      "metadata": {
        "id": "Wf2cYI1kurU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1단계: Hugging Face 토큰 생성 및 설정\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Google Colab에서 토큰 입력 (한 번만 실행)\n",
        "# https://huggingface.co/settings/tokens 에서 토큰 생성 후 입력\n",
        "login()\n",
        "\n",
        "# 또는 직접 토큰 입력 (보안상 권장하지 않음)\n",
        "#os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "BzQU4DQWusb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print(\"=== Helsinki-NLP 영한 번역 모델 로딩 ===\")\n",
        "\n",
        "# 올바른 영어→한국어 번역 모델\n",
        "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-tc-big-en-ko')\n",
        "\n",
        "print(\"✅ 모델 로딩 완료!\")\n",
        "\n",
        "# 번역 실행\n",
        "text = \"Hello, how are you today?\"\n",
        "result = translator(text)\n",
        "print(f\"원문: {text}\")\n",
        "print(f\"번역: {result[0]['translation_text']}\")"
      ],
      "metadata": {
        "id": "YzjQSo2lteru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡[주의] 실행속도 느림(전체 실행 5분정도 걸림)"
      ],
      "metadata": {
        "id": "3tWuei7G8Wn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import time\n",
        "import re\n",
        "\n",
        "class HighQualityTranslator:\n",
        "    \"\"\"고품질 양방향 번역기\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.translator = None\n",
        "        self.model_info = {}\n",
        "        self.translation_count = 0\n",
        "        self._initialize_best_model()\n",
        "\n",
        "    def _initialize_best_model(self):\n",
        "        \"\"\"최고 성능 모델 초기화\"\"\"\n",
        "\n",
        "        print(\"🚀 고품질 번역기 초기화 중...\")\n",
        "\n",
        "        # 실제 작동하는 고품질 모델들 (우선순위 순)\n",
        "        quality_models = [\n",
        "            {\n",
        "                'name': 'facebook/m2m100_1.2B',\n",
        "                'description': 'Facebook M2M100 대형 모델 (최고 품질)',\n",
        "                'size': '1.2GB',\n",
        "                'type': 'm2m100_large'\n",
        "            },\n",
        "            {\n",
        "                'name': 'facebook/m2m100_418M',\n",
        "                'description': 'Facebook M2M100 중형 모델 (균형)',\n",
        "                'size': '418MB',\n",
        "                'type': 'm2m100'\n",
        "            },\n",
        "            {\n",
        "                'name': 'facebook/nllb-200-distilled-600M',\n",
        "                'description': 'Facebook NLLB-200 축약 모델',\n",
        "                'size': '600MB',\n",
        "                'type': 'nllb'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for model_config in quality_models:\n",
        "            try:\n",
        "                print(f\"🔄 시도: {model_config['description']}\")\n",
        "\n",
        "                start_time = time.time()\n",
        "\n",
        "                if 'm2m100' in model_config['type']:\n",
        "                    # M2M100 모델 (가장 안정적)\n",
        "                    self.translator = pipeline(\n",
        "                        'translation',\n",
        "                        model=model_config['name'],\n",
        "                        device=-1,\n",
        "                        torch_dtype='auto'\n",
        "                    )\n",
        "                    self.src_lang_en = 'en'\n",
        "                    self.tgt_lang_ko = 'ko'\n",
        "                    self.src_lang_ko = 'ko'\n",
        "                    self.tgt_lang_en = 'en'\n",
        "\n",
        "                elif model_config['type'] == 'nllb':\n",
        "                    # NLLB 모델\n",
        "                    self.translator = pipeline(\n",
        "                        'translation',\n",
        "                        model=model_config['name'],\n",
        "                        device=-1\n",
        "                    )\n",
        "                    self.src_lang_en = 'eng_Latn'\n",
        "                    self.tgt_lang_ko = 'kor_Hang'\n",
        "                    self.src_lang_ko = 'kor_Hang'\n",
        "                    self.tgt_lang_en = 'eng_Latn'\n",
        "\n",
        "                load_time = time.time() - start_time\n",
        "\n",
        "                # 테스트 번역으로 품질 확인\n",
        "                test_en = \"Hello, how are you?\"\n",
        "                test_ko = \"안녕하세요, 어떻게 지내세요?\"\n",
        "\n",
        "                # 영→한 테스트\n",
        "                en_result = self._translate_with_params(test_en, self.src_lang_en, self.tgt_lang_ko)\n",
        "                # 한→영 테스트\n",
        "                ko_result = self._translate_with_params(test_ko, self.src_lang_ko, self.tgt_lang_en)\n",
        "\n",
        "                print(f\"✅ 성공! 로딩 시간: {load_time:.2f}초\")\n",
        "                print(f\"📊 영→한 테스트: '{test_en}' → '{en_result}'\")\n",
        "                print(f\"📊 한→영 테스트: '{test_ko}' → '{ko_result}'\")\n",
        "\n",
        "                self.model_info = {\n",
        "                    'name': model_config['name'],\n",
        "                    'description': model_config['description'],\n",
        "                    'size': model_config['size'],\n",
        "                    'type': model_config['type'],\n",
        "                    'load_time': f\"{load_time:.2f}초\"\n",
        "                }\n",
        "\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 실패: {str(e)[:60]}...\")\n",
        "                continue\n",
        "\n",
        "        if self.translator is None:\n",
        "            raise RuntimeError(\"사용 가능한 고품질 번역 모델을 찾을 수 없습니다.\")\n",
        "\n",
        "    def _translate_with_params(self, text, src_lang, tgt_lang):\n",
        "        \"\"\"파라미터를 사용한 번역 실행\"\"\"\n",
        "        try:\n",
        "            if 'm2m100' in self.model_info['type']:\n",
        "                # M2M100 모델용\n",
        "                result = self.translator(text, src_lang=src_lang, tgt_lang=tgt_lang, max_length=128)\n",
        "            else:\n",
        "                # NLLB 모델용\n",
        "                result = self.translator(text, src_lang=src_lang, tgt_lang=tgt_lang, max_length=128)\n",
        "\n",
        "            return result[0]['translation_text']\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"번역 오류: {str(e)[:50]}...\"\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        \"\"\"개선된 언어 감지\"\"\"\n",
        "        # 한글 문자 비율 계산\n",
        "        korean_chars = len(re.findall(r'[가-힣]', text))\n",
        "        total_chars = len(re.sub(r'\\s+', '', text))  # 공백 제외\n",
        "\n",
        "        if total_chars == 0:\n",
        "            return 'unknown'\n",
        "\n",
        "        korean_ratio = korean_chars / total_chars\n",
        "\n",
        "        # 한글이 30% 이상이면 한국어로 판단\n",
        "        if korean_ratio >= 0.3:\n",
        "            return 'ko'\n",
        "        # 영문자나 숫자가 주를 이루면 영어로 판단\n",
        "        elif re.search(r'[a-zA-Z]', text):\n",
        "            return 'en'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "\n",
        "    def translate_en_to_ko(self, english_text):\n",
        "        \"\"\"영어→한국어 번역\"\"\"\n",
        "        if not english_text.strip():\n",
        "            return \"번역할 텍스트가 없습니다.\"\n",
        "\n",
        "        try:\n",
        "            result = self._translate_with_params(\n",
        "                english_text.strip(),\n",
        "                self.src_lang_en,\n",
        "                self.tgt_lang_ko\n",
        "            )\n",
        "            self.translation_count += 1\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"영→한 번역 실패: {str(e)}\"\n",
        "\n",
        "    def translate_ko_to_en(self, korean_text):\n",
        "        \"\"\"한국어→영어 번역\"\"\"\n",
        "        if not korean_text.strip():\n",
        "            return \"번역할 텍스트가 없습니다.\"\n",
        "\n",
        "        try:\n",
        "            result = self._translate_with_params(\n",
        "                korean_text.strip(),\n",
        "                self.src_lang_ko,\n",
        "                self.tgt_lang_en\n",
        "            )\n",
        "            self.translation_count += 1\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"한→영 번역 실패: {str(e)}\"\n",
        "\n",
        "    def auto_translate(self, text):\n",
        "        \"\"\"자동 언어 감지 및 번역\"\"\"\n",
        "        if not text.strip():\n",
        "            return \"번역할 텍스트가 없습니다.\"\n",
        "\n",
        "        detected_lang = self.detect_language(text)\n",
        "\n",
        "        if detected_lang == 'ko':\n",
        "            translation = self.translate_ko_to_en(text)\n",
        "            return f\"🇰🇷→🇺🇸 {translation}\"\n",
        "        elif detected_lang == 'en':\n",
        "            translation = self.translate_en_to_ko(text)\n",
        "            return f\"🇺🇸→🇰🇷 {translation}\"\n",
        "        else:\n",
        "            # 일단 영어로 가정하고 번역 시도\n",
        "            translation = self.translate_en_to_ko(text)\n",
        "            return f\"🔤→🇰🇷 {translation}\"\n",
        "\n",
        "    def get_model_info(self):\n",
        "        \"\"\"모델 정보 반환\"\"\"\n",
        "        info = self.model_info.copy()\n",
        "        info['total_translations'] = self.translation_count\n",
        "        return info\n",
        "\n",
        "    def batch_translate(self, texts, direction='auto'):\n",
        "        \"\"\"배치 번역\"\"\"\n",
        "        results = []\n",
        "\n",
        "        print(f\"📦 배치 번역 시작 ({len(texts)}개 문장)\")\n",
        "\n",
        "        for i, text in enumerate(texts, 1):\n",
        "            if direction == 'en_to_ko':\n",
        "                result = self.translate_en_to_ko(text)\n",
        "            elif direction == 'ko_to_en':\n",
        "                result = self.translate_ko_to_en(text)\n",
        "            else:\n",
        "                result = self.auto_translate(text)\n",
        "\n",
        "            results.append(result)\n",
        "            print(f\"진행률: {i}/{len(texts)} ({i/len(texts)*100:.1f}%)\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# 실제 사용 및 테스트\n",
        "print(\"=== 고품질 양방향 번역기 테스트 ===\")\n",
        "\n",
        "try:\n",
        "    # 번역기 초기화\n",
        "    translator = HighQualityTranslator()\n",
        "\n",
        "    # 모델 정보 출력\n",
        "    info = translator.get_model_info()\n",
        "    print(f\"\\n📋 로딩된 모델 정보:\")\n",
        "    for key, value in info.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 1. 영어→한국어 번역 테스트\n",
        "    print(f\"🔤 영어→한국어 번역 테스트:\")\n",
        "    en_sentences = [\n",
        "        \"Good morning! How are you today?\",\n",
        "        \"I love studying artificial intelligence and machine learning.\",\n",
        "        \"The weather is beautiful, perfect for a walk in the park.\",\n",
        "        \"Thank you so much for your help with this project.\",\n",
        "        \"Python is an excellent programming language for beginners.\"\n",
        "    ]\n",
        "\n",
        "    for i, sentence in enumerate(en_sentences, 1):\n",
        "        translation = translator.translate_en_to_ko(sentence)\n",
        "        print(f\"\\n{i}. EN: {sentence}\")\n",
        "        print(f\"   KO: {translation}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 2. 한국어→영어 번역 테스트\n",
        "    print(f\"🔤 한국어→영어 번역 테스트:\")\n",
        "    ko_sentences = [\n",
        "        \"안녕하세요! 오늘 기분이 어떠세요?\",\n",
        "        \"저는 인공지능과 머신러닝을 공부하는 것을 좋아합니다.\",\n",
        "        \"날씨가 정말 좋네요, 공원을 산책하기 완벽해요.\",\n",
        "        \"이 프로젝트를 도와주셔서 정말 감사합니다.\",\n",
        "        \"파이썬은 초보자들에게 훌륭한 프로그래밍 언어입니다.\"\n",
        "    ]\n",
        "\n",
        "    for i, sentence in enumerate(ko_sentences, 1):\n",
        "        translation = translator.translate_ko_to_en(sentence)\n",
        "        print(f\"\\n{i}. KO: {sentence}\")\n",
        "        print(f\"   EN: {translation}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # 3. 자동 언어 감지 번역 테스트\n",
        "    print(f\"🤖 자동 언어 감지 번역 테스트:\")\n",
        "    mixed_sentences = [\n",
        "        \"Hello world!\",\n",
        "        \"안녕 세상!\",\n",
        "        \"I'm learning Korean language.\",\n",
        "        \"한국어를 배우고 있어요.\",\n",
        "        \"Artificial Intelligence is amazing!\",\n",
        "        \"인공지능은 정말 놀라워요!\"\n",
        "    ]\n",
        "\n",
        "    for i, sentence in enumerate(mixed_sentences, 1):\n",
        "        detected_lang = translator.detect_language(sentence)\n",
        "        translation = translator.auto_translate(sentence)\n",
        "        print(f\"\\n{i}. 입력: {sentence}\")\n",
        "        print(f\"   감지언어: {detected_lang}\")\n",
        "        print(f\"   번역: {translation}\")\n",
        "\n",
        "    # 4. 최종 통계\n",
        "    final_info = translator.get_model_info()\n",
        "    print(f\"\\n📊 번역 통계:\")\n",
        "    print(f\"   총 번역 횟수: {final_info['total_translations']}회\")\n",
        "    print(f\"   사용 모델: {final_info['description']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ 번역기 오류: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "6HApDSDG01zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "v7LZ-3Lr82Lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phi‑3**\n",
        "\n",
        "- **Microsoft가 개발한 소형 언어 모델(SLM) 시리즈**\n",
        "    - Phi‑3‑mini (3.8B 파라미터), Phi‑3‑small (약 7B), Phi‑3‑medium (14B) 등 다양한 변형을 포함\n",
        "- 파라미터 & 컨텍스트 윈도우\n",
        "    - Phi-3-mini: 3.8B 파라미터, 4K 및 확장된 128K 컨텍스트 윈도우 지원\n",
        "    - Phi‑3‑small: 약 7B, 기본 8K 컨텍스트\n",
        "    - Phi‑3‑medium: 14B 기능, 더 넓은 컨텍스트 처리 능력\n",
        "    - **microsoft/Phi-3-mini-4k-instruct**\n",
        "\n",
        "- 학습 데이터 & 미세조정\n",
        "    - 총 3.3조 토큰 이상으로 구성된 고품질 필터링 웹 데이터, 합성 데이터 등을 활용. 이후 **Supervised Fine-Tuning (SFT)** 과 Direct Preference Optimization (DPO) 기법을 통해 인간 선호도 및 안전 기준에 맞춰 미세 조정됨\n",
        "- 성능 지표\n",
        "    - 최소형 모델(Phi-3-mini)도 MMLU 69%, MT-bench 8.38 등 성능으로 Mixtral 8x7B, GPT-3.5와 유사한 수준\n",
        "    - Phi‑3‑small 및 medium는 각각 MMLU 75% 및 78%, MT‑bench 8.7 및 8.9를 달성\n",
        "    - 여러 벤치마크(HellaSwag, WinoGrande, TruthfulQA, HumanEval 등)에서 강력한 성능을 보임\n",
        "\n",
        "- 관련 모델들\n",
        "    - Phi-3-mini-128k-instruct: 128K 컨텍스트 지원 버전\n",
        "    - Phi-3.5-mini-instruct: 다국어 지원 강화 버전 (2024년 8월)\n",
        "    - Phi-4-mini-instruct: 최신 아키텍처 적용 버전\n"
      ],
      "metadata": {
        "id": "olZXCZdcWh_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| 항목 | 세부 정보 |\n",
        "|--- |--- |\n",
        "|모델 크기 |3.8억 파라미터 |\n",
        "|컨텍스트 길이 |4K 토큰 |\n",
        "|훈련 토큰 수 |3.3조 토큰 |\n",
        "|어휘 크기 |32,064 토큰 |\n",
        "|아키텍처 |Dense decoder-only Transformer |\n",
        "|출시일 |2024년 6월 |\n",
        "|라이선스 |MIT 라이선스 |"
      ],
      "metadata": {
        "id": "58ekeCbKGWqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "|벤치마크 |Phi-3-mini 성능 |비교 모델 성능 |\n",
        "|--- |--- |--- |\n",
        "|MMLU |69% |GPT-3.5: ~70% |\n",
        "|MT-bench |8.38 |GPT-3.5: ~8.5 |\n",
        "|HumanEval |높은 성능 |확실하지 않음 |\n",
        "|GSM-8K |우수 |확실하지 않음 |"
      ],
      "metadata": {
        "id": "emkEAj1TGbNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QuickTour"
      ],
      "metadata": {
        "id": "GYYPM8LKYKtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델 로드 및 토큰 생성\n"
      ],
      "metadata": {
        "id": "iiJCX0VgX5F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 중간수준 API(Task-specific Models) 방식 --> 코랩(무료)에서 실행 오래 걸림(5분⭡)"
      ],
      "metadata": {
        "id": "ye9p2FAuak2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3-mini 로드 및 간단 생성 (Transformers)\n",
        "#   선형적 생성 흐름을 체험하며 모델 작동 원리를 설명하기 좋다.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 모델과 토크나이저를 로드합니다.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
      ],
      "metadata": {
        "id": "BZfCMj2BYK4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 고수준 API(pipeline) 방식\n",
        "\n",
        "모델과 토크나이저를 직접 사용할 수 있지만 pipeline 함수를 사용하는 것이 훨씬 쉽습니다."
      ],
      "metadata": {
        "id": "2c9r0tymaW76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 파이프라인을 만듭니다.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "5Foxafz3aXEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡 **[참고] PyTorch의 언어 모델 생성 함수에서 사용되는 주요 매개변수**\n",
        "\n",
        "| 매개변수 | 의미 | 기본값 | 범위 | 설명 |\n",
        "|--- |--- |--- |--- |--- |\n",
        "| **max_new_tokens**  |모델이 생성할 새로운 토큰의 최대 개수  |모델마다 다름 (보통 50-100)  |1 ~ 모델의 최대 컨텍스트 길이  |  |\n",
        "|**temperature**  |모델 출력의 창의성과 일관성을 조절하는 매개변수  |1.0  |0.0 ~ 2.0 (일반적으로 0.1 ~ 1.5)  |Softmax 함수의 온도 매개변수  |\n",
        "|**do_sample**  |샘플링 방식 선택 (확률적 vs 결정적)  |False  |Boolean (True/False)  |  |"
      ],
      "metadata": {
        "id": "4clCIvrf_qbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# max_new_tokens\n",
        "#--------------------------\n",
        "# 짧은 답변 (요약, 분류)\n",
        "max_new_tokens=50\n",
        "\n",
        "# 중간 길이 답변 (설명, Q&A)\n",
        "max_new_tokens=200\n",
        "\n",
        "# 긴 답변 (에세이, 보고서)\n",
        "max_new_tokens=500"
      ],
      "metadata": {
        "id": "01YVt_XCA6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Temperature** 값에 따른 특성\n",
        "\n",
        "|Temperature|특성|사용 상황|예시|\n",
        "|--- |--- |--- |---|\n",
        "|0.0 | 완전히 결정적, 항상 같은 출력 | 정확한 답변 필요 | 수학 문제, 번역 |\n",
        "|0.3-0.5 | 안정적, 일관성 높음 | 비즈니스 문서, 기술 설명 | 보고서 작성 |\n",
        "|0.7 | 균형 잡힌 창의성과 일관성 | 일반적인 대화, 교육 | 챗봇, Q&A |\n",
        "|1.0 | 표준 창의성 | 기본 설정 | 기본 대화\n",
        "|1.5-2.0 | 매우 창의적, 예측 불가 | 창작 활동, 브레인스토밍 | 소설, 시 창작 |"
      ],
      "metadata": {
        "id": "36N_NtELBcBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# Temperature 작동 원리\n",
        "#--------------------------\n",
        "# 수학적 표현\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    \"\"\"Temperature scaling 적용\"\"\"\n",
        "    if temperature == 0:\n",
        "        return torch.argmax(logits, dim=-1)\n",
        "    else:\n",
        "        scaled_logits = logits / temperature\n",
        "        probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "        return torch.multinomial(probabilities, 1)\n",
        "\n",
        "# 예시: 다음 단어 예측 확률\n",
        "original_probs = [0.6, 0.3, 0.1]  # \"좋은\", \"나쁜\", \"보통\"\n",
        "\n",
        "# Temperature = 0.5 (더 확실한 선택)\n",
        "temp_05_probs = [0.8, 0.15, 0.05]\n",
        "\n",
        "# Temperature = 1.5 (더 다양한 선택)\n",
        "temp_15_probs = [0.45, 0.35, 0.2]"
      ],
      "metadata": {
        "id": "kL8K-xSIBQUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **do_sample**옵션 비교\n",
        "\n",
        "| do_sample |  방식 | 특징 | 언제 사용? |\n",
        "|--- |--- |--- |--- |\n",
        "| False | Greedy Decoding | 항상 가장 확률 높은 토큰 선택 | 정확성이 중요한 작업 |\n",
        "| True | Sampling | 확률에 따라 랜덤하게 선택 | 창의성이 필요한 작업 |"
      ],
      "metadata": {
        "id": "j9P_SQq__0Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do_sample=False (Greedy)\n",
        "다음 단어 확률: {\"좋은\": 0.6, \"나쁜\": 0.3, \"보통\": 0.1}\n",
        "선택: 항상 \"좋은\" (가장 높은 확률)\n",
        "\n",
        "# do_sample=True (Sampling)\n",
        "다음 단어 확률: {\"좋은\": 0.6, \"나쁜\": 0.3, \"보통\": 0.1}\n",
        "선택: 60% 확률로 \"좋은\", 30% 확률로 \"나쁜\", 10% 확률로 \"보통\""
      ],
      "metadata": {
        "id": "TvPx7P_UCtMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NR4g4e44_0GB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 프롬프트를 작성하고 모델에 주입"
      ],
      "metadata": {
        "id": "sxUo-N0cbC_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 (사용자 입력 / 쿼리)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# 출력 생성\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "hJOl9GZIbDJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 기본 텍스트 생성"
      ],
      "metadata": {
        "id": "hLTlimBYRfid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # GPU 메모리 절약\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 채팅 형식 프롬프트 구성\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"당신은 도움이 되는 AI 어시스턴트입니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"파이썬에서 리스트와 튜플의 차이점을 설명해주세요.\"}\n",
        "]\n",
        "\n",
        "# 토큰화 및 생성\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# 텍스트 생성\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,  #모델이 생성할 새로운 토큰의 최대 개수\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# 결과 출력\n",
        "print('-' * 50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1z-FreQiRlsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 코딩 어시스턴트"
      ],
      "metadata": {
        "id": "uRwt3o1zSpV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coding_assistant_example():\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"당신은 파이썬 프로그래밍 전문가입니다.\"},\n",
        "        {\"role\": \"user\", \"content\": \"피보나치 수열을 계산하는 효율적인 함수를 작성해주세요.\"}\n",
        "    ]\n",
        "\n",
        "    # 파이프라인 생성\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # 응답 생성\n",
        "    print('-' * 50)\n",
        "    response = pipe(messages)\n",
        "    print(response[0]['generated_text'])\n",
        "\n",
        "coding_assistant_example()"
      ],
      "metadata": {
        "id": "Bgv8CP_rSw6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 교육용 Q&A 시스템"
      ],
      "metadata": {
        "id": "9qDx6AzEShCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def educational_qa_system():\n",
        "    \"\"\"교육용 질의응답 시스템\"\"\"\n",
        "\n",
        "    class Phi3EducationBot:\n",
        "        def __init__(self, model_name=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            self.pipe = pipeline(\"text-generation\",\n",
        "                               model=self.model,\n",
        "                               tokenizer=self.tokenizer)\n",
        "\n",
        "        def answer_question(self, question, subject=\"컴퓨터공학\"):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"당신은 {subject} 전문 강사입니다. 학생들이 이해하기 쉽게 설명해주세요.\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ]\n",
        "\n",
        "            response = self.pipe(messages, max_new_tokens=250)\n",
        "            return response[0]['generated_text']\n",
        "\n",
        "    # 사용 예시\n",
        "    bot = Phi3EducationBot()\n",
        "    answer = bot.answer_question(\"머신러닝과 딥러닝의 차이점은 무엇인가요?\")\n",
        "    print(answer)\n",
        "\n",
        "educational_qa_system()"
      ],
      "metadata": {
        "id": "d7m4784iSMhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [미션] OOO 시스템 만들기"
      ],
      "metadata": {
        "id": "Dk2UMp3ETSyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}