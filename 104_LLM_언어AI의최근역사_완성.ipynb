{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6BqX9v9SUmhB4LYLLJnIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/ktcloud_genai/blob/main/104_LLM_%EC%96%B8%EC%96%B4AI%EC%9D%98%EC%B5%9C%EA%B7%BC%EC%97%AD%EC%82%AC_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 언어 AI의 최근 역사"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡**참고 교재**\n",
        "    - 핸즈온LLM https://www.hanbit.co.kr/store/books/look.php?p_code=B2599445240\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[주의!] (코랩)한글 폰트 지정하는 방법**"
      ],
      "metadata": {
        "id": "eLfwMhbVRg6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) 환경 점검: 설치된 폰트 실제 파일이 있는지 먼저 확인\n",
        "!ls -R /usr/share/fonts | head -n 50\n",
        "\n",
        "# 1) (이미 설치했다면 생략 가능) 한글 폰트 설치\n",
        "!apt-get -y update\n",
        "!apt-get -y install fonts-nanum fonts-noto-cjk\n",
        "\n",
        "# 2) 시스템 폰트 캐시 재생성\n",
        "!fc-cache -fv\n",
        "\n",
        "# 3) Matplotlib 내부 폰트 캐시 삭제 (중요)\n",
        "import os, shutil\n",
        "import matplotlib\n",
        "cache_dir = matplotlib.get_cachedir()      # 보통 ~/.cache/matplotlib\n",
        "# print(\"matplotlib cache:\", cache_dir)\n",
        "shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "\n",
        "# 4) 런타임 재시작 필요 없이, 폰트 매니저 강제 리로드\n",
        "from matplotlib import font_manager\n",
        "_ = font_manager._load_fontmanager(try_read_cache=False)\n",
        "\n",
        "# 5) 폰트 존재 여부 확인 (fc-list와 파이썬에서 모두 확인)\n",
        "# !fc-list | grep -i \"nanum\\|noto\" | head -n 40\n",
        "\n",
        "# from matplotlib import font_manager\n",
        "# fonts = font_manager.findSystemFonts()\n",
        "# [f for f in fonts if (\"Nanum\" in f or \"NotoSansCJK\" in f or \"Noto\" in f)][:10]\n",
        "\n",
        "\n",
        "# 6) 한글 폰트 지정\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib import font_manager, pyplot as plt\n",
        "\n",
        "font_path = \"/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\"\n",
        "font_manager.fontManager.addfont(font_path)\n",
        "family_name = font_manager.FontProperties(fname=font_path).get_name()\n",
        "# print(\"적용할 패밀리명:\", family_name)\n",
        "\n",
        "# plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "plt.rcParams[\"font.family\"] = family_name # NanumBarunGothic\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "print(\"✅ 나눔폰트 설정 완료\")# 폰트 지정\n",
        "\n",
        "\n",
        "# 7) 예시 데이터\n",
        "documents = [\"문서1\",\"문서2\",\"문서3\"]\n",
        "similarity_matrix = np.array([[1.0,0.7,0.2],\n",
        "                              [0.7,1.0,0.4],\n",
        "                              [0.2,0.4,1.0]])\n",
        "\n",
        "plt.figure(figsize=(3,4))\n",
        "sns.heatmap(similarity_matrix,\n",
        "            annot=True,\n",
        "            cmap='Blues',\n",
        "            xticklabels=[f'문서{i+1}' for i in range(len(documents))],\n",
        "            yticklabels=[f'문서{i+1}' for i in range(len(documents))])\n",
        "plt.title('문서 간 코사인 유사도')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rj_HPelGfns7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DJRiPmIyRePs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BoW(Bag-of-Words)**"
      ],
      "metadata": {
        "id": "BHtmx0nuFlEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **비구조적인 텍스트를 표현**하는 한 방법\n",
        "- 1950년대에 언급되었지만 2000년대에 인기를 얻음\n",
        "- 공백을 기준으로 개별 단어로 분할--> Token\n",
        "- **토큰화(Tokenization)** 과정:  \n",
        "    - 문장을 개별 단어나 부분단어(subword)로 분할하는 과정\n",
        "- **BoW의 목표**:  \n",
        "    - **벡터(or 벡터 표현)라고 부르는 수치 형태로 텍스트를 표현**\n",
        "- 이런 모델을 **표현 모델(representation model)** 이라고 부름"
      ],
      "metadata": {
        "id": "4c93FnL3PZp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : BoW 구현하고 텍스트 분류하기**"
      ],
      "metadata": {
        "id": "GIW9EoDWR-u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **BoW Basic Class(직접 구현)**"
      ],
      "metadata": {
        "id": "V8gnwjtCbttx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "class BagOfWords:\n",
        "    def __init__(self, min_freq=1, max_vocab_size=None, remove_stopwords=True):\n",
        "        \"\"\"\n",
        "        Bag of Words 구현 클래스\n",
        "\n",
        "        Args:\n",
        "            min_freq: 최소 단어 빈도\n",
        "            max_vocab_size: 최대 어휘 크기\n",
        "            remove_stopwords: 불용어 제거 여부\n",
        "        \"\"\"\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.vocab = {}\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        # 한국어 불용어 (간단한 예제)\n",
        "        self.stopwords = {\n",
        "            '은', '는', '이', '가', '을', '를', '에', '에서', '로', '으로',\n",
        "            '의', '와', '과', '도', '만', '까지', '부터', '조차', '마저',\n",
        "            '그리고', '그러나', '하지만', '또한', '그래서', '따라서'\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"텍스트 전처리\"\"\"\n",
        "        # 소문자 변환 및 특수문자 제거 (한글 보존)\n",
        "        text = re.sub(r'[^\\w\\s가-힣]', '', text.lower())\n",
        "        words = text.split()\n",
        "\n",
        "        # 불용어 제거\n",
        "        if self.remove_stopwords:\n",
        "            words = [word for word in words if word not in self.stopwords]\n",
        "\n",
        "        return [word for word in words if word.strip()]\n",
        "\n",
        "    def build_vocab(self, documents):\n",
        "        \"\"\"어휘 구축\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # 모든 문서에서 단어 빈도 계산\n",
        "        for doc in documents:\n",
        "            words = self.preprocess_text(doc)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # 최소 빈도 필터링\n",
        "        filtered_words = {word: count for word, count in word_counts.items()\n",
        "                         if count >= self.min_freq}\n",
        "\n",
        "        # 빈도순 정렬\n",
        "        sorted_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 최대 어휘 크기 제한\n",
        "        if self.max_vocab_size:\n",
        "            sorted_words = sorted_words[:self.max_vocab_size]\n",
        "\n",
        "        # 어휘 매핑 생성\n",
        "        self.vocab = dict(sorted_words)\n",
        "        self.word_to_idx = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(f\"구축된 어휘 크기: {self.vocab_size}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def text_to_bow(self, text):\n",
        "        \"\"\"텍스트를 BoW 벡터로 변환\"\"\"\n",
        "        words = self.preprocess_text(text)\n",
        "        bow_vector = np.zeros(self.vocab_size)\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.word_to_idx:\n",
        "                idx = self.word_to_idx[word]\n",
        "                bow_vector[idx] += 1\n",
        "\n",
        "        return bow_vector\n",
        "\n",
        "    def documents_to_bow_matrix(self, documents):\n",
        "        \"\"\"문서들을 BoW 행렬로 변환\"\"\"\n",
        "        bow_matrix = []\n",
        "        for doc in documents:\n",
        "            bow_vector = self.text_to_bow(doc)\n",
        "            bow_matrix.append(bow_vector)\n",
        "\n",
        "        return np.array(bow_matrix)\n",
        "\n",
        "    def get_top_words(self, bow_vector, top_n=5):\n",
        "        \"\"\"BoW 벡터에서 상위 단어들 추출\"\"\"\n",
        "        word_scores = [(self.idx_to_word[idx], score)\n",
        "                      for idx, score in enumerate(bow_vector) if score > 0]\n",
        "        word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return word_scores[:top_n]\n",
        "\n",
        "    def calculate_similarity(self, doc1, doc2):\n",
        "        \"\"\"두 문서 간 코사인 유사도 계산\"\"\"\n",
        "        bow1 = self.text_to_bow(doc1)\n",
        "        bow2 = self.text_to_bow(doc2)\n",
        "\n",
        "        # 코사인 유사도\n",
        "        dot_product = np.dot(bow1, bow2)\n",
        "        norm1 = np.linalg.norm(bow1)\n",
        "        norm2 = np.linalg.norm(bow2)\n",
        "\n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return dot_product / (norm1 * norm2)\n"
      ],
      "metadata": {
        "id": "M8R7Gl3-byCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. 기본 BoW 구현 시연**"
      ],
      "metadata": {
        "id": "ObS9srkGb6LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 예제 문서들\n",
        "# ----------------------------\n",
        "documents = [\n",
        "    \"강아지는 귀여운 동물입니다\",\n",
        "    \"고양이도 귀여운 반려동물입니다\",\n",
        "    \"반려동물은 가족과 같습니다\",\n",
        "    \"인공지능은 미래 기술입니다\",\n",
        "    \"기술 발전은 빠릅니다\",\n",
        "    \"미래에는 로봇이 일상이 될 것입니다\"\n",
        "] # 6 docs\n",
        "\n",
        "# documents = [\n",
        "#     \"자연언어처리는 인공지능의 중요한 분야입니다\",\n",
        "#     \"머신러닝과 딥러닝이 자연언어처리를 발전시켰습니다\",\n",
        "#     \"텍스트 마이닝은 빅데이터 분석에 활용됩니다\",\n",
        "#     \"감정분석과 문서분류는 NLP의 대표적인 응용분야입니다\",\n",
        "#     \"챗봇과 번역시스템이 일상생활에 널리 사용됩니다\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "Da782x9ycyqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_basic(documents):\n",
        "    \"\"\"기본 BoW 시연\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"✅ 1. 기본 BoW 구현 시연\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ----------------------------\n",
        "    # BoW 모델 생성 및 학습\n",
        "    # ----------------------------\n",
        "    bow_model = BagOfWords(min_freq=1, remove_stopwords=True)\n",
        "    vocab = bow_model.build_vocab(documents)\n",
        "\n",
        "    print(\"\\n➡️ 구축된 어휘:\")\n",
        "    for word, freq in list(vocab.items())[:10]:\n",
        "        print(f\"  {word}: {freq}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 문서를 BoW 벡터로 변환\n",
        "    # ----------------------------\n",
        "    print(\"\\n➡️ 문서별 BoW 벡터:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        bow_vector = bow_model.text_to_bow(doc)\n",
        "        top_words = bow_model.get_top_words(bow_vector, top_n=3)\n",
        "        print(f\"\\n문서 {i+1}: {doc}\")\n",
        "        print(f\"BoW 벡터 형태: {bow_vector.shape}\")\n",
        "        print(f\"상위 단어: {top_words}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 문서 간 유사도 계산\n",
        "    # ----------------------------\n",
        "    print(\"\\n➡️ 문서 간 유사도:\")\n",
        "    for i in range(len(documents)):\n",
        "        for j in range(i+1, len(documents)):\n",
        "            similarity = bow_model.calculate_similarity(documents[i], documents[j])\n",
        "            print(f\"문서{i+1} vs 문서{j+1}: {similarity:.4f}\")\n",
        "\n",
        "\n",
        "# 테스트\n",
        "demonstrate_bow_basic(documents)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zyGE8uTfcA2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. sklearn을 활용한 고급 BoW**"
      ],
      "metadata": {
        "id": "lhUeOruicSmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_with_sklearn(documents):\n",
        "    \"\"\"sklearn을 활용한 고급 BoW\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"✅ 2. sklearn을 활용한 고급 BoW\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 1. CountVectorizer (기본 BoW)\n",
        "    # ----------------------------\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    count_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(\"➡️ CountVectorizer 결과:\")\n",
        "    print(f\"특성 수: {len(count_vectorizer.get_feature_names_out())}\")\n",
        "    print(f\"행렬 형태: {count_matrix.shape}\")\n",
        "\n",
        "    # 특성 이름들\n",
        "    feature_names = count_vectorizer.get_feature_names_out()\n",
        "    print(f\"특성 예시: {list(feature_names)[:10]}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2. TF-IDF\n",
        "    # ----------------------------\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(f\"\\n➡️ TF-IDF 행렬 형태: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # 문서별 상위 TF-IDF 단어\n",
        "    tfidf_array = tfidf_matrix.toarray()\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(\"\\n➡️ 각 문서의 상위 TF-IDF 단어:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        # 해당 문서의 TF-IDF 점수\n",
        "        tfidf_scores = tfidf_array[i]\n",
        "        # 상위 3개 단어 인덱스\n",
        "        top_indices = np.argsort(tfidf_scores)[-3:][::-1]\n",
        "        top_words = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices if tfidf_scores[idx] > 0]\n",
        "\n",
        "        print(f\"문서 {i+1}: {top_words}\")\n",
        "\n",
        "    # 문서 간 유사도 매트릭스\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    print(f\"\\n➡️ 유사도 매트릭스 형태: {similarity_matrix.shape}\")\n",
        "\n",
        "    # 유사도 히트맵 시각화\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                annot=True,\n",
        "                cmap='Blues',\n",
        "                xticklabels=[f'문서{i+1}' for i in range(len(documents))],\n",
        "                yticklabels=[f'문서{i+1}' for i in range(len(documents))])\n",
        "    plt.title('문서 간 코사인 유사도')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 테스트\n",
        "demonstrate_bow_with_sklearn(documents)"
      ],
      "metadata": {
        "id": "bp8xf_w7ch5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. BoW 를 이용한 텍스트 분류**"
      ],
      "metadata": {
        "id": "ZZGCdRpNcllO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_applications():\n",
        "    \"\"\"BoW 응용 예제\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"✅ 3. BoW 실전 응용 - 텍스트 분류\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 카테고리별 예제 데이터\n",
        "    tech_docs = [\n",
        "        \"인공지능과 머신러닝 기술이 급속히 발전하고 있습니다\",\n",
        "        \"딥러닝은 신경망을 기반으로 한 기계학습 방법입니다\",\n",
        "        \"자연언어처리 기술로 텍스트를 자동으로 분석할 수 있습니다\",\n",
        "        \"컴퓨터 비전은 이미지 인식과 처리에 사용됩니다\"\n",
        "    ]\n",
        "\n",
        "    sports_docs = [\n",
        "        \"축구는 전 세계에서 가장 인기 있는 스포츠입니다\",\n",
        "        \"올림픽에서 다양한 종목의 경기가 열립니다\",\n",
        "        \"농구는 팀워크가 중요한 운동입니다\",\n",
        "        \"수영은 전신을 사용하는 유산소 운동입니다\"\n",
        "    ]\n",
        "\n",
        "    food_docs = [\n",
        "        \"한식은 발효음식이 많아 건강에 좋습니다\",\n",
        "        \"이탈리아 파스타는 다양한 소스와 함께 즐깁니다\",\n",
        "        \"일본 초밥은 신선한 재료가 핵심입니다\",\n",
        "        \"프랑스 요리는 정교한 조리법으로 유명합니다\"\n",
        "    ]\n",
        "\n",
        "    # 전체 문서와 라벨\n",
        "    all_docs = tech_docs + sports_docs + food_docs\n",
        "    labels = ['기술'] * len(tech_docs) + ['스포츠'] * len(sports_docs) + ['음식'] * len(food_docs)\n",
        "\n",
        "    # TF-IDF 벡터화\n",
        "    vectorizer = TfidfVectorizer(max_features=100)\n",
        "    doc_vectors = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "    print(f\"➡️ 문서 수: {len(all_docs)}\")\n",
        "    print(f\"➡️ 특성 수: {doc_vectors.shape[1]}\")\n",
        "\n",
        "    # 카테고리별 대표 단어 추출\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # 카테고리별 평균 TF-IDF 계산\n",
        "    categories = ['기술', '스포츠', '음식']\n",
        "    category_vectors = {}\n",
        "\n",
        "    for category in categories:\n",
        "        # 해당 카테고리의 문서 인덱스\n",
        "        category_indices = [i for i, label in enumerate(labels) if label == category]\n",
        "\n",
        "        # 해당 카테고리 문서들의 평균 벡터\n",
        "        category_matrix = doc_vectors[category_indices]\n",
        "        avg_vector = np.mean(category_matrix.toarray(), axis=0)\n",
        "        category_vectors[category] = avg_vector\n",
        "\n",
        "        # 상위 단어들\n",
        "        top_indices = np.argsort(avg_vector)[-5:][::-1]\n",
        "        top_words = [feature_names[idx] for idx in top_indices]\n",
        "\n",
        "        print(f\"\\n{category} 카테고리 대표 단어: {top_words}\")\n",
        "\n",
        "    # 새로운 문서 분류 예제\n",
        "    test_docs = [\n",
        "        \"딥러닝 모델을 학습시키는 방법을 알아보겠습니다\",\n",
        "        \"월드컵 축구 경기가 열렸습니다\",\n",
        "        \"맛있는 김치찌개 레시피를 소개합니다\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n➡️ 새로운 문서 분류:\")\n",
        "    test_vectors = vectorizer.transform(test_docs)\n",
        "\n",
        "    for i, test_doc in enumerate(test_docs):\n",
        "        test_vector = test_vectors[i].toarray()[0]\n",
        "\n",
        "        # 각 카테고리와의 유사도 계산\n",
        "        similarities = {}\n",
        "        for category, category_vector in category_vectors.items():\n",
        "            similarity = cosine_similarity([test_vector], [category_vector])[0][0]\n",
        "            similarities[category] = similarity\n",
        "\n",
        "        # 가장 유사한 카테고리\n",
        "        predicted_category = max(similarities.items(), key=lambda x: x[1])\n",
        "\n",
        "        print(f\"\\n 테스트 문서: {test_doc}\")\n",
        "        print(f\" 유사도 - {similarities}\")\n",
        "        print(f\" 예측 카테고리: {predicted_category[0]} (유사도: {predicted_category[1]:.4f})\")\n",
        "\n",
        "# 테스트\n",
        "demonstrate_bow_applications()"
      ],
      "metadata": {
        "id": "m4nikP8vovr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. BoW 특성 분석**"
      ],
      "metadata": {
        "id": "eNWwP0D1o9OI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWRq3G5oE3EE"
      },
      "outputs": [],
      "source": [
        "def analyze_bow_characteristics():\n",
        "    \"\"\"BoW 특성 분석\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"✅ 4. BoW 특성 분석\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    documents = [\n",
        "        \"고양이가 집에서 잠을 잡니다\",\n",
        "        \"집에서 고양이가 잠을 잡니다\",  # 어순이 다른 같은 의미\n",
        "        \"강아지가 공원에서 뛰어놉니다\",\n",
        "        \"공원에서 강아지가 뛰어놉니다\"   # 어순이 다른 같은 의미\n",
        "    ]\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(documents)\n",
        "    print('bow_matrix type: ', type(bow_matrix))   # <class 'scipy.sparse._csr.csr_matrix'>\n",
        "\n",
        "    print(\"➡️ BoW의 특성 분석:\")\n",
        "\n",
        "    print(\"\\n1. 어순 무시 특성:\")\n",
        "    # 어순이 다른 문서들의 벡터 비교\n",
        "    bow_array = bow_matrix.toarray()\n",
        "    for i in range(0, len(documents), 2): # 2개씩 비교\n",
        "        vec1 = bow_array[i]\n",
        "        vec2 = bow_array[i+1]\n",
        "        similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "\n",
        "        print(f\"문서 {i+1}: {documents[i]}\")\n",
        "        print(f\"문서 {i+2}: {documents[i+1]}\")\n",
        "        print(f\"유사도: {similarity:.4f} (완전히 같음)\" if similarity == 1.0 else f\"유사도: {similarity:.4f}\")\n",
        "        print()\n",
        "\n",
        "    # 희소성 분석\n",
        "    total_elements = bow_matrix.shape[0] * bow_matrix.shape[1]\n",
        "    non_zero_elements = bow_matrix.nnz\n",
        "    sparsity = 1 - (non_zero_elements / total_elements)\n",
        "\n",
        "    print(f\"2. 희소성(Sparsity) 분석:\")\n",
        "    print(f\"전체 원소 수: {total_elements}\")\n",
        "    print(f\"0이 아닌 원소 수: {non_zero_elements}\")\n",
        "    print(f\"희소성: {sparsity:.4f} ({sparsity*100:.2f}%)\")\n",
        "\n",
        "    # 차원 분석\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    print(f\"\\n3. 차원 분석:\")\n",
        "    print(f\"문서 수: {bow_matrix.shape[0]}\")\n",
        "    print(f\"특성(단어) 수: {bow_matrix.shape[1]}\")\n",
        "    print(f\"특성들: {list(feature_names)}\")\n",
        "\n",
        "\n",
        "# 테스트\n",
        "analyze_bow_characteristics()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mvkW2Wb_HoWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
        "- TF-IDF :\n",
        "    - 문서 내에서 단어의 중요도를 측정하는 가중치 기법으로, 단어 빈도(TF)와 역문서 빈도(IDF)를 결합하여 각 단어가 특정 문서에서 얼마나 중요한지를 수치화함\n",
        "    - TF-IDF 값이 낮으면 중요도가 낮은 것, TF-IDF 값이 크면 중요도가 큰 것\n",
        "    - 참고 : https://wikidocs.net/31698\n",
        "- 등장 배경\n",
        "    - 단순한 단어 빈도만으로는 문서의 주제를 정확히 파악하기 어려웠음\n",
        "    - 예를 들어 \"그리고\", \"그러나\" 같은 단어는 자주 나타나지만 문서의 내용과는 관련이 적음. --> TF-IDF는 이런 문제를 해결하기 위해 개발"
      ],
      "metadata": {
        "id": "psOBzjYiM5AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**기본 TF-IDF 계산 과정:**\n",
        "\n",
        "1. **단어 빈도 (TF) 계산**\n",
        "   - Raw TF: $tf(t,d) = f(t,d)$\n",
        "   - Relative TF: $tf(t,d) = f(t,d) / Σf(w,d)$\n",
        "   - Log TF: $tf(t,d) = 1 + log(f(t,d))$ (f(t,d) > 0인 경우)\n",
        "\n",
        "2. **역문서 빈도 (IDF) 계산**\n",
        "   - Basic IDF: $idf(t) = log(N / df(t))$\n",
        "   - Smooth IDF: $idf(t) = log(N / (1 + df(t)))$\n",
        "   - Probabilistic IDF: $idf(t) = log((N - df(t)) / df(t))$\n",
        "\n",
        "3. **최종 TF-IDF 가중치**\n",
        "   -   $w(t,d) = tf(t,d) × idf(t)$\n",
        "\n",
        "여기서:\n",
        "- t: 단어 (term)\n",
        "- d: 문서 (document)\n",
        "- f(t,d): 문서 d에서 단어 t의 빈도\n",
        "- N: 전체 문서 수\n",
        "- df(t): 단어 t가 포함된 문서 수"
      ],
      "metadata": {
        "id": "bdb83S7QNkus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**정규화 공식**\n",
        "|정규화 방법|공식|목적|\n",
        "|------|---|---|\n",
        "|L1 정규화 | `w'(t,d) = w(t,d) / Σ | w(i,d)  |\n",
        "|L2 정규화| w'(t,d) = w(t,d) / √(Σw(i,d)²)| 벡터의 크기가 1이 되도록 조정|\n",
        "|코사인 정규화| `sim(d1,d2) = (d1·d2) / (| d1 |"
      ],
      "metadata": {
        "id": "hqrlC2fEPDE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : TF-IDF 실전 응용 - 문서 분류 및 클러스터링**"
      ],
      "metadata": {
        "id": "R0jhrNRFSKZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TF-IDF Basic Class(직접 구현)**"
      ],
      "metadata": {
        "id": "VHwtaSi8GetF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# TF-IDF(수식구현 클래스)\n",
        "# ----------------------------\n",
        "class TFIDFImplementation:\n",
        "    def __init__(self, use_log_tf=True, smooth_idf=True, normalize=True):\n",
        "        \"\"\"\n",
        "        TF-IDF 완전 구현 클래스\n",
        "\n",
        "        Args:\n",
        "            use_log_tf: 로그 TF 사용 여부\n",
        "            smooth_idf: 스무스 IDF 사용 여부\n",
        "            normalize: L2 정규화 사용 여부\n",
        "        \"\"\"\n",
        "        self.use_log_tf = use_log_tf\n",
        "        self.smooth_idf = smooth_idf\n",
        "        self.normalize = normalize\n",
        "        self.vocab = {}\n",
        "        self.idf_values = {}\n",
        "        self.documents = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"텍스트 전처리\"\"\"\n",
        "        # 소문자 변환 및 특수문자 제거 (한글 보존)\n",
        "        text = re.sub(r'[^\\w\\s가-힣]', '', text.lower())\n",
        "        return [word for word in text.split() if word.strip()]\n",
        "\n",
        "    def build_vocabulary(self, documents):\n",
        "        \"\"\"어휘 구축 및 IDF 계산\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        # 전체 어휘 수집\n",
        "        all_words = set()\n",
        "        processed_docs = []\n",
        "\n",
        "        for doc in documents:\n",
        "            words = self.preprocess_text(doc)\n",
        "            processed_docs.append(words)\n",
        "            all_words.update(words)\n",
        "\n",
        "        # 어휘 인덱싱\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(all_words))}\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        # 각 단어가 포함된 문서 수 계산\n",
        "        doc_freq = defaultdict(int)\n",
        "        for words in processed_docs:\n",
        "            unique_words = set(words)\n",
        "            for word in unique_words:\n",
        "                doc_freq[word] += 1\n",
        "\n",
        "        # IDF 계산\n",
        "        total_docs = len(documents)\n",
        "        for word in self.vocab:\n",
        "            if self.smooth_idf:\n",
        "                self.idf_values[word] = math.log(total_docs / (1 + doc_freq[word]))\n",
        "            else:\n",
        "                self.idf_values[word] = math.log(total_docs / doc_freq[word]) if doc_freq[word] > 0 else 0\n",
        "\n",
        "        print(f\"어휘 크기: {vocab_size}\")\n",
        "        print(f\"문서 수: {total_docs}\")\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def calculate_tf(self, words):\n",
        "        \"\"\"TF 계산\"\"\"\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "\n",
        "        tf_values = {}\n",
        "        for word in word_count:\n",
        "            if self.use_log_tf:\n",
        "                tf_values[word] = 1 + math.log(word_count[word])\n",
        "            else:\n",
        "                tf_values[word] = word_count[word] / total_words if total_words > 0 else 0\n",
        "\n",
        "        return tf_values\n",
        "\n",
        "    def document_to_vector(self, document):\n",
        "        \"\"\"문서를 TF-IDF 벡터로 변환\"\"\"\n",
        "        words = self.preprocess_text(document)\n",
        "        tf_values = self.calculate_tf(words)\n",
        "\n",
        "        # TF-IDF 벡터 생성\n",
        "        vector = np.zeros(len(self.vocab))\n",
        "\n",
        "        for word, tf in tf_values.items():\n",
        "            if word in self.vocab:\n",
        "                idx = self.vocab[word]\n",
        "                tfidf_value = tf * self.idf_values[word]\n",
        "                vector[idx] = tfidf_value\n",
        "\n",
        "        # L2 정규화\n",
        "        if self.normalize:\n",
        "            norm = np.linalg.norm(vector)\n",
        "            if norm > 0:\n",
        "                vector = vector / norm\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"문서들을 TF-IDF 행렬로 변환\"\"\"\n",
        "        processed_docs = self.build_vocabulary(documents)\n",
        "\n",
        "        # 모든 문서를 벡터로 변환\n",
        "        tfidf_matrix = []\n",
        "        for doc in documents:\n",
        "            vector = self.document_to_vector(doc)\n",
        "            tfidf_matrix.append(vector)\n",
        "\n",
        "        return np.array(tfidf_matrix)\n",
        "\n",
        "    def get_top_words(self, document, top_n=10):\n",
        "        \"\"\"문서에서 TF-IDF 값이 높은 단어들 추출\"\"\"\n",
        "        vector = self.document_to_vector(document)\n",
        "        word_scores = []\n",
        "\n",
        "        for word, idx in self.vocab.items():\n",
        "            if vector[idx] > 0:\n",
        "                word_scores.append((word, vector[idx]))\n",
        "\n",
        "        word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return word_scores[:top_n]\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        \"\"\"특성(단어) 이름 반환\"\"\"\n",
        "        return [word for word, _ in sorted(self.vocab.items(), key=lambda x: x[1])]\n"
      ],
      "metadata": {
        "id": "nT9xJkmXPdnC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 문서들\n",
        "documents = [\n",
        "    \"인공지능은 컴퓨터 과학의 한 분야로 기계가 인간의 지능을 모방하도록 하는 기술입니다\",\n",
        "    \"머신러닝은 인공지능의 하위 분야로 데이터로부터 학습하는 알고리즘을 다룹니다\",\n",
        "    \"딥러닝은 머신러닝의 한 방법으로 신경망을 이용하여 복잡한 패턴을 학습합니다\",\n",
        "    \"자연언어처리는 컴퓨터가 인간의 언어를 이해하고 생성하는 기술입니다\",\n",
        "    \"컴퓨터 비전은 컴퓨터가 이미지와 비디오를 분석하고 이해하는 기술입니다\"\n",
        "]\n",
        "\n",
        "# documents = [\n",
        "#     \"자연언어처리 기술이 빠르게 발전하고 있습니다\",\n",
        "#     \"머신러닝과 딥러닝이 AI 발전을 이끌고 있습니다\",\n",
        "#     \"빅데이터 분석에 다양한 기술이 활용됩니다\",\n",
        "#     \"클라우드 컴퓨팅이 IT 인프라를 혁신하고 있습니다\",\n",
        "#     \"사물인터넷과 스마트시티가 주목받고 있습니다\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "B_LXWLeh4tXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Basic TF-IDF 구현**"
      ],
      "metadata": {
        "id": "DI-sVtmXG72g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. 기본 구현 : TF-IDF(수식구현 클래스) 행렬 생성\n",
        "# ----------------------------\n",
        "def demonstrate_basic_tfidf():\n",
        "    \"\"\"기본 TF-IDF 구현 시연\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"✅ 1. 기본 TF-IDF 구현 시연\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # TF-IDF 모델 생성 및 학습\n",
        "    tfidf_model = TFIDFImplementation()\n",
        "    tfidf_matrix = tfidf_model.fit_transform(documents)\n",
        "\n",
        "    print(f\"TF-IDF 행렬 형태: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # 각 문서별 상위 단어들\n",
        "    print(\"\\n각 문서별 상위 TF-IDF 단어:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        top_words = tfidf_model.get_top_words(doc, top_n=5)\n",
        "        print(f\"\\n문서 {i+1}: {doc[:30]}...\")\n",
        "        for word, score in top_words:\n",
        "            print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "    # IDF 값 분석\n",
        "    print(f\"\\n상위 IDF 값 (희귀한 단어들):\")\n",
        "    idf_sorted = sorted(tfidf_model.idf_values.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, idf in idf_sorted[:10]:\n",
        "        print(f\"  {word}: {idf:.4f}\")\n",
        "\n",
        "    return tfidf_model, tfidf_matrix\n",
        "\n",
        "\n",
        "model1, matrix1 = demonstrate_basic_tfidf()"
      ],
      "metadata": {
        "id": "bWJr_NiA2swd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. sklearn 제공 TF-IDF 구현**"
      ],
      "metadata": {
        "id": "-fbgqwtWHS_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2. sklearn 비교 : TF-IDF(sklearn) 행렬 생성\n",
        "# ----------------------------\n",
        "def demonstrate_sklearn_tfidf():\n",
        "    \"\"\"sklearn TF-IDF와 비교 시연\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 2. sklearn TF-IDF와 성능 비교\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # sklearn TfidfVectorizer\n",
        "    sklearn_tfidf = TfidfVectorizer()\n",
        "    sklearn_matrix = sklearn_tfidf.fit_transform(documents)\n",
        "\n",
        "    print(\"sklearn TF-IDF:\")\n",
        "    print(f\"행렬 형태: {sklearn_matrix.shape}\")\n",
        "    print(f\"특성 수: {len(sklearn_tfidf.get_feature_names_out())}\")\n",
        "\n",
        "    # 문서별 상위 단어 (sklearn)\n",
        "    feature_names = sklearn_tfidf.get_feature_names_out()\n",
        "    sklearn_array = sklearn_matrix.toarray()\n",
        "\n",
        "    print(f\"\\nsklearn 상위 TF-IDF 단어들:\")\n",
        "    for i in range(len(documents)):\n",
        "        tfidf_scores = sklearn_array[i]\n",
        "        top_indices = np.argsort(tfidf_scores)[-5:][::-1]\n",
        "        top_words = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices if tfidf_scores[idx] > 0]\n",
        "        print(f\"문서 {i+1}: {top_words}\")\n",
        "\n",
        "    return sklearn_tfidf, sklearn_matrix\n",
        "\n",
        "model2, matrix2 = demonstrate_sklearn_tfidf()"
      ],
      "metadata": {
        "id": "A4GNl-B42s0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3 .Basic TF-IDF vs sklearn TF-IDF 비교**"
      ],
      "metadata": {
        "id": "YdLor_wNFM4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3. Basic TF-IDF vs sklearn TF-IDF 비교\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "def _dense_row(mat, i):\n",
        "    \"\"\"i번째 문서의 TF-IDF 벡터를 1D numpy array로 반환 (sparse/dense 모두 지원).\"\"\"\n",
        "    if issparse(mat):\n",
        "        return mat.getrow(i).toarray().ravel()\n",
        "    arr = np.asarray(mat)\n",
        "    return arr[i].ravel()\n",
        "\n",
        "def _idf_dict_from_sklearn(model2):\n",
        "    \"\"\"sklearn TfidfVectorizer에서 {term: idf} 사전 얻기.\"\"\"\n",
        "    terms = model2.get_feature_names_out().tolist()\n",
        "    idfs = model2.idf_.tolist()\n",
        "    return dict(zip(terms, idfs))\n",
        "\n",
        "def _weights_dict_from_sklearn_doc(model2, matrix2, doc_idx):\n",
        "    \"\"\"sklearn에서 특정 문서의 TF-IDF를 {term: weight} 사전으로 변환.\"\"\"\n",
        "    terms = model2.get_feature_names_out().tolist()\n",
        "    row = _dense_row(matrix2, doc_idx)\n",
        "    return {t: float(w) for t, w in zip(terms, row) if w != 0.0}\n",
        "\n",
        "def _weights_dict_from_basic_doc(model1, doc_text, max_terms=1_000_000):\n",
        "    \"\"\"\n",
        "    사용자 구현 TFIDFImplementation에서 문서별 TF-IDF를 {term: weight}로 추출.\n",
        "    get_top_words(doc, top_n=...)가 있는 전형적 구현을 가정.\n",
        "    \"\"\"\n",
        "    if hasattr(model1, \"get_top_words\"):\n",
        "        pairs = model1.get_top_words(doc_text, top_n=max_terms)  # [(term, score), ...]\n",
        "        return {t: float(s) for t, s in pairs if s != 0.0}\n",
        "    # fallback: 사용자가 feature_names_ 같은 속성을 제공한 경우 확장 가능\n",
        "    raise AttributeError(\"model1에서 문서별 가중치를 얻는 방법(get_top_words)이 필요합니다.\")\n",
        "\n",
        "def _idf_stats_from_basic(model1):\n",
        "    \"\"\"사용자 구현 모델에서 IDF 통계를 계산 (idf_values: {term: idf} 가정).\"\"\"\n",
        "    if hasattr(model1, \"idf_values\") and isinstance(model1.idf_values, dict):\n",
        "        vals = np.array(list(model1.idf_values.values()), dtype=float)\n",
        "        return {\n",
        "            \"count\": int(vals.size),\n",
        "            \"mean\": float(vals.mean()) if vals.size else np.nan,\n",
        "            \"std\": float(vals.std(ddof=0)) if vals.size else np.nan,\n",
        "            \"min\": float(vals.min()) if vals.size else np.nan,\n",
        "            \"max\": float(vals.max()) if vals.size else np.nan,\n",
        "        }\n",
        "    return {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
        "\n",
        "def _idf_stats_from_sklearn(model2):\n",
        "    vals = np.array(model2.idf_, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(vals.size),\n",
        "        \"mean\": float(vals.mean()),\n",
        "        \"std\": float(vals.std(ddof=0)),\n",
        "        \"min\": float(vals.min()),\n",
        "        \"max\": float(vals.max()),\n",
        "    }\n",
        "\n",
        "def _pearson_on_common_terms(idf_map1, idf_map2):\n",
        "    \"\"\"공통 용어의 IDF 피어슨 상관계수 (용어 불일치 많아도 안전).\"\"\"\n",
        "    common = set(idf_map1.keys()) & set(idf_map2.keys())\n",
        "    if not common:\n",
        "        return {\"n_common\": 0, \"pearson\": np.nan}\n",
        "    v1 = np.array([idf_map1[t] for t in common], dtype=float)\n",
        "    v2 = np.array([idf_map2[t] for t in common], dtype=float)\n",
        "    if v1.std() == 0 or v2.std() == 0:\n",
        "        return {\"n_common\": int(len(common)), \"pearson\": np.nan}\n",
        "    pearson = float(np.corrcoef(v1, v2)[0, 1])\n",
        "    return {\"n_common\": int(len(common)), \"pearson\": pearson}\n",
        "\n",
        "def _topk_overlap_and_jaccard(weights1, weights2, k):\n",
        "    \"\"\"각 문서에서 Top-K 단어 겹침과 자카드.\"\"\"\n",
        "    top1 = [t for t, _ in sorted(weights1.items(), key=lambda x: x[1], reverse=True)[:k]]\n",
        "    top2 = [t for t, _ in sorted(weights2.items(), key=lambda x: x[1], reverse=True)[:k]]\n",
        "    s1, s2 = set(top1), set(top2)\n",
        "    inter = s1 & s2\n",
        "    union = s1 | s2\n",
        "    jaccard = (len(inter) / len(union)) if union else np.nan\n",
        "    return {\n",
        "        \"top1\": top1,\n",
        "        \"top2\": top2,\n",
        "        \"overlap\": sorted(inter),\n",
        "        \"jaccard\": float(jaccard),\n",
        "    }\n",
        "\n",
        "def _cosine_from_weight_dicts(w1, w2):\n",
        "    \"\"\"두 가중치 사전({term: weight})의 코사인 유사도.\"\"\"\n",
        "    if not w1 and not w2:\n",
        "        return np.nan\n",
        "    keys = set(w1.keys()) | set(w2.keys())\n",
        "    a = np.array([w1.get(k, 0.0) for k in keys], dtype=float)\n",
        "    b = np.array([w2.get(k, 0.0) for k in keys], dtype=float)\n",
        "    na = np.linalg.norm(a)\n",
        "    nb = np.linalg.norm(b)\n",
        "    if na == 0.0 or nb == 0.0:\n",
        "        return 0.0\n",
        "    return float(a.dot(b) / (na * nb))\n",
        "\n",
        "def compare_tfidf_models(model1, matrix1, model2, matrix2, documents, top_k=5):\n",
        "    \"\"\"\n",
        "    사용자 구현 TF-IDF(model1/matrix1)과 sklearn TF-IDF(model2/matrix2)를 비교.\n",
        "    - shapes, vocab size\n",
        "    - IDF 통계 / 공통 용어 IDF 상관\n",
        "    - 문서별 Top-K 겹침(Jaccard)\n",
        "    - 문서별 코사인 유사도(사전 기반)\n",
        "    \"\"\"\n",
        "    \"\"\"sklearn TF-IDF와 비교 시연\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 3. 사용자 구현 TF-IDF(model1/matrix1)과 sklearn TF-IDF(model2/matrix2)를 비교\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "    # 1) 기본 메타 비교\n",
        "    shape1 = getattr(matrix1, \"shape\", None)\n",
        "    shape2 = getattr(matrix2, \"shape\", None)\n",
        "\n",
        "    # vocab size 추정\n",
        "    # model1: idf_values가 있으면 그 크기를 사용, 없으면 행렬 열 크기 사용\n",
        "    if hasattr(model1, \"idf_values\") and isinstance(model1.idf_values, dict):\n",
        "        vocab1_size = len(model1.idf_values)\n",
        "        idf_map1 = dict(model1.idf_values)\n",
        "    else:\n",
        "        vocab1_size = shape1[1] if shape1 else None\n",
        "        idf_map1 = {}\n",
        "\n",
        "    vocab2_size = len(model2.get_feature_names_out())\n",
        "    idf_map2 = _idf_dict_from_sklearn(model2)\n",
        "\n",
        "    # 2) IDF 통계 및 상관\n",
        "    idf_stats1 = _idf_stats_from_basic(model1)\n",
        "    idf_stats2 = _idf_stats_from_sklearn(model2)\n",
        "    idf_corr = _pearson_on_common_terms(idf_map1, idf_map2) if idf_map1 else {\"n_common\": 0, \"pearson\": np.nan}\n",
        "\n",
        "    # 3) 문서별 비교\n",
        "    docwise = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        try:\n",
        "            w1 = _weights_dict_from_basic_doc(model1, doc, max_terms=1_000_000)\n",
        "        except Exception:\n",
        "            # fallback: 행렬만으로는 용어-열 매핑을 모르면 사전 구성 불가\n",
        "            w1 = {}\n",
        "\n",
        "        w2 = _weights_dict_from_sklearn_doc(model2, matrix2, i)\n",
        "\n",
        "        overlap = _topk_overlap_and_jaccard(w1, w2, top_k)\n",
        "        cosine = _cosine_from_weight_dicts(w1, w2)\n",
        "\n",
        "        docwise.append({\n",
        "            \"doc_index\": i,\n",
        "            \"doc_preview\": doc[:40] + (\"...\" if len(doc) > 40 else \"\"),\n",
        "            \"topk_overlap\": overlap,\n",
        "            \"cosine_similarity\": cosine,\n",
        "            \"nonzero_terms_basic\": len(w1),\n",
        "            \"nonzero_terms_sklearn\": len(w2),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"matrix_shapes\": {\"basic\": shape1, \"sklearn\": shape2},\n",
        "        \"vocab_sizes\": {\"basic\": vocab1_size, \"sklearn\": vocab2_size},\n",
        "        \"idf_stats\": {\"basic\": idf_stats1, \"sklearn\": idf_stats2},\n",
        "        \"idf_common_correlation\": idf_corr,\n",
        "        \"docwise\": docwise,\n",
        "    }\n",
        "\n",
        "def print_comparison_report(result, top_k=5):\n",
        "    \"\"\"compare_tfidf_models 결과를 간단히 출력.\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"🔎 TF-IDF 모델 비교 리포트\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"- 행렬 크기: basic={result['matrix_shapes']['basic']}, sklearn={result['matrix_shapes']['sklearn']}\")\n",
        "    print(f\"- 어휘수:    basic={result['vocab_sizes']['basic']}, sklearn={result['vocab_sizes']['sklearn']}\")\n",
        "    b = result[\"idf_stats\"][\"basic\"]; s = result[\"idf_stats\"][\"sklearn\"]\n",
        "    print(\"\\n[IDF 요약 통계]\")\n",
        "    print(f\"  • basic   | count={b['count']}, mean={b['mean']:.4f}, std={b['std']:.4f}, min={b['min']:.4f}, max={b['max']:.4f}\")\n",
        "    print(f\"  • sklearn | count={s['count']}, mean={s['mean']:.4f}, std={s['std']:.4f}, min={s['min']:.4f}, max={s['max']:.4f}\")\n",
        "    corr = result[\"idf_common_correlation\"]\n",
        "    print(f\"  • 공통 용어 IDF 상관 (Pearson): n={corr['n_common']}, r={corr['pearson']:.4f}\" if corr[\"n_common\"] else\n",
        "          \"  • 공통 용어 IDF 상관: 공통 용어가 없어 계산 불가\")\n",
        "\n",
        "    print(\"\\n[문서별 Top-{} & 코사인 유사도]\".format(top_k))\n",
        "    for item in result[\"docwise\"]:\n",
        "        ov = item[\"topk_overlap\"]\n",
        "        print(\"-\"*70)\n",
        "        print(f\"문서 {item['doc_index']+1}: {item['doc_preview']}\")\n",
        "        print(f\"  · nonzero terms | basic={item['nonzero_terms_basic']}, sklearn={item['nonzero_terms_sklearn']}\")\n",
        "        print(f\"  · cosine(sim)   | {item['cosine_similarity']:.4f}\")\n",
        "        print(f\"  · top{top_k} overlap({len(ov['overlap'])}개, jaccard={ov['jaccard']:.3f}): {ov['overlap']}\")\n"
      ],
      "metadata": {
        "id": "GIZThDXiFdau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞서 만든 model1, matrix1, model2, matrix2, documents 가 준비되어 있다고 가정\n",
        "result = compare_tfidf_models(model1, matrix1, model2, matrix2, documents, top_k=5)\n",
        "print_comparison_report(result, top_k=5)\n"
      ],
      "metadata": {
        "id": "yDjw4MIQFeq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. TF-IDF로 문서 분류 및 클러스터링**"
      ],
      "metadata": {
        "id": "WrXqxgjBH3Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4. TF-IDF로 문서 분류 및 클러스터링\n",
        "# ----------------------------\n",
        "def demonstrate_tfidf_applications():\n",
        "    \"\"\"TF-IDF 실전 응용 예제\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 4. TF-IDF 실전 응용 - 문서 분류 및 클러스터링\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 카테고리별 문서 데이터\n",
        "    tech_docs = [\n",
        "        \"인공지능과 머신러닝 기술이 급속히 발전하고 있어 다양한 산업에 혁신을 가져오고 있습니다\",\n",
        "        \"딥러닝은 신경망을 기반으로 한 기계학습 방법으로 이미지 인식과 자연언어처리에 뛰어난 성능을 보입니다\",\n",
        "        \"빅데이터 분석 기술을 통해 기업들은 고객의 행동 패턴을 파악하고 맞춤형 서비스를 제공합니다\",\n",
        "        \"클라우드 컴퓨팅 기술로 기업들은 IT 인프라 비용을 절감하면서도 확장성을 확보할 수 있습니다\"\n",
        "    ]\n",
        "\n",
        "    business_docs = [\n",
        "        \"스타트업 기업들이 혁신적인 비즈니스 모델로 시장에 진입하여 큰 성과를 거두고 있습니다\",\n",
        "        \"디지털 마케팅 전략을 통해 기업들은 고객과의 접점을 늘리고 브랜드 인지도를 높이고 있습니다\",\n",
        "        \"글로벌 경제 불확실성 속에서도 일부 기업들은 새로운 시장 기회를 찾아 성장하고 있습니다\",\n",
        "        \"투자자들은 지속가능한 경영과 ESG 요소를 고려하여 투자 결정을 내리는 추세입니다\"\n",
        "    ]\n",
        "\n",
        "    health_docs = [\n",
        "        \"개인 맞춤형 의료 서비스가 발전하면서 질병 예방과 치료 효과가 크게 향상되고 있습니다\",\n",
        "        \"헬스케어 분야에 AI 기술이 도입되어 진단 정확도가 높아지고 있습니다\",\n",
        "        \"정신건강에 대한 관심이 높아지면서 다양한 치료 방법과 예방 프로그램이 개발되고 있습니다\",\n",
        "        \"고령화 사회에 대응하여 실버케어 산업이 새로운 성장 동력으로 주목받고 있습니다\"\n",
        "    ]\n",
        "\n",
        "    # 전체 문서와 라벨\n",
        "    all_docs = tech_docs + business_docs + health_docs\n",
        "    labels = ['기술'] * len(tech_docs) + ['비즈니스'] * len(business_docs) + ['건강'] * len(health_docs)\n",
        "\n",
        "    # TF-IDF 벡터화\n",
        "    vectorizer = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "    print(f\"전체 문서 수: {len(all_docs)}\")\n",
        "    print(f\"TF-IDF 행렬 형태: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # 1. 문서 유사도 분석\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                annot=False,\n",
        "                cmap='Blues',\n",
        "                xticklabels=[f'{labels[i]}{i%4+1}' for i in range(len(all_docs))],\n",
        "                yticklabels=[f'{labels[i]}{i%4+1}' for i in range(len(all_docs))])\n",
        "    plt.title('Document Similarity Heatmap (TF-IDF + Cosine)', fontsize=16, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. 클러스터링\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(tfidf_matrix.toarray())\n",
        "\n",
        "    print(f\"\\n클러스터링 결과:\")\n",
        "    cluster_labels = ['클러스터 A', '클러스터 B', '클러스터 C']\n",
        "    for i, (doc, true_label, pred_cluster) in enumerate(zip(all_docs, labels, clusters)):\n",
        "        print(f\"문서 {i+1} ({true_label}): {cluster_labels[pred_cluster]}\")\n",
        "\n",
        "    # 3. 카테고리별 대표 단어 추출\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f\"\\n카테고리별 대표 단어 (TF-IDF 기준):\")\n",
        "    categories = ['기술', '비즈니스', '건강']\n",
        "    category_vectors = {}\n",
        "\n",
        "    for category in categories:\n",
        "        # 해당 카테고리 문서들의 평균 벡터\n",
        "        category_indices = [i for i, label in enumerate(labels) if label == category]\n",
        "        category_matrix = tfidf_matrix[category_indices]\n",
        "        avg_vector = np.mean(category_matrix.toarray(), axis=0)\n",
        "        category_vectors[category] = avg_vector\n",
        "\n",
        "        # 상위 단어들\n",
        "        top_indices = np.argsort(avg_vector)[-8:][::-1]\n",
        "        top_words = [(feature_names[idx], avg_vector[idx]) for idx in top_indices if avg_vector[idx] > 0]\n",
        "\n",
        "        print(f\"\\n{category} 카테고리:\")\n",
        "        for word, score in top_words:\n",
        "            print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "    return vectorizer, tfidf_matrix, category_vectors\n",
        "\n",
        "vectorizer, tfidf_matrix, category_vectors = demonstrate_tfidf_applications()"
      ],
      "metadata": {
        "id": "0NtIZfRF3Ts_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. TF-IDF 기반 검색 엔진 예제**"
      ],
      "metadata": {
        "id": "SGJoNBCtIfCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. TF-IDF 기반 검색 엔진\n",
        "# ----------------------------\n",
        "def demonstrate_tfidf_search_engine():\n",
        "    \"\"\"TF-IDF 기반 검색 엔진 예제\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 5. TF-IDF 기반 검색 엔진\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 뉴스 기사 예제 데이터\n",
        "    news_articles = [\n",
        "        {\n",
        "            'title': 'AI 기술 발전 현황',\n",
        "            'content': '인공지능 기술이 빠르게 발전하면서 다양한 산업 분야에 혁신을 가져오고 있습니다. 특히 자연언어처리와 컴퓨터 비전 분야에서 괄목할 만한 성과를 보이고 있습니다.',\n",
        "            'category': '기술'\n",
        "        },\n",
        "        {\n",
        "            'title': '스타트업 투자 동향',\n",
        "            'content': '올해 스타트업에 대한 투자가 전년 대비 크게 증가했습니다. 특히 핀테크와 헬스케어 분야에 대한 투자자들의 관심이 높습니다.',\n",
        "            'category': '비즈니스'\n",
        "        },\n",
        "        {\n",
        "            'title': '건강한 식단 관리법',\n",
        "            'content': '균형잡힌 영양소 섭취와 규칙적인 운동이 건강 관리의 핵심입니다. 특히 현대인들에게 부족한 비타민과 미네랄 섭취에 주의해야 합니다.',\n",
        "            'category': '건강'\n",
        "        },\n",
        "        {\n",
        "            'title': '머신러닝 알고리즘 소개',\n",
        "            'content': '머신러닝은 데이터로부터 패턴을 학습하는 알고리즘입니다. 지도학습, 비지도학습, 강화학습 등 다양한 방법론이 있습니다.',\n",
        "            'category': '기술'\n",
        "        },\n",
        "        {\n",
        "            'title': '디지털 마케팅 전략',\n",
        "            'content': '소셜미디어와 검색엔진 마케팅을 통해 고객들과의 접점을 늘리는 것이 중요합니다. 데이터 분석을 통한 타겟 마케팅이 효과적입니다.',\n",
        "            'category': '비즈니스'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # 문서 내용 추출\n",
        "    documents = [article['content'] for article in news_articles]\n",
        "    titles = [article['title'] for article in news_articles]\n",
        "\n",
        "    # TF-IDF 벡터화\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=100)\n",
        "    doc_vectors = vectorizer.fit_transform(documents)\n",
        "\n",
        "    def search_documents(query, top_k=3):\n",
        "        \"\"\"검색 쿼리에 대한 관련 문서 반환\"\"\"\n",
        "        # 쿼리 벡터화\n",
        "        query_vector = vectorizer.transform([query])\n",
        "\n",
        "        # 유사도 계산\n",
        "        similarities = cosine_similarity(query_vector, doc_vectors)[0]\n",
        "\n",
        "        # 상위 k개 문서\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:\n",
        "                results.append({\n",
        "                    'title': titles[idx],\n",
        "                    'content': documents[idx],\n",
        "                    'category': news_articles[idx]['category'],\n",
        "                    'similarity': similarities[idx]\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    # 검색 예제\n",
        "    queries = [\n",
        "        \"인공지능과 머신러닝\",\n",
        "        \"투자와 스타트업\",\n",
        "        \"건강과 영양\",\n",
        "        \"데이터 분석\"\n",
        "    ]\n",
        "\n",
        "    print(\"검색 결과:\")\n",
        "    for query in queries:\n",
        "        print(f\"\\n🔍 검색어: '{query}'\")\n",
        "        results = search_documents(query, top_k=2)\n",
        "\n",
        "        if not results:\n",
        "            print(\" ❌ 검색 결과가 없습니다.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"  {i+1}. {result['title']} (유사도: {result['similarity']:.4f})\")\n",
        "            print(f\"     카테고리: {result['category']}\")\n",
        "            print(f\"     내용: {result['content'][:50]}...\")\n",
        "\n",
        "demonstrate_tfidf_search_engine()\n"
      ],
      "metadata": {
        "id": "0uaxq4cH3Tw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. TF-IDF 특성 분석**"
      ],
      "metadata": {
        "id": "x-mU78ugJzPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. TF-IDF 특성 분석\n",
        "# ----------------------------\n",
        "def analyze_tfidf_properties():\n",
        "    \"\"\"TF-IDF 속성 분석\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✅ 6. TF-IDF 특성 분석\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 분석용 문서들\n",
        "    documents = [\n",
        "        \"컴퓨터 컴퓨터 컴퓨터 과학 기술\",  # 높은 TF\n",
        "        \"양자 암호학 블록체인\",  # 희귀 단어들\n",
        "        \"그리고 그러나 또한 하지만 그래서\",  # 일반적인 단어들\n",
        "        \"인공지능 머신러닝 딥러닝 자연언어처리\"  # 기술 용어들\n",
        "    ]\n",
        "\n",
        "    # 커스텀 TF-IDF와 sklearn 비교\n",
        "    custom_tfidf = TFIDFImplementation()\n",
        "    custom_matrix = custom_tfidf.fit_transform(documents)\n",
        "\n",
        "    sklearn_tfidf = TfidfVectorizer()\n",
        "    sklearn_matrix = sklearn_tfidf.fit_transform(documents)\n",
        "\n",
        "    print(\"문서별 TF-IDF 분석:\")\n",
        "\n",
        "    for i, doc in enumerate(documents):\n",
        "        print(f\"\\n문서 {i+1}: {doc}\")\n",
        "\n",
        "        # 커스텀 구현 결과\n",
        "        top_words_custom = custom_tfidf.get_top_words(doc, top_n=3)\n",
        "        print(f\"커스텀 구현 상위 단어: {top_words_custom}\")\n",
        "\n",
        "        # sklearn 결과\n",
        "        sklearn_vector = sklearn_matrix[i].toarray()[0]\n",
        "        feature_names = sklearn_tfidf.get_feature_names_out()\n",
        "        sklearn_scores = [(feature_names[idx], sklearn_vector[idx])\n",
        "                         for idx in np.argsort(sklearn_vector)[-3:][::-1]\n",
        "                         if sklearn_vector[idx] > 0]\n",
        "        print(f\"sklearn 상위 단어: {sklearn_scores}\")\n",
        "\n",
        "    # TF vs IDF 분석 시각화\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # TF 분포\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sample_doc = \"인공지능 기술이 발전하면서 인공지능 응용 분야가 확대되고 있습니다 인공지능\"\n",
        "    words = custom_tfidf.preprocess_text(sample_doc)\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    plt.bar(word_freq.keys(), word_freq.values(), color='skyblue')\n",
        "    plt.title('Word Frequency (TF)', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # IDF 분포\n",
        "    plt.subplot(2, 2, 2)\n",
        "    idf_words = list(custom_tfidf.idf_values.keys())[:10]\n",
        "    idf_vals = [custom_tfidf.idf_values[word] for word in idf_words]\n",
        "\n",
        "    plt.bar(idf_words, idf_vals, color='lightcoral')\n",
        "    plt.title('Inverse Document Frequency (IDF)', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('IDF Values')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # TF-IDF 최종 점수\n",
        "    plt.subplot(2, 2, 3)\n",
        "    tfidf_vector = custom_tfidf.document_to_vector(sample_doc)\n",
        "    top_indices = np.argsort(tfidf_vector)[-5:][::-1]\n",
        "    top_words = [list(custom_tfidf.vocab.keys())[i] for i in top_indices if tfidf_vector[i] > 0]\n",
        "    top_scores = [tfidf_vector[i] for i in top_indices if tfidf_vector[i] > 0]\n",
        "\n",
        "    plt.bar(top_words, top_scores, color='lightgreen')\n",
        "    plt.title('TF-IDF Scores', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('TF-IDF Values')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # 문서 길이별 TF-IDF 영향 분석\n",
        "    plt.subplot(2, 2, 4)\n",
        "    doc_lengths = [len(doc.split()) for doc in documents]\n",
        "    max_tfidf_scores = [np.max(custom_matrix[i]) for i in range(len(documents))]\n",
        "\n",
        "    plt.scatter(doc_lengths, max_tfidf_scores, s=100, alpha=0.7)\n",
        "    plt.title('Document Length vs Max TF-IDF Score', fontsize=14)\n",
        "    plt.xlabel('Document Length (words)')\n",
        "    plt.ylabel('Max TF-IDF Score')\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(doc_lengths, max_tfidf_scores)):\n",
        "        plt.annotate(f'Doc{i+1}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "analyze_tfidf_properties()\n"
      ],
      "metadata": {
        "id": "dfwzxdCw3T7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: TF-IDF와 코사인 유사도를 이용한 영화 추천 시스템**"
      ],
      "metadata": {
        "id": "4epk_JtzIL1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "9cZ00ARZI5PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class KoreanMovieRecommendationSystem:\n",
        "    def __init__(self):\n",
        "        self.movies_df = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.cosine_sim = None\n",
        "        self.okt = Okt()\n",
        "\n",
        "    def download_movie_data(self):\n",
        "\n",
        "        # 예시 한글 영화 데이터 생성\n",
        "        sample_movies = [\n",
        "            {\n",
        "                'title': '기생충',\n",
        "                'plot': '반지하에 살던 기택 가족이 부유한 박 사장 가족의 집에 취업하면서 벌어지는 계급 갈등과 사회적 모순을 그린 작품. 가난한 가족과 부유한 가족 사이의 첨예한 대립과 예상치 못한 사건들이 연쇄적으로 일어난다.',\n",
        "                'genre': '드라마, 스릴러',\n",
        "                'year': 2019\n",
        "            },\n",
        "            {\n",
        "                'title': '올드보이',\n",
        "                'plot': '15년간 감금당한 오대수가 복수를 위해 자신을 가둔 자를 찾아나서는 복수극. 미스터리한 감금의 이유와 충격적인 진실이 밝혀지는 강렬한 스릴러 작품이다.',\n",
        "                'genre': '스릴러, 미스터리',\n",
        "                'year': 2003\n",
        "            },\n",
        "            {\n",
        "                'title': '아가씨',\n",
        "                'plot': '일제강점기 조선을 배경으로 한 귀족 아가씨와 하녀, 그리고 사기꾼이 얽힌 복잡한 사기극과 사랑 이야기. 반전에 반전을 거듭하는 정교한 플롯과 아름다운 영상미가 돋보인다.',\n",
        "                'genre': '드라마, 스릴러',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': '곡성',\n",
        "                'plot': '평화로운 시골 마을에 일본인 남자가 나타난 후 기이한 질병과 살인사건이 연이어 발생한다. 경찰관 종구가 딸을 구하기 위해 미스터리의 진상을 파헤치는 공포 스릴러.',\n",
        "                'genre': '공포, 미스터리',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': '타짜',\n",
        "                'plot': '화투계의 전설적인 도박사들과 신출내기 곰이 만나 벌이는 속임수와 배신의 이야기. 치밀한 심리전과 화투 게임의 세계를 생생하게 그린 작품.',\n",
        "                'genre': '범죄, 드라마',\n",
        "                'year': 2006\n",
        "            },\n",
        "            {\n",
        "                'title': '살인의 추억',\n",
        "                'plot': '1980년대 경기도 화성에서 연쇄살인사건이 일어나고, 무능한 경찰들이 범인을 찾기 위해 고군분투하는 과정을 그린 블랙 코미디 스릴러.',\n",
        "                'genre': '스릴러, 범죄',\n",
        "                'year': 2003\n",
        "            },\n",
        "            {\n",
        "                'title': '부산행',\n",
        "                'plot': '좀비 바이러스가 전국으로 확산되는 상황에서 KTX를 타고 부산으로 향하는 사람들의 생존기. 가족애와 인간성을 그린 좀비 액션 영화.',\n",
        "                'genre': '액션, 공포',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': '신과함께-죄와벌',\n",
        "                'plot': '사후세계를 배경으로 죽은 자가 7개의 지옥에서 재판을 받으며 환생을 위해 노력하는 이야기. 한국적 판타지와 가족애를 담은 작품.',\n",
        "                'genre': '판타지, 드라마',\n",
        "                'year': 2017\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.movies_df = pd.DataFrame(sample_movies)\n",
        "        print(\"영화 데이터 로드 완료!\")\n",
        "        print(f\"총 {len(self.movies_df)}편의 영화 데이터\")\n",
        "        return self.movies_df\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        한글 텍스트 전처리\n",
        "        \"\"\"\n",
        "        # 특수문자 제거\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # 형태소 분석 및 명사, 동사, 형용사만 추출\n",
        "        tokens = self.okt.pos(text)\n",
        "        meaningful_words = []\n",
        "\n",
        "        for word, pos in tokens:\n",
        "            if pos in ['Noun', 'Verb', 'Adjective'] and len(word) > 1:\n",
        "                meaningful_words.append(word)\n",
        "\n",
        "        return ' '.join(meaningful_words)\n",
        "\n",
        "    def create_tfidf_matrix(self):\n",
        "        \"\"\"\n",
        "        TF-IDF 행렬 생성\n",
        "        \"\"\"\n",
        "        # 줄거리 텍스트 전처리\n",
        "        processed_plots = [self.preprocess_text(plot) for plot in self.movies_df['plot']]\n",
        "\n",
        "        # TF-IDF 벡터화\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,  # 최대 특성 수\n",
        "            stop_words=None,    # 한글은 별도의 불용어 처리\n",
        "            ngram_range=(1, 2)  # 1-gram, 2-gram 사용\n",
        "        )\n",
        "\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_plots)\n",
        "        print(f\"TF-IDF 행렬 크기: {self.tfidf_matrix.shape}\")\n",
        "\n",
        "    def calculate_similarity(self):\n",
        "        \"\"\"\n",
        "        코사인 유사도 계산\n",
        "        \"\"\"\n",
        "        self.cosine_sim = cosine_similarity(self.tfidf_matrix, self.tfidf_matrix)\n",
        "        print(\"코사인 유사도 계산 완료!\")\n",
        "\n",
        "    def get_recommendations(self, movie_title, num_recommendations=3):\n",
        "        \"\"\"\n",
        "        영화 추천 함수\n",
        "        \"\"\"\n",
        "        # 영화 제목으로 인덱스 찾기\n",
        "        try:\n",
        "            movie_idx = self.movies_df[self.movies_df['title'] == movie_title].index[0]\n",
        "        except IndexError:\n",
        "            print(f\"'{movie_title}' 영화를 찾을 수 없습니다.\")\n",
        "            return None\n",
        "\n",
        "        # 해당 영화와 다른 영화들 간의 유사도 점수 계산\n",
        "        sim_scores = list(enumerate(self.cosine_sim[movie_idx]))\n",
        "\n",
        "        # 유사도 점수로 정렬 (자기 자신 제외)\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:]\n",
        "\n",
        "        # 상위 N개 영화의 인덱스 추출\n",
        "        top_movies_indices = [i[0] for i in sim_scores[:num_recommendations]]\n",
        "\n",
        "        # 추천 영화 정보 반환\n",
        "        recommendations = self.movies_df.iloc[top_movies_indices][['title', 'plot', 'genre', 'year']].copy()\n",
        "        recommendations['similarity_score'] = [sim_scores[i][1] for i in range(num_recommendations)]\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def run_recommendation_system(self):\n",
        "        \"\"\"\n",
        "        추천 시스템 실행\n",
        "        \"\"\"\n",
        "        print(\"=== 한글 영화 추천 시스템 ===\\n\")\n",
        "\n",
        "        # 1. 데이터 로드\n",
        "        self.download_movie_data()\n",
        "\n",
        "        # 2. TF-IDF 행렬 생성\n",
        "        print(\"\\n텍스트 전처리 및 TF-IDF 행렬 생성 중...\")\n",
        "        self.create_tfidf_matrix()\n",
        "\n",
        "        # 3. 코사인 유사도 계산\n",
        "        print(\"\\n코사인 유사도 계산 중...\")\n",
        "        self.calculate_similarity()\n",
        "\n",
        "        print(\"\\n=== 추천 시스템 준비 완료 ===\")\n",
        "        return True\n",
        "\n",
        "# 시스템 실행 및 테스트\n",
        "def main():\n",
        "    # 추천 시스템 초기화 및 실행\n",
        "    recommender = KoreanMovieRecommendationSystem()\n",
        "    recommender.run_recommendation_system()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"영화 추천 테스트\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 테스트용 영화들\n",
        "    test_movies = ['기생충', '올드보이', '부산행']\n",
        "\n",
        "    for movie in test_movies:\n",
        "        print(f\"\\n✅ '{movie}'와 유사한 영화 추천:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        recommendations = recommender.get_recommendations(movie, 3)\n",
        "\n",
        "        if recommendations is not None:\n",
        "            for idx, row in recommendations.iterrows():\n",
        "                print(f\"🎬 {row['title']} ({row['year']})\")\n",
        "                print(f\"   장르: {row['genre']}\")\n",
        "                print(f\"   유사도: {row['similarity_score']:.4f}\")\n",
        "                print(f\"   줄거리: {row['plot'][:100]}...\")\n",
        "                print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GQgj0APBIP4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cv61nvoEM5tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word2Vec**"
      ],
      "metadata": {
        "id": "103amdw1GdG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **임베딩(Embedding)** :\n",
        "    - 데이터의 의미를 포착하기 위한 **벡터 표현**\n",
        "- **단어 임베딩(Word Embedding)**\n",
        "    - 단어 하나를 고차원 벡터 공간의 점으로 표현, 의미적으로 유사한 단어들이 가깝게 위치함\n",
        "- **Word2Vec** :\n",
        "    - 2013년 Google의 Tomas Mikolov가 개발한 단어 임베딩(Word Embedding) 기법\n",
        "    - 단어를 고차원 벡터 공간에서 밀집된 실수 벡터로 표현하는 방법\n",
        "    - **Word2Vec 는 텍스트의 의미를 포착하는 데 성공한 첫 번째 시도**\n",
        "    - **신경망을 사용**해 주어진 **문장에서 다음에 어떤 단어가 등장하는지** 살펴봄으로써 단어 임베딩을 생성함 --> **의미 표현을 생성함**\n",
        "- **Word2Vec 방법**:\n",
        "    - 어휘사전에 있는 모든 단어에 대해 랜덤하게 초기화된 일련의 값을 단어 임베딩으로 할당\n",
        "    - 훈련 스텝마다 훈련 데이터에서 단어 쌍을 가져와 모델이 문장 안에서 단어 쌍이 이웃에 나타날 가능성이 있는지 예측\n",
        "    - 훈련 과정 동안 Word2Vec 는 단어 사이 관계를 학습하고 이 정보를 임베딩에 저장\n",
        "    - 두 단어가 이웃에 나타날 가능성이 높다면 두 단어의 임베딩은 서로 매우 가까워짐"
      ],
      "metadata": {
        "id": "39IocrbFHJGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : word2vec로 문장 유사도 확인**\n",
        "\n",
        "- **작업 순서**\n",
        "  1) 말뭉치 구성 & 전처리\n",
        "  2) Word2Vec (CBOW vs Skip-gram) 학습\n",
        "  3) 유사어 탐색, 유사도 계산, 간단 유추(analogy)\n",
        "  4) 문서 임베딩(평균)으로 유사도 히트맵\n",
        "  5) t-SNE로 단어 벡터 2D 시각화\n",
        "- **gensim** 라이브러리\n",
        "    - 대규모 텍스트를 효율적으로 처리하고 Word2Vec, LDA 같은 토픽 모델링과 임베딩을 쉽게 구현할 수 있는 Python 라이브러리"
      ],
      "metadata": {
        "id": "zxibfPJDSyWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. 말뭉치 구성 & 전처리**\n",
        "- 토큰화(Tokenization): 문장을 단어 단위로 쪼개는 과정\n",
        "- 소문자 변환: 대소문자를 통일하여 'AI'와 'ai'를 같은 단어로 인식하게함"
      ],
      "metadata": {
        "id": "OK_EeYrQKSlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "OOwxJZgFGgwY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLTK 리소스 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. 말뭉치 구성\n",
        "corpus = [\n",
        "    \"I like to eat pizza and pasta.\",\n",
        "    \"I enjoy eating sushi and noodles.\",\n",
        "    \"Cats and dogs are common pets.\",\n",
        "    \"Tigers and lions are wild animals.\",\n",
        "    \"AI is a revolutionary technology.\",\n",
        "    \"Machine learning is a subfield of AI.\",\n",
        "    \"Pizza is a popular dish.\",\n",
        "    \"Dogs are loyal companions.\",\n",
        "    \"Technology is advancing rapidly.\"\n",
        "]\n",
        "\n",
        "# 2. 전처리: 토큰화 및 소문자 변환\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "print(\"--- 전처리된 말뭉치(일부) ---\")\n",
        "print(tokenized_corpus[0])\n",
        "print(tokenized_corpus[2])\n"
      ],
      "metadata": {
        "id": "4nSI_sPcKS0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Word2Vec (CBOW vs Skip-gram) 학습**\n",
        "- Word2Vec 모델을 학습시킴\n",
        "- **CBOW** (Continuous Bag of Words): 주변 단어들을 이용해 중간에 있는 단어를 예측하는 방식\n",
        "- **Skip-gram**: 어떤 단어를 이용해 주변 단어들을 예측하는 방식, 보통 Skip-gram이 더 작은 데이터셋에서 좋은 성능을 보임"
      ],
      "metadata": {
        "id": "AlSWMbvEKS-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Word2Vec 모델 학습\n",
        "# CBOW 모델\n",
        "cbow_model = Word2Vec(\n",
        "    sentences=tokenized_corpus, # 학습할 말뭉치 (토큰화된 문장 리스트)\n",
        "    vector_size=100, # 단어 벡터의 차원\n",
        "    window=5,        # 학습 시 고려할 주변 단어의 범위\n",
        "    min_count=1,     # 최소 등장 빈도 (이 미만 단어는 무시)\n",
        "    sg=0,            # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Skip-gram 모델\n",
        "skipgram_model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,          # 1: Skip-gram\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- CBOW 모델 학습 완료 ---\")\n",
        "print(\"벡터 크기:\", cbow_model.vector_size)\n",
        "print(\"단어 수:\", len(cbow_model.wv))\n",
        "\n",
        "print(\"\\n--- Skip-gram 모델 학습 완료 ---\")\n",
        "print(\"벡터 크기:\", skipgram_model.vector_size)\n",
        "print(\"단어 수:\", len(skipgram_model.wv))"
      ],
      "metadata": {
        "id": "roKR2GpfKTHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **tokenized_corpus에 매핑된 skipgram_model.wv 출력**\n"
      ],
      "metadata": {
        "id": "uwH5zKQCTTMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(tokenized_corpus):\n",
        "    print(f\"\\n--- 문장 {i+1} ---\")\n",
        "    for word in sentence:\n",
        "        if word in skipgram_model.wv:\n",
        "            print(f\"{word:10s} -> {skipgram_model.wv[word][:5]}\")  # 앞 5개 값만 표시\n"
      ],
      "metadata": {
        "id": "FaYof-hLST9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. 유사어 탐색, 유사도 계산, 간단 유추(analogy)**\n",
        "- 학습된 모델을 활용하여 단어 간의 유사도를 계산하고, 흥미로운 단어 유추(Analogy) 실험\n",
        "- most_similar(): 주어진 단어와 코사인 유사도가 가장 높은 단어들을 찾는다.\n",
        "- similarity(): 두 단어의 벡터 간의 코사인 유사도를 계산하여 의미적 유사도를 측정함\n",
        "- 유추: 벡터 연산(vector('king') - vector('man') + vector('woman'))을 통해 단어 간의 관계를 추론한다."
      ],
      "metadata": {
        "id": "6tcgLGN1KTOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 유사어 탐색 (가장 비슷한 단어 찾기)\n",
        "print(\"\\n✅ 'pizza'와 가장 비슷한 단어 (Skip-gram) ---\")\n",
        "similar_words = skipgram_model.wv.most_similar(\"pizza\", topn=5)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word}: {similarity:.4f}\")\n",
        "\n",
        "\n",
        "# 5. 유사도 계산 (두 단어 간의 코사인 유사도)\n",
        "print(\"\\n✅ 'dogs'와 'cats'의 유사도 ---\")\n",
        "similarity_score = skipgram_model.wv.similarity('dogs', 'cats')\n",
        "print(f\"  유사도 점수: {similarity_score:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n✅ 'pizza'와 'dogs'의 유사도 ---\")\n",
        "similarity_score = skipgram_model.wv.similarity('pizza', 'dogs')\n",
        "print(f\"  유사도 점수: {similarity_score:.4f}\")\n",
        "\n",
        "\n",
        "# 6. 간단한 유추(analogy): \"King - Man + Woman = ?\"\n",
        "\n",
        "# 안전 유사/유추 헬퍼\n",
        "print(\"\\n✅ King - Man + Woman 유추 ---\")\n",
        "def check_tokens(kv, tokens):\n",
        "    missing = [t for t in tokens if t not in kv.key_to_index]\n",
        "    return missing\n",
        "\n",
        "def safe_most_similar(kv, positive, negative=None, topn=1):\n",
        "    negative = negative or []\n",
        "    missing = check_tokens(kv, positive + negative)\n",
        "    if missing:\n",
        "        raise KeyError(f\"사전에 없는 단어: {missing}\")\n",
        "    return kv.most_similar(positive=positive, negative=negative, topn=topn)\n",
        "\n",
        "# 먼저 현재 skip-gram 모델로 시도\n",
        "try:\n",
        "    analogy_result = safe_most_similar(\n",
        "        skipgram_model.wv, positive=['woman','king'], negative=['man'], topn=1\n",
        "    )\n",
        "    print(\" (내 말뭉치) 유추:\", analogy_result[0])\n",
        "except KeyError as e:\n",
        "    print(\"⚠️ 현재 Word2Vec 어휘에 단어가 없습니다 -> GloVe로 폴백합니다.\", e)\n",
        "\n",
        "    # GloVe 100d 로 폴백\n",
        "    import gensim.downloader as api\n",
        "    glove = api.load(\"glove-wiki-gigaword-100\")  # 약 128MB\n",
        "    analogy_result = glove.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
        "    print(\"\\n✅ (GloVe) 유추:\", analogy_result[0])\n"
      ],
      "metadata": {
        "id": "ZbTuXdheKTWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. 문서 임베딩(평균) 이후 유사도를 히트맵으로 시각화**\n",
        "    \n",
        "- 단어 임베딩을 이용해 문장(문서)의 임베딩을 만들고, 문장 간의 유사도를 히트맵으로 시각화함. --> 여기서는 각 문장의 단어 벡터들의 평균을 사용함\n",
        "- 음식에 대한 문장, 동물에 대한 문장들이 높은 유사도를 보"
      ],
      "metadata": {
        "id": "QLg7nSC6KTcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 문서 임베딩(평균) 계산\n",
        "def get_sentence_vector(sentence, model):\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if not vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "sentence_vectors = np.array([get_sentence_vector(s, skipgram_model) for s in tokenized_corpus])\n",
        "print(\"\\n--- 문장 벡터 크기 ---\")\n",
        "print(sentence_vectors.shape)\n",
        "\n",
        "\n",
        "# 8. 코사인 유사도 계산\n",
        "similarity_matrix = cosine_similarity(sentence_vectors)\n",
        "\n",
        "\n",
        "# 9. 유사도 히트맵 시각화(문장 간 코사인 유사도 히트맵)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap=\"YlGnBu\",\n",
        "            xticklabels=[f\"Doc {i+1}\" for i in range(len(corpus))],\n",
        "            yticklabels=[f\"Doc {i+1}\" for i in range(len(corpus))])\n",
        "plt.title(\"Cosine similarity heatmap between sentences\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nsKSS983KTkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. t-SNE로 단어 벡터 2D 시각화**\n",
        "    \n",
        "- **t-SNE**(t-Distributed Stochastic Neighbor Embedding) 는 2008년 Geoffrey Hinton 연구팀이 제안.\n",
        "- **t-SNE**는 고차원 데이터를 저차원(보통 2D나 3D)으로 시각화하기 위해 자주 사용하는 비선형 차원 축소  --> **확률적 분포간 유사도**\n",
        "- 고차원의 단어 벡터를 2차원으로 축소하여 시각화함으로써, **의미적으로 유사한 단어들이 서로 가깝게 위치**하는 것을 직관적으로 볼 수 있음\n",
        "- 'pizza'와 'pasta', 'cats'와 'dogs' 같은 유사한 단어들이 서로 가까운 곳에 위치하는가???"
      ],
      "metadata": {
        "id": "pybl_aXVKTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. t-SNE를 이용한 단어 벡터 2D 시각화\n",
        "\n",
        "# 단어와 벡터 준비\n",
        "words = [word for word in skipgram_model.wv.key_to_index if len(word) > 2]\n",
        "vectors = np.array([skipgram_model.wv[word] for word in words])  # 리스트 → 넘파이 배열\n",
        "\n",
        "# t-SNE 변환\n",
        "# tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
        "\n",
        "vectors_2d = tsne.fit_transform(vectors)\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), ha='center', va='center')\n",
        "\n",
        "plt.title(\"Word2Vec t-SNE visualization\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8D5FftF_WlOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ5vOUjDYJCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: gensim 라이브러리를 이용한 Word2Vec로 문장 유사도 확인**\n",
        "\n",
        "- gensim (generate + similarity)\n",
        "    - 대규모 텍스트 말뭉치(Corpus)를 효율적으로 처리하고, 토픽 모델링·문서 유사도 분석·단어 임베딩 등을 손쉽게 수행\n",
        "- 대표기능\n",
        "    - Word2Vec, FastText: 단어 벡터 학습 및 유사어 찾기\n",
        "    - Doc2Vec: 문서 임베딩\n",
        "    - Topic Modeling: LDA, LSI\n",
        "    - 유사도 검색: 문서 간 유사도 계산, 검색 엔진 구현"
      ],
      "metadata": {
        "id": "aXA7UflTYtxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim==4.3.2 scipy==1.12.0"
      ],
      "metadata": {
        "id": "0zyO9cmzXu5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 샘플 말뭉치\n",
        "sentences = [\n",
        "    [\"인공지능\", \"머신러닝\", \"딥러닝\"],\n",
        "    [\"자연어처리\", \"컴퓨터\", \"언어\"],\n",
        "    [\"투자\", \"스타트업\", \"비즈니스\"]\n",
        "]\n",
        "\n",
        "# 모델 학습 (Skip-gram)\n",
        "model = Word2Vec(\n",
        "    sentences,      # 학습할 말뭉치 (토큰화된 문장 리스트)\n",
        "    vector_size=50, # 단어 벡터의 차원\n",
        "    window=3,       # 학습 시 고려할 주변 단어의 범위\n",
        "    sg=1,           # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
        "    min_count=1     # 최소 등장 빈도 (이 미만 단어는 무시)\n",
        ")\n",
        "\n",
        "\n",
        "# 유사어 탐색\n",
        "model.wv.most_similar(\"인공지능\")\n"
      ],
      "metadata": {
        "id": "BiQWcXsmZ71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. 말중치 구성 & 정처리**"
      ],
      "metadata": {
        "id": "c7Fe4Dyhuha0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Iterable\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) 말뭉치 구성: TF-IDF 예제의 문장들을 그대로 사용 (한국어 유지)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "documents_basic = [\n",
        "    \"인공지능은 컴퓨터 과학의 한 분야로 기계가 인간의 지능을 모방하도록 하는 기술입니다\",\n",
        "    \"머신러닝은 인공지능의 하위 분야로 데이터로부터 학습하는 알고리즘을 다룹니다\",\n",
        "    \"딥러닝은 머신러닝의 한 방법으로 신경망을 이용하여 복잡한 패턴을 학습합니다\",\n",
        "    \"자연언어처리는 컴퓨터가 인간의 언어를 이해하고 생성하는 기술입니다\",\n",
        "    \"컴퓨터 비전은 컴퓨터가 이미지와 비디오를 분석하고 이해하는 기술입니다\"\n",
        "]\n",
        "\n",
        "documents_compare = [\n",
        "    \"자연언어처리 기술이 빠르게 발전하고 있습니다\",\n",
        "    \"머신러닝과 딥러닝이 AI 발전을 이끌고 있습니다\",\n",
        "    \"빅데이터 분석에 다양한 기술이 활용됩니다\",\n",
        "    \"클라우드 컴퓨팅이 IT 인프라를 혁신하고 있습니다\",\n",
        "    \"사물인터넷과 스마트시티가 주목받고 있습니다\"\n",
        "]\n",
        "\n",
        "tech_docs = [\n",
        "    \"인공지능과 머신러닝 기술이 급속히 발전하고 있어 다양한 산업에 혁신을 가져오고 있습니다\",\n",
        "    \"딥러닝은 신경망을 기반으로 한 기계학습 방법으로 이미지 인식과 자연언어처리에 뛰어난 성능을 보입니다\",\n",
        "    \"빅데이터 분석 기술을 통해 기업들은 고객의 행동 패턴을 파악하고 맞춤형 서비스를 제공합니다\",\n",
        "    \"클라우드 컴퓨팅 기술로 기업들은 IT 인프라 비용을 절감하면서도 확장성을 확보할 수 있습니다\"\n",
        "]\n",
        "\n",
        "business_docs = [\n",
        "    \"스타트업 기업들이 혁신적인 비즈니스 모델로 시장에 진입하여 큰 성과를 거두고 있습니다\",\n",
        "    \"디지털 마케팅 전략을 통해 기업들은 고객과의 접점을 늘리고 브랜드 인지도를 높이고 있습니다\",\n",
        "    \"글로벌 경제 불확실성 속에서도 일부 기업들은 새로운 시장 기회를 찾아 성장하고 있습니다\",\n",
        "    \"투자자들은 지속가능한 경영과 ESG 요소를 고려하여 투자 결정을 내리는 추세입니다\"\n",
        "]\n",
        "\n",
        "health_docs = [\n",
        "    \"개인 맞춤형 의료 서비스가 발전하면서 질병 예방과 치료 효과가 크게 향상되고 있습니다\",\n",
        "    \"헬스케어 분야에 AI 기술이 도입되어 진단 정확도가 높아지고 있습니다\",\n",
        "    \"정신건강에 대한 관심이 높아지면서 다양한 치료 방법과 예방 프로그램이 개발되고 있습니다\",\n",
        "    \"고령화 사회에 대응하여 실버케어 산업이 새로운 성장 동력으로 주목받고 있습니다\"\n",
        "]\n",
        "\n",
        "news_articles = [\n",
        "    {\n",
        "        'title': 'AI 기술 발전 현황',\n",
        "        'content': '인공지능 기술이 빠르게 발전하면서 다양한 산업 분야에 혁신을 가져오고 있습니다. 특히 자연언어처리와 컴퓨터 비전 분야에서 괄목할 만한 성과를 보이고 있습니다.',\n",
        "        'category': '기술'\n",
        "    },\n",
        "    {\n",
        "        'title': '스타트업 투자 동향',\n",
        "        'content': '올해 스타트업에 대한 투자가 전년 대비 크게 증가했습니다. 특히 핀테크와 헬스케어 분야에 대한 투자자들의 관심이 높습니다.',\n",
        "        'category': '비즈니스'\n",
        "    },\n",
        "    {\n",
        "        'title': '건강한 식단 관리법',\n",
        "        'content': '균형잡힌 영양소 섭취와 규칙적인 운동이 건강 관리의 핵심입니다. 특히 현대인들에게 부족한 비타민과 미네랄 섭취에 주의해야 합니다.',\n",
        "        'category': '건강'\n",
        "    },\n",
        "    {\n",
        "        'title': '머신러닝 알고리즘 소개',\n",
        "        'content': '머신러닝은 데이터로부터 패턴을 학습하는 알고리즘입니다. 지도학습, 비지도학습, 강화학습 등 다양한 방법론이 있습니다.',\n",
        "        'category': '기술'\n",
        "    },\n",
        "    {\n",
        "        'title': '디지털 마케팅 전략',\n",
        "        'content': '소셜미디어와 검색엔진 마케팅을 통해 고객들과의 접점을 늘리는 것이 중요합니다. 데이터 분석을 통한 타겟 마케팅이 효과적입니다.',\n",
        "        'category': '비즈니스'\n",
        "    }\n",
        "]\n",
        "\n",
        "all_docs_texts = (\n",
        "    documents_basic +\n",
        "    documents_compare +\n",
        "    tech_docs + business_docs + health_docs +\n",
        "    [a['content'] for a in news_articles]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XnHtYRhJutHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. 전처리 & 토큰화 (한글 보존 + 간단 정규식)**"
      ],
      "metadata": {
        "id": "v1ROGQ3OuxCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 2) 전처리 & 토큰화 (한글 보존 + 간단 정규식)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def tokenize_ko(text: str) -> List[str]:\n",
        "    # 소문자 + 한글/영문/숫자/공백만 남김\n",
        "    text = re.sub(r'[^\\w\\s가-힣]', ' ', text.lower())\n",
        "    tokens = [t for t in text.split() if t.strip()]\n",
        "    return tokens\n",
        "\n",
        "corpus: List[List[str]] = [tokenize_ko(t) for t in all_docs_texts]\n",
        "\n",
        "print(f\"문서 수: {len(corpus)}\")\n",
        "print(\"샘플 토큰:\", corpus[0][:15])\n",
        "print()\n",
        "\n",
        "# corpus(문서) 출력\n",
        "# for i, c in enumerate(corpus):\n",
        "#     print(f'{i:2<}, {c}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O2FyIRV8u25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Word2Vec 학습** (CBOW vs Skip-gram)"
      ],
      "metadata": {
        "id": "Xa8sw19SvCZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3) Word2Vec 학습 (CBOW vs Skip-gram)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def train_w2v(corpus: List[List[str]], sg: int = 1, vector_size: int = 100, window: int = 5, min_count: int = 1, epochs: int = 100) -> Word2Vec:\n",
        "    model = Word2Vec(\n",
        "        sentences=corpus,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=4,\n",
        "        sg=sg,              # 0: CBOW, 1: Skip-gram\n",
        "        negative=10,\n",
        "        seed=42\n",
        "    )\n",
        "    model.train(corpus, total_examples=len(corpus), epochs=epochs)\n",
        "    return model\n",
        "\n",
        "print(\"\\n[학습] Skip-gram(sg=1) 학습 중...\")\n",
        "w2v_sg = train_w2v(corpus, sg=1, vector_size=100, window=5, min_count=1, epochs=200)\n",
        "\n",
        "print(\"[학습] CBOW(sg=0) 학습 중...\")\n",
        "w2v_cbow = train_w2v(corpus, sg=0, vector_size=100, window=5, min_count=1, epochs=200)\n",
        "\n"
      ],
      "metadata": {
        "id": "pRQu4Wfcu76n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. 유사어/유사도/유추(analogy)**"
      ],
      "metadata": {
        "id": "22mDxBILvHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4) 유사어/유사도/유추(analogy) 테스트\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def safe_most_similar(model: Word2Vec, positive: List[str], topn: int = 5):\n",
        "    try:\n",
        "        return model.wv.most_similar(positive=positive, topn=topn)\n",
        "    except KeyError as e:\n",
        "        return [(f\"✅ [어휘없음] {str(e)}\", 0.0)]\n",
        "\n",
        "def safe_similarity(model: Word2Vec, w1: str, w2: str) -> float:\n",
        "    try:\n",
        "        return float(model.wv.similarity(w1, w2))\n",
        "    except KeyError:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def safe_analogy(model: Word2Vec, pos: Iterable[str], neg: Iterable[str], topn: int = 5):\n",
        "    try:\n",
        "        return model.wv.most_similar(positive=list(pos), negative=list(neg), topn=topn)\n",
        "    except KeyError as e:\n",
        "        return [(f\"[어휘없음] {str(e)}\", 0.0)]\n",
        "\n",
        "\n",
        "seed_words = [\"인공지능\", \"머신러닝\", \"딥러닝\", \"투자\", \"건강\", \"자연언어처리\", \"비전\"]\n",
        "\n",
        "print(\"\\n✅ [유사어 예시: Skip-gram]\")\n",
        "for w in seed_words:\n",
        "    if w in w2v_sg.wv:\n",
        "        sims = safe_most_similar(w2v_sg, [w], topn=5)\n",
        "        sorted_sims = sorted(sims, key=lambda x: x[1], reverse=True)\n",
        "        print(f\"  '{w}'와(과) 유사한 단어:\", sorted_sims)\n",
        "\n",
        "print(\"\\n✅ [단어 유사도 예시: Skip-gram]\")\n",
        "pairs = [(\"인공지능\",\"머신러닝\"), (\"머신러닝\",\"딥러닝\"), (\"투자\",\"비즈니스\"), (\"자연언어처리\",\"비전\")]\n",
        "for a,b in pairs:\n",
        "    print(f\"  sim({a}, {b}) = {safe_similarity(w2v_sg, a, b):.4f}\")\n",
        "\n",
        "print(\"\\n✅ [간단 유추(Analogy) 예시: Skip-gram]\")\n",
        "# '머신러닝 - 지도학습 + 비지도학습' 은 말뭉치에 단어가 부족할 수 있으므로 안전하게 체크\n",
        "analogy_tests = [\n",
        "    ([\"인공지능\",\"딥러닝\"], [\"머신러닝\"]),   # 인공지능 + 딥러닝 - 머신러닝 ≈ ?\n",
        "    ([\"투자\",\"기업\"], [\"고객\"]),           # 투자 + 기업 - 고객 ≈ ?\n",
        "]\n",
        "for pos, neg in analogy_tests:\n",
        "    print(f\"  +{pos} -{neg} =>\", safe_analogy(w2v_sg, pos, neg, topn=5))\n"
      ],
      "metadata": {
        "id": "O4VX_SsLvQ2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. 문서 임베딩(평균) 유사도 히트맵 시각화**"
      ],
      "metadata": {
        "id": "fzIva1XXvYxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 5) 문서 임베딩(평균) 유사도 히트맵\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def doc_embedding(model: Word2Vec, tokens: List[str]) -> np.ndarray:\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    if not vecs:\n",
        "        return np.zeros(model.vector_size, dtype=float)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "doc_titles = (\n",
        "    [f\"basic{i+1}\" for i in range(len(documents_basic))] +\n",
        "    [f\"cmp{i+1}\" for i in range(len(documents_compare))] +\n",
        "    [f\"tech{i+1}\" for i in range(len(tech_docs))] +\n",
        "    [f\"biz{i+1}\" for i in range(len(business_docs))] +\n",
        "    [f\"health{i+1}\" for i in range(len(health_docs))] +\n",
        "    [f\"news{i+1}\" for i in range(len(news_articles))]\n",
        ")\n",
        "\n",
        "doc_vecs = np.vstack([doc_embedding(w2v_sg, tokens) for tokens in corpus])\n",
        "sim_mat = cosine_similarity(doc_vecs)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(sim_mat, aspect='auto')\n",
        "plt.title('Document Similarity (Avg Word2Vec, cosine)')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(doc_titles)), doc_titles, rotation=90, fontsize=8)\n",
        "plt.yticks(range(len(doc_titles)), doc_titles, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Wx5m1xNvfOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9FeIRlN_YHM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : Word2Vec Quicktour**"
      ],
      "metadata": {
        "id": "Fn6kTUL0d8XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 간단 테스트\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def simple_word2vec_demo():\n",
        "    \"\"\"\n",
        "    Word2Vec 기본 개념을 보여주는 간단한 예제\n",
        "    \"\"\"\n",
        "    print(\"🧠 Word2Vec 기본 개념 이해하기\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 예제 문서들\n",
        "    sentences = [\n",
        "        \"왕과 여왕은 왕궁에 삽니다\",\n",
        "        \"남자와 여자는 사람입니다\",\n",
        "        \"아버지와 어머니는 부모입니다\",\n",
        "        \"아들과 딸은 자녀입니다\",\n",
        "        \"형과 누나는 형제자매입니다\"\n",
        "    ]\n",
        "\n",
        "    print(\"📄 분석할 문장들:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"  문장{i+1}: {sentence}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 1단계: 어휘 구축\n",
        "    #-------------------------------------\n",
        "    all_words = set()\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # 간단한 전처리\n",
        "        words = sentence.replace('과', '').replace('는', '').replace('에', '').split()\n",
        "        processed_sentences.append(words)\n",
        "        all_words.update(words)\n",
        "\n",
        "    vocab = sorted(list(all_words))\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    print(f\"\\n📚 구축된 어휘 ({len(vocab)}개):\")\n",
        "    print(f\"  {vocab}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 2단계: Skip-gram 학습 데이터 생성\n",
        "    #-------------------------------------\n",
        "    print(f\"\\n🎯 Skip-gram 학습 데이터 생성 (윈도우 크기: 2):\")\n",
        "\n",
        "    training_pairs = []\n",
        "    for sentence_words in processed_sentences:\n",
        "        for i, target_word in enumerate(sentence_words):\n",
        "            # 윈도우 범위 설정\n",
        "            start = max(0, i - 2)\n",
        "            end = min(len(sentence_words), i + 3)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if j != i:  # 타겟 단어 제외\n",
        "                    context_word = sentence_words[j]\n",
        "                    training_pairs.append((target_word, context_word))\n",
        "\n",
        "    print(f\"  총 {len(training_pairs)}개의 학습 쌍 생성\")\n",
        "    print(\"  예시 학습 쌍들:\")\n",
        "    for i, (target, context) in enumerate(training_pairs[:8]):\n",
        "        print(f\"    {target} → {context}\")\n",
        "    if len(training_pairs) > 8:\n",
        "        print(f\"    ... 외 {len(training_pairs)-8}개\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 3단계: 간단한 벡터 초기화 및 학습 시뮬레이션\n",
        "    #-------------------------------------\n",
        "    print(f\"\\n⚡ 벡터 학습 시뮬레이션:\")\n",
        "\n",
        "    vector_size = 4  # 간단한 시각화를 위해 4차원\n",
        "    word_vectors = {}\n",
        "\n",
        "    # 랜덤 초기화\n",
        "    np.random.seed(42)  # 재현 가능한 결과\n",
        "    for word in vocab:\n",
        "        word_vectors[word] = np.random.normal(0, 0.1, vector_size)\n",
        "\n",
        "    print(\"  초기 벡터 (일부):\")\n",
        "    for word in vocab[:5]:\n",
        "        print(f\"    {word}: [{', '.join([f'{x:.3f}' for x in word_vectors[word]])}]\")\n",
        "\n",
        "    # 간단한 학습 시뮬레이션 (실제 역전파는 복잡하므로 개념적 설명)\n",
        "    print(f\"\\n📈 학습 과정 시뮬레이션:\")\n",
        "    print(\"  (실제로는 경사하강법과 역전파를 통해 벡터가 업데이트됩니다)\")\n",
        "\n",
        "    # 유사한 단어들을 가까이 배치하는 시뮬레이션\n",
        "    similar_pairs = [\n",
        "        (\"왕\", \"여왕\"), (\"남자\", \"여자\"), (\"아버지\", \"어머니\"), (\"아들\", \"딸\"), (\"형\", \"누나\")\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in similar_pairs:\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            # 유사한 단어들의 벡터를 점진적으로 가깝게 만들기\n",
        "            avg_vector = (word_vectors[word1] + word_vectors[word2]) / 2\n",
        "            word_vectors[word1] = 0.7 * word_vectors[word1] + 0.3 * avg_vector\n",
        "            word_vectors[word2] = 0.7 * word_vectors[word2] + 0.3 * avg_vector\n",
        "\n",
        "    print(\"  학습 후 벡터 (일부):\")\n",
        "    for word in vocab[:5]:\n",
        "        print(f\"    {word}: [{', '.join([f'{x:.3f}' for x in word_vectors[word]])}]\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 4단계: 유사도 계산\n",
        "    #-------------------------------------\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm1 = np.linalg.norm(vec1)\n",
        "        norm2 = np.linalg.norm(vec2)\n",
        "        return dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
        "\n",
        "    print(f\"\\n🔗 단어 간 유사도 계산:\")\n",
        "\n",
        "    test_pairs = [\n",
        "        (\"왕\", \"여왕\"),\n",
        "        (\"남자\", \"여자\"),\n",
        "        (\"아버지\", \"어머니\"),\n",
        "        (\"왕\", \"남자\"),\n",
        "        (\"왕\", \"딸\")\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in test_pairs:\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            similarity = cosine_similarity(word_vectors[word1], word_vectors[word2])\n",
        "            print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 5단계: 시각화 (2차원으로 축소)\n",
        "    #-------------------------------------\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # 벡터들을 배열로 변환\n",
        "    vectors = np.array([word_vectors[word] for word in vocab])\n",
        "\n",
        "    # PCA로 2차원 축소\n",
        "    pca = PCA(n_components=2)\n",
        "    vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=100, alpha=0.7, c='skyblue')\n",
        "\n",
        "    # 단어 라벨 추가\n",
        "    for i, word in enumerate(vocab):\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=12)\n",
        "\n",
        "    plt.title('Word2Vec 벡터 시각화 (PCA 2차원 축소)', fontsize=16)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n💡 Word2Vec 핵심 원리:\")\n",
        "    print(\"  ✅ 비슷한 맥락에서 나타나는 단어들은 비슷한 벡터를 가집니다\")\n",
        "    print(\"  ✅ 벡터 간 거리가 가까우면 의미가 유사합니다\")\n",
        "    print(\"  ✅ 벡터 연산으로 단어 관계를 표현할 수 있습니다\")\n",
        "\n",
        "    return word_vectors\n",
        "\n",
        "\n",
        "def gensim_word2vec_example():\n",
        "    \"\"\"\n",
        "    Gensim을 사용한 실제 Word2Vec 예제\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"🔬 Gensim Word2Vec 실습\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "\n",
        "        # 한국어 문장들 (토큰화된 형태)\n",
        "        sentences = [\n",
        "            ['사과', '바나나', '과일', '맛있다'],\n",
        "            ['딸기', '포도', '과일', '달다'],\n",
        "            ['과일', '건강', '좋다', '비타민'],\n",
        "            ['개', '고양이', '동물', '귀엽다'],\n",
        "            ['토끼', '햄스터', '동물', '작다'],\n",
        "            ['동물', '생명', '소중하다', '보호'],\n",
        "            ['자동차', '기차', '교통수단', '빠르다'],\n",
        "            ['버스', '지하철', '교통수단', '편리하다'],\n",
        "            ['교통수단', '이동', '필요하다', '중요'],\n",
        "            ['컴퓨터', '스마트폰', '전자기기', '유용하다'],\n",
        "            ['태블릿', '노트북', '전자기기', '편리하다'],\n",
        "            ['전자기기', '기술', '발전', '놀랍다']\n",
        "        ]\n",
        "\n",
        "        print(\"📚 학습 데이터:\")\n",
        "        for i, sentence in enumerate(sentences[:5]):\n",
        "            print(f\"  문장{i+1}: {' '.join(sentence)}\")\n",
        "        print(f\"  ... 총 {len(sentences)}개 문장\")\n",
        "\n",
        "        # Word2Vec 모델 학습\n",
        "        model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=50,      # 벡터 차원\n",
        "            window=3,           # 윈도우 크기\n",
        "            min_count=1,        # 최소 빈도\n",
        "            workers=1,          # 병렬 처리 수\n",
        "            sg=1,              # Skip-gram (0이면 CBOW)\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Word2Vec 모델 학습 완료!\")\n",
        "        print(f\"   어휘 크기: {len(model.wv.key_to_index)}\")\n",
        "        print(f\"   벡터 차원: {model.wv.vector_size}\")\n",
        "\n",
        "        # 1. 유사한 단어 찾기\n",
        "        print(f\"\\n🔍 유사한 단어 찾기:\")\n",
        "        test_words = ['과일', '동물', '교통수단', '전자기기']\n",
        "\n",
        "        for word in test_words:\n",
        "            if word in model.wv:\n",
        "                try:\n",
        "                    similar_words = model.wv.most_similar(word, topn=3)\n",
        "                    print(f\"  '{word}' 유사 단어: {similar_words}\")\n",
        "                except:\n",
        "                    print(f\"  '{word}': 유사 단어 계산 실패\")\n",
        "\n",
        "        # 2. 단어 간 유사도\n",
        "        print(f\"\\n🎯 단어 간 유사도:\")\n",
        "        word_pairs = [\n",
        "            ('사과', '바나나'),      # 같은 카테고리\n",
        "            ('개', '고양이'),        # 같은 카테고리\n",
        "            ('자동차', '컴퓨터'),    # 다른 카테고리\n",
        "            ('과일', '동물')         # 다른 카테고리\n",
        "        ]\n",
        "\n",
        "        for word1, word2 in word_pairs:\n",
        "            if word1 in model.wv and word2 in model.wv:\n",
        "                similarity = model.wv.similarity(word1, word2)\n",
        "                print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "        # 3. 단어 벡터 시각화\n",
        "        print(f\"\\n📊 벡터 시각화:\")\n",
        "\n",
        "        # 주요 단어들의 벡터 추출\n",
        "        words_to_plot = ['과일', '사과', '바나나', '동물', '개', '고양이',\n",
        "                        '교통수단', '자동차', '전자기기', '컴퓨터']\n",
        "\n",
        "        vectors = []\n",
        "        labels = []\n",
        "\n",
        "        for word in words_to_plot:\n",
        "            if word in model.wv:\n",
        "                vectors.append(model.wv[word])\n",
        "                labels.append(word)\n",
        "\n",
        "        # PCA로 2차원 축소\n",
        "        from sklearn.decomposition import PCA\n",
        "\n",
        "        vectors = np.array(vectors)\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        # 카테고리별 색상\n",
        "        categories = {\n",
        "            '과일': ['과일', '사과', '바나나'],\n",
        "            '동물': ['동물', '개', '고양이'],\n",
        "            '교통수단': ['교통수단', '자동차'],\n",
        "            '전자기기': ['전자기기', '컴퓨터']\n",
        "        }\n",
        "\n",
        "        colors = {'과일': 'red', '동물': 'blue', '교통수단': 'green', '전자기기': 'orange'}\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        for category, words_in_cat in categories.items():\n",
        "            indices = [i for i, word in enumerate(labels) if word in words_in_cat]\n",
        "            if indices:\n",
        "                plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                           c=colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "        # 단어 라벨 추가\n",
        "        for i, word in enumerate(labels):\n",
        "            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "        plt.title('Gensim Word2Vec 벡터 시각화', fontsize=16)\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"✅ 시각화 완료! 같은 카테고리의 단어들이 가까이 모여있는지 확인하세요.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ Gensim이 설치되지 않았습니다.\")\n",
        "        print(\"설치: pip install gensim\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def word2vec_applications_demo():\n",
        "    \"\"\"\n",
        "    Word2Vec 실전 응용 예제\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"🚀 Word2Vec 실전 응용\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "        from sklearn.cluster import KMeans\n",
        "\n",
        "        # 더 큰 데이터셋\n",
        "        extended_sentences = [\n",
        "            # 음식 관련\n",
        "            ['한식', '김치', '된장찌개', '맛있다', '전통', '음식'],\n",
        "            ['중식', '짜장면', '탕수육', '맛있다', '중국', '음식'],\n",
        "            ['일식', '초밥', '라멘', '신선하다', '일본', '음식'],\n",
        "            ['양식', '파스타', '피자', '맛있다', '서양', '음식'],\n",
        "            ['음식', '요리', '맛', '건강', '영양', '중요'],\n",
        "\n",
        "            # 스포츠 관련\n",
        "            ['축구', '월드컵', '선수', '경기', '골', '승부'],\n",
        "            ['야구', '홈런', '투수', '타자', '경기장', '응원'],\n",
        "            ['농구', '덩크슛', '선수', '코트', '팀워크', '전략'],\n",
        "            ['테니스', '라켓', '선수', '코트', '서브', '경기'],\n",
        "            ['스포츠', '운동', '건강', '체력', '팀워크', '중요'],\n",
        "\n",
        "            # 기술 관련\n",
        "            ['컴퓨터', '프로그래밍', '소프트웨어', '개발', '기술', '혁신'],\n",
        "            ['스마트폰', '앱', '모바일', '편리', '통신', '기술'],\n",
        "            ['인터넷', '웹사이트', '정보', '검색', '연결', '네트워크'],\n",
        "            ['인공지능', '머신러닝', '데이터', '학습', '미래', '기술'],\n",
        "            ['기술', '발전', '혁신', '미래', '사회', '변화']\n",
        "        ]\n",
        "\n",
        "        print(f\"📚 확장 데이터셋으로 학습 (총 {len(extended_sentences)}개 문장)\")\n",
        "\n",
        "        # 모델 학습\n",
        "        app_model = Word2Vec(\n",
        "            sentences=extended_sentences,\n",
        "            vector_size=100,\n",
        "            window=4,\n",
        "            min_count=1,\n",
        "            workers=1,\n",
        "            sg=1,\n",
        "            epochs=200\n",
        "        )\n",
        "\n",
        "        print(\"✅ 확장 모델 학습 완료!\")\n",
        "\n",
        "        # 1. 카테고리 분류\n",
        "        print(f\"\\n📂 자동 카테고리 분류:\")\n",
        "\n",
        "        # 주요 단어들의 벡터 추출\n",
        "        food_words = ['한식', '중식', '일식', '양식', '김치', '짜장면', '초밥', '파스타']\n",
        "        sports_words = ['축구', '야구', '농구', '테니스', '선수', '경기', '골', '홈런']\n",
        "        tech_words = ['컴퓨터', '스마트폰', '인터넷', '인공지능', '프로그래밍', '앱']\n",
        "\n",
        "        all_words = food_words + sports_words + tech_words\n",
        "        vectors = []\n",
        "        labels = []\n",
        "        categories = []\n",
        "\n",
        "        for word in all_words:\n",
        "            if word in app_model.wv:\n",
        "                vectors.append(app_model.wv[word])\n",
        "                labels.append(word)\n",
        "                if word in food_words:\n",
        "                    categories.append('음식')\n",
        "                elif word in sports_words:\n",
        "                    categories.append('스포츠')\n",
        "                else:\n",
        "                    categories.append('기술')\n",
        "\n",
        "        # K-means 클러스터링\n",
        "        vectors = np.array(vectors)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "        clusters = kmeans.fit_predict(vectors)\n",
        "\n",
        "        print(\"  클러스터링 결과:\")\n",
        "        cluster_words = defaultdict(list)\n",
        "        for word, cluster, true_cat in zip(labels, clusters, categories):\n",
        "            cluster_words[cluster].append(f\"{word}({true_cat})\")\n",
        "\n",
        "        for cluster_id, words in cluster_words.items():\n",
        "            print(f\"    클러스터 {cluster_id}: {words}\")\n",
        "\n",
        "        # 2. 문서 유사도 계산\n",
        "        print(f\"\\n📄 문서 유사도 계산:\")\n",
        "\n",
        "        test_documents = [\n",
        "            \"한식 김치 된장찌개 맛있다\",\n",
        "            \"축구 선수 월드컵 경기\",\n",
        "            \"컴퓨터 프로그래밍 소프트웨어\",\n",
        "            \"중식 짜장면 맛있다\"\n",
        "        ]\n",
        "\n",
        "        def document_vector(doc, model):\n",
        "            \"\"\"문서를 벡터로 변환 (단어 벡터들의 평균)\"\"\"\n",
        "            words = doc.split()\n",
        "            vectors = []\n",
        "            for word in words:\n",
        "                if word in model.wv:\n",
        "                    vectors.append(model.wv[word])\n",
        "\n",
        "            if vectors:\n",
        "                return np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                return np.zeros(model.wv.vector_size)\n",
        "\n",
        "        doc_vectors = [document_vector(doc, app_model) for doc in test_documents]\n",
        "\n",
        "        # 문서 간 유사도 계산\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        doc_similarity = cosine_similarity(doc_vectors)\n",
        "\n",
        "        print(\"  문서 간 유사도 매트릭스:\")\n",
        "        for i, doc in enumerate(test_documents):\n",
        "            print(f\"  문서{i+1}: {doc}\")\n",
        "\n",
        "        print(f\"\\n  유사도 매트릭스:\")\n",
        "        for i in range(len(test_documents)):\n",
        "            for j in range(len(test_documents)):\n",
        "                print(f\"  {doc_similarity[i][j]:.3f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        # 3. 시각화\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # 클러스터링 결과 시각화\n",
        "        plt.subplot(1, 3, 1)\n",
        "\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        colors = ['red', 'blue', 'green']\n",
        "        for i, cluster in enumerate(set(clusters)):\n",
        "            indices = [j for j, c in enumerate(clusters) if c == cluster]\n",
        "            plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                       c=colors[i], label=f'클러스터 {cluster}', s=100, alpha=0.7)\n",
        "\n",
        "        plt.title('Word2Vec 클러스터링', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 카테고리별 색상\n",
        "        plt.subplot(1, 3, 2)\n",
        "\n",
        "        category_colors = {'음식': 'red', '스포츠': 'blue', '기술': 'green'}\n",
        "        for category in category_colors.keys():\n",
        "            indices = [i for i, cat in enumerate(categories) if cat == category]\n",
        "            if indices:\n",
        "                plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                           c=category_colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "        plt.title('실제 카테고리', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 문서 유사도 히트맵\n",
        "        plt.subplot(1, 3, 3)\n",
        "        import seaborn as sns\n",
        "\n",
        "        sns.heatmap(doc_similarity, annot=True, fmt='.2f', cmap='Blues',\n",
        "                    xticklabels=[f'문서{i+1}' for i in range(len(test_documents))],\n",
        "                    yticklabels=[f'문서{i+1}' for i in range(len(test_documents))])\n",
        "        plt.title('문서 유사도', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return app_model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ 필요한 라이브러리가 설치되지 않았습니다.\")\n",
        "        print(\"설치: pip install gensim scikit-learn\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# 전체 실행 함수\n",
        "def run_simple_word2vec_tests():\n",
        "    \"\"\"\n",
        "    간단한 Word2Vec 테스트들 실행\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"🧠 Word2Vec 마스터하기!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. 기본 개념 이해\n",
        "        print(\"1️⃣ 기본 개념 시연...\")\n",
        "        word_vectors = simple_word2vec_demo()\n",
        "\n",
        "        # 2. Gensim 실습\n",
        "        print(\"\\n2️⃣ Gensim Word2Vec 실습...\")\n",
        "        gensim_model = gensim_word2vec_example()\n",
        "\n",
        "        # 3. 실전 응용\n",
        "        print(\"\\n3️⃣ 실전 응용 예제...\")\n",
        "        app_model = word2vec_applications_demo()\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"🎉 Word2Vec 기본기 학습 완료!\")\n",
        "        print(\"💡 핵심 포인트:\")\n",
        "        print(\"   ✅ Skip-gram: 중심단어 → 주변단어 예측\")\n",
        "        print(\"   ✅ CBOW: 주변단어 → 중심단어 예측\")\n",
        "        print(\"   ✅ 벡터 공간: 유사한 단어는 가까이 위치\")\n",
        "        print(\"   ✅ 응용: 분류, 클러스터링, 추천 시스템\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 오류 발생: {e}\")\n",
        "        print(\"⚠️ 필수 라이브러리 설치 필요:\")\n",
        "        print(\"   pip install gensim scikit-learn matplotlib seaborn\")\n",
        "\n",
        "\n",
        "# 실행\n",
        "if __name__ == \"__main__\":\n",
        "    run_simple_word2vec_tests()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H6rx2baoHlXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 6) t-SNE로 단어 벡터 2D 시각화\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# 빈도가 2회 이상인 단어만 시각화 (잡음 제거)\n",
        "freq = Counter([w for sent in corpus for w in sent])\n",
        "vocab_tsne = [w for w,c in freq.items() if c >= 2 and w in w2v_sg.wv]\n",
        "\n",
        "if len(vocab_tsne) >= 10:\n",
        "    X = np.vstack([w2v_sg.wv[w] for w in vocab_tsne])\n",
        "    tsne = TSNE(n_components=2, perplexity=min(30, max(5, len(vocab_tsne)//3)), random_state=42, n_iter=1000)\n",
        "    X2 = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(9,7))\n",
        "    # plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "    plt.scatter(X2[:,0], X2[:,1])\n",
        "    for i, w in enumerate(vocab_tsne):\n",
        "        plt.annotate(w, (X2[i,0], X2[i,1]), fontsize=9, alpha=0.9)\n",
        "    plt.title('t-SNE visualization of Word2Vec (Skip-gram)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"단어 수가 적어 t-SNE 시각화를 생략합니다. (빈도≥2 단어가 10개 미만)\")\n",
        "\n",
        "# 다음을 시도해보세요\n",
        "# - sg=0(CBOW) 모델로 동일한 분석 비교\n",
        "# - window, vector_size, epochs 변경해 성능 비교\n",
        "# - 도메인별(기술/비즈니스/건강) 말뭉치로 따로 학습해 군집 차이 관찰"
      ],
      "metadata": {
        "id": "iqn71bBeXOzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한글 폰트 설정 강화\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "def ensure_korean_font():\n",
        "    \"\"\"한글 폰트 설정 확인 및 재설정\"\"\"\n",
        "    try:\n",
        "        plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "    except:\n",
        "        try:\n",
        "            plt.rcParams['font.family'] = 'NanumGothic'\n",
        "        except:\n",
        "            plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "    # 사용 가능한 한글 폰트 찾기\n",
        "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "    korean_fonts = [f for f in available_fonts if any(keyword in f for keyword in\n",
        "                   ['Nanum', 'Malgun', 'Dotum', 'Gulim', 'Batang', 'Gungsuh'])]\n",
        "\n",
        "    if korean_fonts:\n",
        "        plt.rcParams['font.family'] = korean_fonts[0]\n",
        "\n",
        "# 초기 폰트 설정\n",
        "ensure_korean_font()\n",
        "\n",
        "class Word2VecImplementation:\n",
        "    def __init__(self, vector_size=100, window_size=3, min_count=1,\n",
        "                 epochs=100, learning_rate=0.025, model_type='skipgram'):\n",
        "        \"\"\"\n",
        "        Word2Vec 구현 클래스\n",
        "\n",
        "        Args:\n",
        "            vector_size: 벡터 차원 수\n",
        "            window_size: 컨텍스트 윈도우 크기\n",
        "            min_count: 최소 단어 빈도\n",
        "            epochs: 학습 에포크 수\n",
        "            learning_rate: 학습률\n",
        "            model_type: 'skipgram' 또는 'cbow'\n",
        "        \"\"\"\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.min_count = min_count\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.vocab = {}\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.word_vectors = None\n",
        "        self.context_vectors = None\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"텍스트 전처리\"\"\"\n",
        "        # 한글과 영문, 숫자만 유지\n",
        "        text = re.sub(r'[^\\w\\s가-힣]', '', text.lower())\n",
        "        return [word for word in text.split() if word.strip()]\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        \"\"\"어휘 구축\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # 모든 문장에서 단어 빈도 계산\n",
        "        for sentence in sentences:\n",
        "            words = self.preprocess_text(sentence)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # 최소 빈도 필터링\n",
        "        filtered_words = {word: count for word, count in word_counts.items()\n",
        "                         if count >= self.min_count}\n",
        "\n",
        "        # 어휘 매핑 생성\n",
        "        self.vocab = filtered_words\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(filtered_words.keys())}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(f\"구축된 어휘 크기: {self.vocab_size}\")\n",
        "        return filtered_words\n",
        "\n",
        "    def generate_training_data(self, sentences):\n",
        "        \"\"\"학습 데이터 생성\"\"\"\n",
        "        training_data = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = self.preprocess_text(sentence)\n",
        "            words = [w for w in words if w in self.word_to_idx]\n",
        "\n",
        "            for i, target_word in enumerate(words):\n",
        "                # 컨텍스트 윈도우 내의 단어들\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(words), i + self.window_size + 1)\n",
        "\n",
        "                context_words = []\n",
        "                for j in range(start, end):\n",
        "                    if j != i:  # 타겟 단어 제외\n",
        "                        context_words.append(words[j])\n",
        "\n",
        "                if len(context_words) >= 1:\n",
        "                    if self.model_type == 'skipgram':\n",
        "                        # Skip-gram: 중심 단어 → 주변 단어들\n",
        "                        for context_word in context_words:\n",
        "                            training_data.append((target_word, context_word))\n",
        "                    else:  # CBOW\n",
        "                        # CBOW: 주변 단어들 → 중심 단어\n",
        "                        training_data.append((context_words, target_word))\n",
        "\n",
        "        return training_data\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"시그모이드 함수\"\"\"\n",
        "        x = np.clip(x, -500, 500)  # 오버플로우 방지\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"모델 학습\"\"\"\n",
        "        print(f\"{self.model_type.upper()} 모델 학습 시작...\")\n",
        "\n",
        "        # 어휘 구축\n",
        "        self.build_vocabulary(sentences)\n",
        "\n",
        "        if self.vocab_size < 2:\n",
        "            print(\"어휘가 너무 적습니다.\")\n",
        "            return\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.word_vectors = np.random.uniform(-0.5, 0.5,\n",
        "                                            (self.vocab_size, self.vector_size))\n",
        "        self.context_vectors = np.random.uniform(-0.5, 0.5,\n",
        "                                               (self.vocab_size, self.vector_size))\n",
        "\n",
        "        # 학습 데이터 생성\n",
        "        training_data = self.generate_training_data(sentences)\n",
        "        print(f\"학습 샘플 수: {len(training_data)}\")\n",
        "\n",
        "        if not training_data:\n",
        "            print(\"학습 데이터가 없습니다.\")\n",
        "            return\n",
        "\n",
        "        # 학습 루프\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            for sample in training_data:\n",
        "                if self.model_type == 'skipgram':\n",
        "                    loss = self._train_skipgram(sample)\n",
        "                else:\n",
        "                    loss = self._train_cbow(sample)\n",
        "                total_loss += loss\n",
        "\n",
        "            if epoch % 20 == 0:\n",
        "                avg_loss = total_loss / len(training_data)\n",
        "                print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        print(\"학습 완료!\")\n",
        "\n",
        "    def _train_skipgram(self, sample):\n",
        "        \"\"\"Skip-gram 학습\"\"\"\n",
        "        target_word, context_word = sample\n",
        "\n",
        "        if target_word not in self.word_to_idx or context_word not in self.word_to_idx:\n",
        "            return 0\n",
        "\n",
        "        target_idx = self.word_to_idx[target_word]\n",
        "        context_idx = self.word_to_idx[context_word]\n",
        "\n",
        "        # Forward pass\n",
        "        target_vector = self.word_vectors[target_idx]\n",
        "        context_vector = self.context_vectors[context_idx]\n",
        "\n",
        "        # 내적 계산\n",
        "        score = np.dot(target_vector, context_vector)\n",
        "        pred = self.sigmoid(score)\n",
        "\n",
        "        # Loss 계산 (이진 분류)\n",
        "        loss = -np.log(pred + 1e-10)\n",
        "\n",
        "        # Backward pass\n",
        "        error = pred - 1  # 정답이 1이므로\n",
        "\n",
        "        # 그래디언트 계산 및 업데이트\n",
        "        word_grad = error * context_vector\n",
        "        context_grad = error * target_vector\n",
        "\n",
        "        self.word_vectors[target_idx] -= self.learning_rate * word_grad\n",
        "        self.context_vectors[context_idx] -= self.learning_rate * context_grad\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _train_cbow(self, sample):\n",
        "        \"\"\"CBOW 학습\"\"\"\n",
        "        context_words, target_word = sample\n",
        "\n",
        "        if target_word not in self.word_to_idx:\n",
        "            return 0\n",
        "\n",
        "        # 컨텍스트 벡터들의 평균\n",
        "        context_indices = [self.word_to_idx[w] for w in context_words\n",
        "                          if w in self.word_to_idx]\n",
        "\n",
        "        if not context_indices:\n",
        "            return 0\n",
        "\n",
        "        context_vector = np.mean([self.context_vectors[idx] for idx in context_indices], axis=0)\n",
        "        target_idx = self.word_to_idx[target_word]\n",
        "\n",
        "        # Forward pass\n",
        "        scores = np.dot(self.word_vectors, context_vector)\n",
        "        scores = scores - np.max(scores)  # 수치 안정성\n",
        "\n",
        "        # Softmax\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        # Loss 계산\n",
        "        loss = -np.log(probs[target_idx] + 1e-10)\n",
        "\n",
        "        # Backward pass\n",
        "        grad_output = probs.copy()\n",
        "        grad_output[target_idx] -= 1.0\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        word_grad = np.outer(grad_output, context_vector)\n",
        "        self.word_vectors -= self.learning_rate * word_grad\n",
        "\n",
        "        context_grad = np.dot(grad_output, self.word_vectors)\n",
        "        for idx in context_indices:\n",
        "            self.context_vectors[idx] -= self.learning_rate * context_grad / len(context_indices)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        \"\"\"단어 벡터 반환\"\"\"\n",
        "        if word in self.word_to_idx:\n",
        "            idx = self.word_to_idx[word]\n",
        "            return self.word_vectors[idx]\n",
        "        return None\n",
        "\n",
        "    def similarity(self, word1, word2):\n",
        "        \"\"\"두 단어 간 코사인 유사도\"\"\"\n",
        "        vec1 = self.get_vector(word1)\n",
        "        vec2 = self.get_vector(word2)\n",
        "\n",
        "        if vec1 is None or vec2 is None:\n",
        "            return 0.0\n",
        "\n",
        "        # 코사인 유사도\n",
        "        norm1 = np.linalg.norm(vec1)\n",
        "        norm2 = np.linalg.norm(vec2)\n",
        "\n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "    def most_similar(self, word, top_k=5):\n",
        "        \"\"\"가장 유사한 단어들 찾기\"\"\"\n",
        "        if word not in self.word_to_idx:\n",
        "            return []\n",
        "\n",
        "        target_vector = self.get_vector(word)\n",
        "        similarities = []\n",
        "\n",
        "        for other_word in self.word_to_idx.keys():\n",
        "            if other_word != word:\n",
        "                sim = self.similarity(word, other_word)\n",
        "                similarities.append((other_word, sim))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:top_k]\n",
        "\n",
        "    def analogy(self, word_a, word_b, word_c, top_k=1):\n",
        "        \"\"\"단어 유추: A is to B as C is to ?\"\"\"\n",
        "        try:\n",
        "            vec_a = self.get_vector(word_a)\n",
        "            vec_b = self.get_vector(word_b)\n",
        "            vec_c = self.get_vector(word_c)\n",
        "\n",
        "            if any(v is None for v in [vec_a, vec_b, vec_c]):\n",
        "                return []\n",
        "\n",
        "            # 벡터 연산: B - A + C\n",
        "            result_vector = vec_b - vec_a + vec_c\n",
        "\n",
        "            # 가장 유사한 단어 찾기\n",
        "            similarities = []\n",
        "            for word in self.word_to_idx.keys():\n",
        "                if word not in [word_a, word_b, word_c]:\n",
        "                    word_vec = self.get_vector(word)\n",
        "                    if word_vec is not None:\n",
        "                        sim = cosine_similarity([result_vector], [word_vec])[0][0]\n",
        "                        similarities.append((word, sim))\n",
        "\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_k]\n",
        "\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "\n",
        "def demonstrate_basic_word2vec():\n",
        "    \"\"\"기본 Word2Vec 구현 시연\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. 기본 Word2Vec 구현 시연\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 예제 문서들 (더 많은 데이터로 확장)\n",
        "    sentences = [\n",
        "        \"왕은 왕궁에서 생활합니다\",\n",
        "        \"여왕은 왕궁에서 생활합니다\",\n",
        "        \"남자는 남성이라고도 부릅니다\",\n",
        "        \"여자는 여성이라고도 부릅니다\",\n",
        "        \"왕과 여왕은 왕실 가족입니다\",\n",
        "        \"남자와 여자는 사람입니다\",\n",
        "        \"아버지는 가족의 가장입니다\",\n",
        "        \"어머니는 가족을 돌봅니다\",\n",
        "        \"아들은 아버지의 자녀입니다\",\n",
        "        \"딸은 어머니의 자녀입니다\",\n",
        "        \"컴퓨터는 전자 기기입니다\",\n",
        "        \"노트북은 휴대용 컴퓨터입니다\",\n",
        "        \"스마트폰은 휴대용 전화기입니다\",\n",
        "        \"태블릿은 휴대용 기기입니다\",\n",
        "        \"인공지능은 컴퓨터 기술입니다\",\n",
        "        \"머신러닝은 인공지능 분야입니다\",\n",
        "        \"딥러닝은 머신러닝 방법입니다\",\n",
        "        \"데이터는 정보의 집합입니다\"\n",
        "    ]\n",
        "\n",
        "    # Skip-gram 모델 학습\n",
        "    print(\"\\n🧠 Skip-gram 모델 학습:\")\n",
        "    skipgram_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=2,\n",
        "        epochs=200,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    skipgram_model.train(sentences)\n",
        "\n",
        "    # CBOW 모델 학습\n",
        "    print(f\"\\n🧠 CBOW 모델 학습:\")\n",
        "    cbow_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=2,\n",
        "        epochs=200,\n",
        "        model_type='cbow',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    cbow_model.train(sentences)\n",
        "\n",
        "    # 결과 비교\n",
        "    test_words = [\"왕\", \"여왕\", \"남자\", \"여자\", \"컴퓨터\"]\n",
        "\n",
        "    print(f\"\\n📊 모델 결과 비교:\")\n",
        "    for word in test_words:\n",
        "        if word in skipgram_model.word_to_idx:\n",
        "            print(f\"\\n🔍 '{word}'와 유사한 단어들:\")\n",
        "\n",
        "            # Skip-gram 결과\n",
        "            skipgram_similar = skipgram_model.most_similar(word, top_k=3)\n",
        "            print(f\"  Skip-gram: {skipgram_similar}\")\n",
        "\n",
        "            # CBOW 결과\n",
        "            cbow_similar = cbow_model.most_similar(word, top_k=3)\n",
        "            print(f\"  CBOW: {cbow_similar}\")\n",
        "\n",
        "    return skipgram_model, cbow_model\n",
        "\n",
        "\n",
        "def demonstrate_gensim_word2vec():\n",
        "    \"\"\"Gensim Word2Vec과 비교\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"2. Gensim Word2Vec과 성능 비교\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "\n",
        "        # 한국어 문장들\n",
        "        korean_sentences = [\n",
        "            ['인공지능', '기술', '발전', '빠르다'],\n",
        "            ['머신러닝', '알고리즘', '학습', '데이터'],\n",
        "            ['딥러닝', '신경망', '복잡한', '문제', '해결'],\n",
        "            ['자연언어', '처리', 'NLP', '중요하다'],\n",
        "            ['컴퓨터', '비전', '이미지', '인식'],\n",
        "            ['빅데이터', '분석', '패턴', '발견'],\n",
        "            ['클라우드', '컴퓨팅', '서비스', '제공'],\n",
        "            ['로봇', '자동화', '미래', '산업'],\n",
        "            ['블록체인', '암호화', '보안', '기술'],\n",
        "            ['사물인터넷', 'IoT', '연결', '네트워크']\n",
        "        ]\n",
        "\n",
        "        # Gensim Word2Vec 모델\n",
        "        gensim_model = Word2Vec(\n",
        "            sentences=korean_sentences,\n",
        "            vector_size=100,\n",
        "            window=3,\n",
        "            min_count=1,\n",
        "            workers=1,\n",
        "            sg=1,  # Skip-gram\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        print(\"✅ Gensim Word2Vec 모델 학습 완료\")\n",
        "        print(f\"어휘 크기: {len(gensim_model.wv.key_to_index)}\")\n",
        "\n",
        "        # 단어 유사도 테스트\n",
        "        test_words = ['인공지능', '머신러닝', '컴퓨터', '데이터']\n",
        "\n",
        "        print(f\"\\n📊 Gensim 모델 결과:\")\n",
        "        for word in test_words:\n",
        "            if word in gensim_model.wv:\n",
        "                try:\n",
        "                    similar_words = gensim_model.wv.most_similar(word, topn=3)\n",
        "                    print(f\"  '{word}' 유사 단어: {similar_words}\")\n",
        "                except:\n",
        "                    print(f\"  '{word}': 유사 단어를 찾을 수 없습니다.\")\n",
        "\n",
        "        # 단어 간 유사도\n",
        "        print(f\"\\n🔗 단어 간 유사도:\")\n",
        "        word_pairs = [\n",
        "            ('인공지능', '머신러닝'),\n",
        "            ('컴퓨터', '데이터'),\n",
        "            ('신경망', '딥러닝')\n",
        "        ]\n",
        "\n",
        "        for word1, word2 in word_pairs:\n",
        "            if word1 in gensim_model.wv and word2 in gensim_model.wv:\n",
        "                similarity = gensim_model.wv.similarity(word1, word2)\n",
        "                print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "        return gensim_model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ Gensim이 설치되지 않았습니다.\")\n",
        "        print(\"설치: pip install gensim\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def demonstrate_word2vec_visualization():\n",
        "    \"\"\"Word2Vec 벡터 시각화\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"3. Word2Vec 벡터 시각화\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 더 많은 데이터로 모델 학습\n",
        "    extended_sentences = [\n",
        "        \"사과는 빨간 과일입니다\", \"바나나는 노란 과일입니다\", \"오렌지는 오렌지색 과일입니다\",\n",
        "        \"포도는 보라색 과일입니다\", \"과일은 달고 맛있습니다\", \"과일은 건강에 좋습니다\",\n",
        "        \"개는 충실한 동물입니다\", \"고양이는 귀여운 동물입니다\", \"토끼는 빠른 동물입니다\",\n",
        "        \"사자는 강한 동물입니다\", \"동물은 생명체입니다\", \"동물은 자연에 삽니다\",\n",
        "        \"자동차는 빠른 교통수단입니다\", \"기차는 긴 교통수단입니다\", \"비행기는 높은 교통수단입니다\",\n",
        "        \"배는 물위의 교통수단입니다\", \"교통수단은 이동에 사용됩니다\", \"교통수단은 편리합니다\",\n",
        "        \"책은 지식의 보고입니다\", \"신문은 정보를 제공합니다\", \"잡지는 재미있는 읽을거리입니다\",\n",
        "        \"컴퓨터는 유용한 도구입니다\", \"스마트폰은 편리한 도구입니다\", \"도구는 일을 돕습니다\"\n",
        "    ]\n",
        "\n",
        "    # 모델 학습\n",
        "    viz_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=3,\n",
        "        epochs=300,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    viz_model.train(extended_sentences)\n",
        "\n",
        "    # 벡터 추출\n",
        "    words = list(viz_model.word_to_idx.keys())\n",
        "    vectors = []\n",
        "    word_labels = []\n",
        "\n",
        "    for word in words:\n",
        "        vector = viz_model.get_vector(word)\n",
        "        if vector is not None:\n",
        "            vectors.append(vector)\n",
        "            word_labels.append(word)\n",
        "\n",
        "    if len(vectors) < 2:\n",
        "        print(\"❌ 시각화할 벡터가 충분하지 않습니다.\")\n",
        "        return\n",
        "\n",
        "    vectors = np.array(vectors)\n",
        "\n",
        "    # PCA로 2차원 축소\n",
        "    pca = PCA(n_components=2)\n",
        "    vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 한글 폰트 재설정\n",
        "    ensure_korean_font()\n",
        "\n",
        "    # 1. PCA 시각화\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1],\n",
        "                         c=np.arange(len(word_labels)),\n",
        "                         cmap='tab20', s=100, alpha=0.7)\n",
        "\n",
        "    # 단어 라벨 추가\n",
        "    for i, word in enumerate(word_labels):\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    plt.title('Word2Vec 벡터 시각화 (PCA)', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. 카테고리별 색상 구분\n",
        "    plt.subplot(2, 2, 2)\n",
        "\n",
        "    # 단어 카테고리 분류 (간단한 규칙 기반)\n",
        "    categories = {\n",
        "        '과일': ['사과', '바나나', '오렌지', '포도', '과일'],\n",
        "        '동물': ['개', '고양이', '토끼', '사자', '동물'],\n",
        "        '교통': ['자동차', '기차', '비행기', '배', '교통수단'],\n",
        "        '도구': ['컴퓨터', '스마트폰', '도구', '책', '신문', '잡지']\n",
        "    }\n",
        "\n",
        "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "    category_colors = {}\n",
        "\n",
        "    for i, (cat, words_in_cat) in enumerate(categories.items()):\n",
        "        for word in words_in_cat:\n",
        "            if word in word_labels:\n",
        "                category_colors[word] = colors[i % len(colors)]\n",
        "\n",
        "    # 카테고리별 색상으로 플롯\n",
        "    for i, word in enumerate(word_labels):\n",
        "        color = category_colors.get(word, 'gray')\n",
        "        plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1],\n",
        "                   c=color, s=100, alpha=0.7)\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    plt.title('카테고리별 단어 분포', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 범례 추가\n",
        "    for cat, color in zip(categories.keys(), colors):\n",
        "        plt.scatter([], [], c=color, s=100, label=cat, alpha=0.7)\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. 유사도 히트맵\n",
        "    plt.subplot(2, 2, 3)\n",
        "\n",
        "    # 주요 단어들만 선택\n",
        "    main_words = [w for w in word_labels if len(w) > 1][:10]\n",
        "    similarity_matrix = np.zeros((len(main_words), len(main_words)))\n",
        "\n",
        "    for i, word1 in enumerate(main_words):\n",
        "        for j, word2 in enumerate(main_words):\n",
        "            similarity_matrix[i, j] = viz_model.similarity(word1, word2)\n",
        "\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                xticklabels=main_words,\n",
        "                yticklabels=main_words,\n",
        "                annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.title('단어 간 유사도 히트맵', fontsize=14)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    # 4. 벡터 크기 분포\n",
        "    plt.subplot(2, 2, 4)\n",
        "    vector_norms = [np.linalg.norm(viz_model.get_vector(word)) for word in word_labels]\n",
        "\n",
        "    plt.hist(vector_norms, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title('벡터 크기 분포', fontsize=14)\n",
        "    plt.xlabel('벡터 크기')\n",
        "    plt.ylabel('빈도')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 클러스터링 분석\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(vectors)\n",
        "\n",
        "    print(f\"\\n🎯 클러스터링 결과:\")\n",
        "    cluster_words = defaultdict(list)\n",
        "    for word, cluster in zip(word_labels, clusters):\n",
        "        cluster_words[cluster].append(word)\n",
        "\n",
        "    for cluster_id, words in cluster_words.items():\n",
        "        print(f\"  클러스터 {cluster_id}: {words}\")\n",
        "\n",
        "    return viz_model\n",
        "\n",
        "\n",
        "def demonstrate_word_analogies():\n",
        "    \"\"\"단어 유추 예제\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"4. Word2Vec 단어 유추 (Word Analogies)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 관계가 명확한 문장들로 학습\n",
        "    analogy_sentences = [\n",
        "        \"남자는 아버지가 됩니다\", \"여자는 어머니가 됩니다\",\n",
        "        \"남자는 왕이 됩니다\", \"여자는 여왕이 됩니다\",\n",
        "        \"남자는 아들입니다\", \"여자는 딸입니다\",\n",
        "        \"남자는 형이 됩니다\", \"여자는 누나가 됩니다\",\n",
        "        \"남자는 남편입니다\", \"여자는 아내입니다\",\n",
        "        \"어린 남자는 소년입니다\", \"어린 여자는 소녀입니다\",\n",
        "        \"서울은 한국의 수도입니다\", \"도쿄는 일본의 수도입니다\",\n",
        "        \"워싱턴은 미국의 수도입니다\", \"런던은 영국의 수도입니다\",\n",
        "        \"한국 사람은 한국어를 씁니다\", \"일본 사람은 일본어를 씁니다\",\n",
        "        \"미국 사람은 영어를 씁니다\", \"중국 사람은 중국어를 씁니다\",\n",
        "        \"개는 멍멍하고 웁니다\", \"고양이는 야옹하고 웁니다\",\n",
        "        \"소는 음메하고 웁니다\", \"돼지는 꿀꿀하고 웁니다\",\n",
        "        \"크다의 반대는 작다입니다\", \"높다의 반대는 낮다입니다\",\n",
        "        \"뜨겁다의 반대는 차갑다입니다\", \"밝다의 반대는 어둡다입니다\"\n",
        "    ]\n",
        "\n",
        "    # 모델 학습\n",
        "    analogy_model = Word2VecImplementation(\n",
        "        vector_size=100,\n",
        "        window_size=3,\n",
        "        epochs=500,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    analogy_model.train(analogy_sentences)\n",
        "\n",
        "    # 단어 유추 테스트\n",
        "    print(f\"\\n🧩 단어 유추 테스트:\")\n",
        "\n",
        "    analogy_tests = [\n",
        "        (\"남자\", \"아버지\", \"여자\"),  # 남자:아버지 = 여자:?\n",
        "        (\"남자\", \"왕\", \"여자\"),      # 남자:왕 = 여자:?\n",
        "        (\"서울\", \"한국\", \"도쿄\"),    # 서울:한국 = 도쿄:?\n",
        "        (\"크다\", \"작다\", \"높다\"),    # 크다:작다 = 높다:?\n",
        "        (\"개\", \"멍멍\", \"고양이\")     # 개:멍멍 = 고양이:?\n",
        "    ]\n",
        "\n",
        "    for word_a, word_b, word_c in analogy_tests:\n",
        "        print(f\"\\n  {word_a} : {word_b} = {word_c} : ?\")\n",
        "\n",
        "        results = analogy_model.analogy(word_a, word_b, word_c, top_k=3)\n",
        "        if results:\n",
        "            print(f\"    예측 결과:\")\n",
        "            for word, score in results:\n",
        "                print(f\"      {word} (유사도: {score:.4f})\")\n",
        "        else:\n",
        "            print(f\"    결과를 찾을 수 없습니다.\")\n",
        "\n",
        "\n",
        "def demonstrate_practical_applications():\n",
        "    \"\"\"Word2Vec 실전 응용\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"5. Word2Vec 실전 응용 - 문서 분류 및 추천\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 뉴스 카테고리별 문서들\n",
        "    news_data = {\n",
        "        '기술': [\n",
        "            \"인공지능 기술이 빠르게 발전하면서 다양한 산업 분야에 혁신을 가져오고 있습니다\",\n",
        "            \"머신러닝과 딥러닝 알고리즘을 활용한 자동화 시스템이 도입되고 있습니다\",\n",
        "            \"빅데이터 분석 기술로 고객 행동 패턴을 예측하는 서비스가 등장했습니다\",\n",
        "            \"클라우드 컴퓨팅 플랫폼이 기업의 디지털 전환을 가속화하고 있습니다\",\n",
        "            \"사물인터넷과 스마트시티 기술이 도시 인프라를 혁신하고 있습니다\"\n",
        "        ],\n",
        "        '스포츠': [\n",
        "            \"월드컵 축구 대회에서 한국 팀이 뛰어난 경기력을 보여주었습니다\",\n",
        "            \"올림픽 수영 선수가 새로운 세계 기록을 경신했습니다\",\n",
        "            \"프로야구 시즌이 시작되면서 팬들의 관심이 높아지고 있습니다\",\n",
        "            \"테니스 그랜드슬램 대회에서 한국 선수가 우승을 차지했습니다\",\n",
        "            \"마라톤 대회에 수많은 시민들이 참여하여 건강한 축제가 되었습니다\"\n",
        "        ],\n",
        "        '경제': [\n",
        "            \"국내 경제 성장률이 전년 대비 상승세를 보이고 있습니다\",\n",
        "            \"주식시장에서 기술주를 중심으로 상승 랠리가 이어지고 있습니다\",\n",
        "            \"부동산 시장의 안정화 정책이 시행되면서 거래량이 조정되고 있습니다\",\n",
        "            \"중소기업 지원 정책으로 창업 생태계가 활성화되고 있습니다\",\n",
        "            \"글로벌 공급망 이슈가 국내 제조업에 영향을 미치고 있습니다\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 모든 문서와 라벨 생성\n",
        "    all_documents = []\n",
        "    all_labels = []\n",
        "\n",
        "    for category, documents in news_data.items():\n",
        "        all_documents.extend(documents)\n",
        "        all_labels.extend([category] * len(documents))\n",
        "\n",
        "    # Word2Vec 모델 학습\n",
        "    app_model = Word2VecImplementation(\n",
        "        vector_size=100,\n",
        "        window_size=4,\n",
        "        epochs=300,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    app_model.train(all_documents)\n",
        "\n",
        "    print(f\"✅ 모델 학습 완료 (어휘 크기: {app_model.vocab_size})\")\n",
        "\n",
        "    # 문서 벡터화 (단어 벡터들의 평균)\n",
        "    def document_to_vector(doc, model):\n",
        "        words = model.preprocess_text(doc)\n",
        "        vectors = []\n",
        "        for word in words:\n",
        "            vec = model.get_vector(word)\n",
        "            if vec is not None:\n",
        "                vectors.append(vec)\n",
        "\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    # 문서 벡터들 생성\n",
        "    doc_vectors = []\n",
        "    for doc in all_documents:\n",
        "        doc_vec = document_to_vector(doc, app_model)\n",
        "        doc_vectors.append(doc_vec)\n",
        "\n",
        "    doc_vectors = np.array(doc_vectors)\n",
        "\n",
        "    # 문서 유사도 분석\n",
        "    print(f\"\\n📊 문서 유사도 분석:\")\n",
        "\n",
        "    # 카테고리 내/간 평균 유사도 계산\n",
        "    category_similarities = defaultdict(list)\n",
        "\n",
        "    for i in range(len(all_documents)):\n",
        "        for j in range(i+1, len(all_documents)):\n",
        "            sim = cosine_similarity([doc_vectors[i]], [doc_vectors[j]])[0][0]\n",
        "\n",
        "            if all_labels[i] == all_labels[j]:\n",
        "                category_similarities['같은_카테고리'].append(sim)\n",
        "            else:\n",
        "                category_similarities['다른_카테고리'].append(sim)\n",
        "\n",
        "    same_cat_avg = np.mean(category_similarities['같은_카테고리'])\n",
        "    diff_cat_avg = np.mean(category_similarities['다른_카테고리'])\n",
        "\n",
        "    print(f\"  같은 카테고리 문서 간 평균 유사도: {same_cat_avg:.4f}\")\n",
        "    print(f\"  다른 카테고리 문서 간 평균 유사도: {diff_cat_avg:.4f}\")\n",
        "    print(f\"  구별 성능: {same_cat_avg - diff_cat_avg:.4f}\")\n",
        "\n",
        "    # 문서 추천 시스템\n",
        "    print(f\"\\n🎯 문서 추천 시스템 테스트:\")\n",
        "\n",
        "    test_queries = [\n",
        "        \"인공지능과 머신러닝 기술 동향\",\n",
        "        \"축구 경기 결과와 선수 분석\",\n",
        "        \"경제 성장과 주식시장 전망\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n  쿼리: '{query}'\")\n",
        "        query_vector = document_to_vector(query, app_model)\n",
        "\n",
        "        # 모든 문서와의 유사도 계산\n",
        "        similarities = []\n",
        "        for i, doc_vec in enumerate(doc_vectors):\n",
        "            sim = cosine_similarity([query_vector], [doc_vec])[0][0]\n",
        "            similarities.append((i, sim, all_labels[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"    추천 문서 (상위 3개):\")\n",
        "        for i, (doc_idx, sim, category) in enumerate(similarities[:3]):\n",
        "            print(f\"      {i+1}. [{category}] 유사도: {sim:.4f}\")\n",
        "            print(f\"         {all_documents[doc_idx][:50]}...\")\n",
        "\n",
        "    # 시각화\n",
        "    ensure_korean_font()\n",
        "\n",
        "    # PCA로 문서 벡터 시각화\n",
        "    pca = PCA(n_components=2)\n",
        "    doc_vectors_2d = pca.fit_transform(doc_vectors)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    colors = {'기술': 'red', '스포츠': 'blue', '경제': 'green'}\n",
        "\n",
        "    for category in colors.keys():\n",
        "        indices = [i for i, label in enumerate(all_labels) if label == category]\n",
        "        plt.scatter(doc_vectors_2d[indices, 0], doc_vectors_2d[indices, 1],\n",
        "                   c=colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "    plt.title('Word2Vec 기반 문서 벡터 시각화', fontsize=16, pad=20)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return app_model, doc_vectors\n",
        "\n",
        "\n",
        "def analyze_word2vec_properties():\n",
        "    \"\"\"Word2Vec 속성 및 특성 분석\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"6. Word2Vec 속성 및 특성 분석\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 분석용 문서 - 명확한 관계가 있는 단어들\n",
        "    analysis_sentences = [\n",
        "        \"사과 바나나 오렌지는 달콤한 과일입니다\",\n",
        "        \"포도 딸기 복숭아도 맛있는 과일입니다\",\n",
        "        \"개 고양이 토끼는 귀여운 동물입니다\",\n",
        "        \"사자 호랑이 코끼리는 큰 동물입니다\",\n",
        "        \"빨간색 파란색 노란색은 아름다운 색깔입니다\",\n",
        "        \"검은색 하얀색 회색도 중요한 색깔입니다\",\n",
        "        \"큰 작은 높은 낮은은 크기를 나타냅니다\",\n",
        "        \"빠른 느린 강한 약한은 정도를 나타냅니다\"\n",
        "    ]\n",
        "\n",
        "    # 모델 학습\n",
        "    analysis_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=3,\n",
        "        epochs=400,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    analysis_model.train(analysis_sentences)\n",
        "\n",
        "    # 1. 벡터 공간에서의 거리와 유사도 관계\n",
        "    print(f\"\\n📐 벡터 거리와 유사도 관계:\")\n",
        "\n",
        "    test_pairs = [\n",
        "        (\"사과\", \"바나나\"),    # 같은 카테고리\n",
        "        (\"사과\", \"개\"),       # 다른 카테고리\n",
        "        (\"큰\", \"작은\"),       # 반대 관계\n",
        "        (\"빨간색\", \"파란색\")  # 같은 카테고리, 다른 특성\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in test_pairs:\n",
        "        if word1 in analysis_model.word_to_idx and word2 in analysis_model.word_to_idx:\n",
        "            vec1 = analysis_model.get_vector(word1)\n",
        "            vec2 = analysis_model.get_vector(word2)\n",
        "\n",
        "            # 유클리드 거리\n",
        "            euclidean_dist = np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "            # 코사인 유사도\n",
        "            cosine_sim = analysis_model.similarity(word1, word2)\n",
        "\n",
        "            print(f\"  {word1} - {word2}:\")\n",
        "            print(f\"    유클리드 거리: {euclidean_dist:.4f}\")\n",
        "            print(f\"    코사인 유사도: {cosine_sim:.4f}\")\n",
        "\n",
        "    # 2. 차원 수에 따른 성능 비교\n",
        "    print(f\"\\n📊 차원 수에 따른 성능 비교:\")\n",
        "\n",
        "    dimensions = [10, 30, 50, 100]\n",
        "    results = []\n",
        "\n",
        "    for dim in dimensions:\n",
        "        model = Word2VecImplementation(\n",
        "            vector_size=dim,\n",
        "            window_size=3,\n",
        "            epochs=200,\n",
        "            model_type='skipgram',\n",
        "            learning_rate=0.1\n",
        "        )\n",
        "        model.train(analysis_sentences)\n",
        "\n",
        "        # 같은 카테고리 단어들의 평균 유사도\n",
        "        fruit_words = [\"사과\", \"바나나\", \"포도\"]\n",
        "        similarities = []\n",
        "\n",
        "        for i in range(len(fruit_words)):\n",
        "            for j in range(i+1, len(fruit_words)):\n",
        "                if fruit_words[i] in model.word_to_idx and fruit_words[j] in model.word_to_idx:\n",
        "                    sim = model.similarity(fruit_words[i], fruit_words[j])\n",
        "                    similarities.append(sim)\n",
        "\n",
        "        avg_sim = np.mean(similarities) if similarities else 0\n",
        "        results.append(avg_sim)\n",
        "        print(f\"  {dim}차원: 과일 단어 평균 유사도 = {avg_sim:.4f}\")\n",
        "\n",
        "    # 3. Skip-gram vs CBOW 성능 비교\n",
        "    print(f\"\\n⚔️ Skip-gram vs CBOW 성능 비교:\")\n",
        "\n",
        "    # Skip-gram 모델\n",
        "    sg_model = Word2VecImplementation(\n",
        "        vector_size=50, window_size=3, epochs=300,\n",
        "        model_type='skipgram', learning_rate=0.1\n",
        "    )\n",
        "    sg_model.train(analysis_sentences)\n",
        "\n",
        "    # CBOW 모델\n",
        "    cbow_model = Word2VecImplementation(\n",
        "        vector_size=50, window_size=3, epochs=300,\n",
        "        model_type='cbow', learning_rate=0.1\n",
        "    )\n",
        "    cbow_model.train(analysis_sentences)\n",
        "\n",
        "    # 성능 비교: 유사 단어 찾기\n",
        "    test_word = \"사과\"\n",
        "    if test_word in sg_model.word_to_idx and test_word in cbow_model.word_to_idx:\n",
        "        sg_similar = sg_model.most_similar(test_word, top_k=3)\n",
        "        cbow_similar = cbow_model.most_similar(test_word, top_k=3)\n",
        "\n",
        "        print(f\"  '{test_word}'의 유사 단어:\")\n",
        "        print(f\"    Skip-gram: {sg_similar}\")\n",
        "        print(f\"    CBOW: {cbow_similar}\")\n",
        "\n",
        "    # 4. 시각화 - 학습 과정 시뮬레이션\n",
        "    ensure_korean_font()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 차원별 성능 그래프\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(dimensions, results, 'bo-', linewidth=2, markersize=8)\n",
        "    plt.title('벡터 차원 수에 따른 성능', fontsize=14)\n",
        "    plt.xlabel('차원 수')\n",
        "    plt.ylabel('평균 유사도')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 단어 벡터 시각화 (PCA)\n",
        "    plt.subplot(2, 2, 2)\n",
        "\n",
        "    # 주요 단어들의 벡터\n",
        "    target_words = [\"사과\", \"바나나\", \"개\", \"고양이\", \"빨간색\", \"파란색\"]\n",
        "    vectors = []\n",
        "    labels = []\n",
        "\n",
        "    for word in target_words:\n",
        "        if word in analysis_model.word_to_idx:\n",
        "            vec = analysis_model.get_vector(word)\n",
        "            if vec is not None:\n",
        "                vectors.append(vec)\n",
        "                labels.append(word)\n",
        "\n",
        "    if len(vectors) >= 2:\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        colors = ['red', 'red', 'blue', 'blue', 'green', 'green']\n",
        "\n",
        "        for i, (word, color) in enumerate(zip(labels, colors)):\n",
        "            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1],\n",
        "                       c=color, s=150, alpha=0.7)\n",
        "            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "    plt.title('단어 벡터 공간 (PCA)', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 유사도 분포 히스토그램\n",
        "    plt.subplot(2, 2, 3)\n",
        "\n",
        "    all_similarities = []\n",
        "    words = list(analysis_model.word_to_idx.keys())\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i+1, min(len(words), i+10)):  # 계산량 제한\n",
        "            sim = analysis_model.similarity(words[i], words[j])\n",
        "            all_similarities.append(sim)\n",
        "\n",
        "    plt.hist(all_similarities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title('단어 간 유사도 분포', fontsize=14)\n",
        "    plt.xlabel('유사도')\n",
        "    plt.ylabel('빈도')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 벡터 크기 분포\n",
        "    plt.subplot(2, 2, 4)\n",
        "\n",
        "    vector_norms = []\n",
        "    for word in words:\n",
        "        vec = analysis_model.get_vector(word)\n",
        "        if vec is not None:\n",
        "            norm = np.linalg.norm(vec)\n",
        "            vector_norms.append(norm)\n",
        "\n",
        "    plt.hist(vector_norms, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "    plt.title('벡터 크기 분포', fontsize=14)\n",
        "    plt.xlabel('벡터 크기')\n",
        "    plt.ylabel('빈도')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return analysis_model\n",
        "\n",
        "\n",
        "def run_complete_word2vec_demo():\n",
        "    \"\"\"전체 Word2Vec 데모 실행\"\"\"\n",
        "    print(\"🚀 Word2Vec 완전 가이드 - 실습 시작!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # 1. 기본 구현\n",
        "        print(\"1️⃣ 기본 Word2Vec 구현...\")\n",
        "        skipgram_model, cbow_model = demonstrate_basic_word2vec()\n",
        "\n",
        "        # 2. Gensim 비교\n",
        "        print(\"\\n2️⃣ Gensim Word2Vec 비교...\")\n",
        "        gensim_model = demonstrate_gensim_word2vec()\n",
        "\n",
        "        # 3. 시각화\n",
        "        print(\"\\n3️⃣ Word2Vec 벡터 시각화...\")\n",
        "        viz_model = demonstrate_word2vec_visualization()\n",
        "\n",
        "        # 4. 단어 유추\n",
        "        print(\"\\n4️⃣ 단어 유추 테스트...\")\n",
        "        demonstrate_word_analogies()\n",
        "\n",
        "        # 5. 실전 응용\n",
        "        print(\"\\n5️⃣ 실전 응용...\")\n",
        "        app_model, doc_vectors = demonstrate_practical_applications()\n",
        "\n",
        "        # 6. 속성 분석\n",
        "        print(\"\\n6️⃣ Word2Vec 속성 분석...\")\n",
        "        analysis_model = analyze_word2vec_properties()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"🎉 Word2Vec 완전 가이드 실습 완료!\")\n",
        "        print(\"📚 학습한 내용:\")\n",
        "        print(\"  ✅ Word2Vec 기본 원리 및 구현 (Skip-gram & CBOW)\")\n",
        "        print(\"  ✅ Gensim과의 성능 비교\")\n",
        "        print(\"  ✅ 벡터 시각화 및 클러스터링\")\n",
        "        print(\"  ✅ 단어 유추 (Word Analogies)\")\n",
        "        print(\"  ✅ 문서 분류 및 추천 시스템\")\n",
        "        print(\"  ✅ Word2Vec 특성 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 실행 중 오류 발생: {str(e)}\")\n",
        "        print(\"기본 예제만 실행합니다.\")\n",
        "\n",
        "        # 기본 예제만 실행\n",
        "        sentences = [\n",
        "            \"인공지능은 미래 기술입니다\",\n",
        "            \"머신러닝으로 데이터를 분석합니다\",\n",
        "            \"딥러닝은 복잡한 패턴을 학습합니다\"\n",
        "        ]\n",
        "\n",
        "        basic_model = Word2VecImplementation(vector_size=50, epochs=100)\n",
        "        basic_model.train(sentences)\n",
        "\n",
        "        print(\"✅ 기본 Word2Vec 모델 생성 완료\")\n",
        "        print(f\"어휘 크기: {basic_model.vocab_size}\")\n",
        "\n",
        "\n",
        "# 실행부\n",
        "if __name__ == \"__main__\":\n",
        "    # 전체 데모 실행\n",
        "    run_complete_word2vec_demo()"
      ],
      "metadata": {
        "id": "EtuRoYnMTgeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-IvIUrIXB3hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **다양한 임베딩 종류**\n"
      ],
      "metadata": {
        "id": "C5oQMyRtB4SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 이미지 임베딩**"
      ],
      "metadata": {
        "id": "7XqRLmBLF3ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python scikit-learn matplotlib seaborn numpy pandas"
      ],
      "metadata": {
        "id": "6wDeF4fIZ9Z6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# 1. 모델 불러오기 (ImageNet 사전학습)\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# 2. 이미지 로드 & 전처리\n",
        "img_path = \"dog.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "# 3. 임베딩 추출\n",
        "embedding = model.predict(x)\n",
        "print(\"임베딩 벡터 shape:\", embedding.shape)  # (1, 2048)\n"
      ],
      "metadata": {
        "id": "W47wSwFy7Sv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# URL 없이 동작: CIFAR-10에서 dog/cat/car 추출 → ResNet50 임베딩 → 코사인 유사도\n",
        "# ===============================================\n",
        "import os, pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "\n",
        "\n",
        "# 0) 저장 폴더\n",
        "IMG_DIR = \"sample_images\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1) CIFAR-10 로드 (자동 다운로드) — 클래스 라벨 매핑\n",
        "# CIFAR-10 label names: 0 airplane, 1 automobile, 2 bird, 3 cat, 4 deer, 5 dog, 6 frog, 7 horse, 8 ship, 9 truck\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = y_train.flatten()\n",
        "\n",
        "label_to_name = {\n",
        "    1: \"car\",   # automobile\n",
        "    3: \"cat\",\n",
        "    5: \"dog\"\n",
        "}\n",
        "\n",
        "picked_paths = {}\n",
        "for label, name in label_to_name.items():\n",
        "    # 해당 클래스를 가진 첫 번째 이미지를 선택\n",
        "    idx = np.where(y_train == label)[0][0]\n",
        "    img_arr = x_train[idx]  # (32,32,3) uint8\n",
        "    # 저장 파일명\n",
        "    fname = f\"{name}.jpg\"\n",
        "    fpath = os.path.join(IMG_DIR, fname)\n",
        "    # 224x224로 업스케일 저장 (ResNet 입력 크기)\n",
        "    Image.fromarray(img_arr).resize((224, 224), Image.BICUBIC).save(fpath, format=\"JPEG\", quality=95)\n",
        "    picked_paths[name] = fpath\n",
        "\n",
        "print(\"저장된 파일:\", picked_paths)\n",
        "\n",
        "\n",
        "# 2) ResNet50 (ImageNet 사전학습, 분류헤드 제거)\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')  # (1, 2048) 임베딩\n",
        "\n",
        "def get_embedding(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))   # 혹시 모를 크기 보정\n",
        "    x = image.img_to_array(img)[None, ...]                   # (1,224,224,3)\n",
        "    x = preprocess_input(x)\n",
        "    emb = model.predict(x, verbose=0)                        # (1,2048)\n",
        "    return emb\n",
        "\n",
        "\n",
        "# 3) 임베딩 추출\n",
        "image_files = [picked_paths[\"dog\"], picked_paths[\"cat\"], picked_paths[\"car\"]]\n",
        "embeddings = {p: get_embedding(p) for p in image_files}\n",
        "\n",
        "\n",
        "# 4) 코사인 유사도 행렬\n",
        "paths = list(embeddings.keys())\n",
        "mat = np.vstack([embeddings[p] for p in paths])              # (N,2048)\n",
        "sim = cosine_similarity(mat, mat)\n",
        "\n",
        "df_sim = pd.DataFrame(sim,\n",
        "                      index=[pathlib.Path(p).name for p in paths],\n",
        "                      columns=[pathlib.Path(p).name for p in paths])\n",
        "print(\"\\n📊 전체 코사인 유사도 행렬\")\n",
        "try:\n",
        "    display(df_sim.style.format(\"{:.4f}\"))\n",
        "except NameError:\n",
        "    print(df_sim.round(4))\n",
        "\n",
        "\n",
        "\n",
        "# 5) dog.jpg 기준 유사도 정렬\n",
        "target_name = \"dog.jpg\"\n",
        "target_path = os.path.join(IMG_DIR, target_name)\n",
        "target_idx = paths.index(target_path)\n",
        "scores = sim[target_idx]\n",
        "\n",
        "ranked = sorted(\n",
        "    [(pathlib.Path(p).name, float(scores[i])) for i, p in enumerate(paths) if i != target_idx],\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ '{target_name}'와 유사도가 높은 순\")\n",
        "for name, s in ranked:\n",
        "    print(f\"  {name:10s} : {s:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# 6) 미니 갤러리 시각화\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
        "for ax, p in zip(axes, image_files):\n",
        "    ax.imshow(Image.open(p))\n",
        "    ax.set_title(pathlib.Path(p).name)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 7) 바 차트\n",
        "plt.figure(figsize=(5,3))\n",
        "names = [n for n, _ in ranked]\n",
        "vals  = [s for _, s in ranked]\n",
        "plt.bar(names, vals)\n",
        "plt.ylim(0, 1.0)\n",
        "for i, v in enumerate(vals):\n",
        "    plt.text(i, v+0.02, f\"{v:.3f}\", ha=\"center\")\n",
        "plt.title(f\"Cosine similarity to {target_name}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NuioOLed_PH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "Q30MWyFt7uR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 임베딩 간단 테스트 - 바로 실행 가능\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "\n",
        "\n",
        "def simple_image_embedding_demo():\n",
        "    \"\"\"\n",
        "    이미지 임베딩 기본 개념을 보여주는 간단한 예제\n",
        "    \"\"\"\n",
        "    print(\"🖼️ 이미지 임베딩 기본 개념 이해하기\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1단계: 간단한 이미지들 생성\n",
        "    def create_simple_images():\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        # 빨간 사각형\n",
        "        red_square = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "        red_square[16:48, 16:48] = [0, 0, 255]  # BGR 형식\n",
        "        images.append(red_square)\n",
        "        labels.append('빨간 사각형')\n",
        "\n",
        "        # 파란 원\n",
        "        blue_circle = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "        cv2.circle(blue_circle, (32, 32), 16, (255, 0, 0), -1)\n",
        "        images.append(blue_circle)\n",
        "        labels.append('파란 원')\n",
        "\n",
        "        # 녹색 선\n",
        "        green_lines = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "        for i in range(0, 64, 8):\n",
        "            cv2.line(green_lines, (i, 0), (i, 64), (0, 255, 0), 2)\n",
        "        images.append(green_lines)\n",
        "        labels.append('녹색 선들')\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    images, image_labels = create_simple_images()\n",
        "\n",
        "    print(\"📄 생성된 이미지들:\")\n",
        "    for i, label in enumerate(image_labels):\n",
        "        print(f\"  이미지{i+1}: {label}\")\n",
        "\n",
        "    # 2단계: 간단한 특징 추출\n",
        "    def extract_simple_features(image):\n",
        "        \"\"\"간단한 이미지 특징 추출\"\"\"\n",
        "        # RGB 채널별 평균값\n",
        "        b_mean = np.mean(image[:, :, 0])  # Blue\n",
        "        g_mean = np.mean(image[:, :, 1])  # Green\n",
        "        r_mean = np.mean(image[:, :, 2])  # Red\n",
        "\n",
        "        # 그레이스케일로 변환\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # 기본 통계\n",
        "        brightness = np.mean(gray)\n",
        "        contrast = np.std(gray)\n",
        "\n",
        "        # 에지 밀도 (간단한 버전)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        edge_density = np.sum(edges > 0) / edges.size\n",
        "\n",
        "        # 특징 벡터 생성\n",
        "        features = np.array([r_mean, g_mean, b_mean, brightness, contrast, edge_density])\n",
        "\n",
        "        return features\n",
        "\n",
        "    print(f\"\\n🔢 특징 추출 과정:\")\n",
        "\n",
        "    all_features = []\n",
        "    for i, (image, label) in enumerate(zip(images, image_labels)):\n",
        "        features = extract_simple_features(image)\n",
        "        all_features.append(features)\n",
        "\n",
        "        print(f\"\\n  {label}:\")\n",
        "        print(f\"    RGB 평균: R={features[0]:.1f}, G={features[1]:.1f}, B={features[2]:.1f}\")\n",
        "        print(f\"    밝기: {features[3]:.1f}\")\n",
        "        print(f\"    대비: {features[4]:.1f}\")\n",
        "        print(f\"    에지 밀도: {features[5]:.4f}\")\n",
        "\n",
        "    all_features = np.array(all_features)\n",
        "\n",
        "    # 3단계: 유사도 계산\n",
        "    print(f\"\\n🔗 이미지 간 유사도 (코사인 유사도):\")\n",
        "\n",
        "    similarity_matrix = cosine_similarity(all_features)\n",
        "\n",
        "    for i in range(len(image_labels)):\n",
        "        for j in range(i+1, len(image_labels)):\n",
        "            sim = similarity_matrix[i][j]\n",
        "            print(f\"  {image_labels[i]} - {image_labels[j]}: {sim:.4f}\")\n",
        "\n",
        "    # 4단계: 시각화\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # 원본 이미지들\n",
        "    for i, (image, label) in enumerate(zip(images, image_labels)):\n",
        "        # BGR을 RGB로 변환\n",
        "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        axes[0, i].imshow(rgb_image)\n",
        "        axes[0, i].set_title(f'{label}')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "    # 특징 벡터 시각화\n",
        "    axes[1, 0].bar(['R', 'G', 'B', '밝기', '대비', '에지'], all_features[0],\n",
        "                   color=['red', 'green', 'blue', 'gray', 'orange', 'purple'], alpha=0.7)\n",
        "    axes[1, 0].set_title('빨간 사각형 특징')\n",
        "    axes[1, 0].set_ylabel('특징값')\n",
        "\n",
        "    axes[1, 1].bar(['R', 'G', 'B', '밝기', '대비', '에지'], all_features[1],\n",
        "                   color=['red', 'green', 'blue', 'gray', 'orange', 'purple'], alpha=0.7)\n",
        "    axes[1, 1].set_title('파란 원 특징')\n",
        "    axes[1, 1].set_ylabel('특징값')\n",
        "\n",
        "    axes[1, 2].bar(['R', 'G', 'B', '밝기', '대비', '에지'], all_features[2],\n",
        "                   color=['red', 'green', 'blue', 'gray', 'orange', 'purple'], alpha=0.7)\n",
        "    axes[1, 2].set_title('녹색 선들 특징')\n",
        "    axes[1, 2].set_ylabel('특징값')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5단계: 특징 공간에서 시각화\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # PCA로 2차원 축소\n",
        "    if all_features.shape[1] > 2:\n",
        "        pca = PCA(n_components=2)\n",
        "        features_2d = pca.fit_transform(all_features)\n",
        "        explained_variance = pca.explained_variance_ratio_.sum()\n",
        "    else:\n",
        "        features_2d = all_features[:, :2]\n",
        "        explained_variance = 1.0\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    colors = ['red', 'blue', 'green']\n",
        "\n",
        "    for i, (point, label, color) in enumerate(zip(features_2d, image_labels, colors)):\n",
        "        plt.scatter(point[0], point[1], c=color, s=200, alpha=0.7, label=label)\n",
        "        plt.annotate(f'{i+1}', (point[0], point[1]),\n",
        "                    ha='center', va='center', fontweight='bold', color='white')\n",
        "\n",
        "    plt.title(f'특징 공간에서의 이미지 위치\\n(PCA 설명력: {explained_variance:.1%})')\n",
        "    plt.xlabel('주성분 1')\n",
        "    plt.ylabel('주성분 2')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 유사도 히트맵\n",
        "    plt.subplot(1, 2, 2)\n",
        "    im = plt.imshow(similarity_matrix, cmap='Blues', vmin=0, vmax=1)\n",
        "    plt.colorbar(im)\n",
        "\n",
        "    # 유사도 값 표시\n",
        "    for i in range(len(image_labels)):\n",
        "        for j in range(len(image_labels)):\n",
        "            plt.text(j, i, f'{similarity_matrix[i, j]:.3f}',\n",
        "                    ha='center', va='center', fontweight='bold')\n",
        "\n",
        "    plt.title('이미지 간 유사도 매트릭스')\n",
        "    plt.xticks(range(len(image_labels)), [f'이미지{i+1}' for i in range(len(image_labels))])\n",
        "    plt.yticks(range(len(image_labels)), [f'이미지{i+1}' for i in range(len(image_labels))])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n💡 이미지 임베딩 핵심 원리:\")\n",
        "    print(\"  ✅ 이미지를 수치 벡터로 변환\")\n",
        "    print(\"  ✅ 비슷한 이미지는 비슷한 벡터를 가짐\")\n",
        "    print(\"  ✅ 벡터 간 거리로 유사도 측정\")\n",
        "    print(\"  ✅ 특징 선택이 성능에 중요한 영향\")\n",
        "\n",
        "    return all_features, image_labels\n",
        "\n",
        "\n",
        "def practical_image_search_demo():\n",
        "    \"\"\"\n",
        "    실용적인 이미지 검색 예제\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"🔍 실용적 이미지 검색 시스템\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 이미지 데이터베이스 생성\n",
        "    def create_image_db():\n",
        "        db_images = []\n",
        "        db_descriptions = []\n",
        "\n",
        "        # 다양한 패턴의 이미지들\n",
        "        patterns = [\n",
        "            {'desc': '빨간_사각형', 'shape': 'rect', 'color': (0, 0, 255)},\n",
        "            {'desc': '파란_사각형', 'shape': 'rect', 'color': (255, 0, 0)},\n",
        "            {'desc': '녹색_원', 'shape': 'circle', 'color': (0, 255, 0)},\n",
        "            {'desc': '노란_원', 'shape': 'circle', 'color': (0, 255, 255)},\n",
        "            {'desc': '수직_선', 'shape': 'v_lines', 'color': (128, 128, 128)},\n",
        "            {'desc': '수평_선', 'shape': 'h_lines', 'color': (128, 128, 128)},\n",
        "            {'desc': '체크_패턴', 'shape': 'checker', 'color': (255, 255, 255)},\n",
        "            {'desc': '대각_선', 'shape': 'diagonal', 'color': (0, 128, 255)}\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "            if pattern['shape'] == 'rect':\n",
        "                cv2.rectangle(img, (16, 16), (48, 48), pattern['color'], -1)\n",
        "            elif pattern['shape'] == 'circle':\n",
        "                cv2.circle(img, (32, 32), 16, pattern['color'], -1)\n",
        "            elif pattern['shape'] == 'v_lines':\n",
        "                for x in range(8, 64, 8):\n",
        "                    cv2.line(img, (x, 0), (x, 64), pattern['color'], 2)\n",
        "            elif pattern['shape'] == 'h_lines':\n",
        "                for y in range(8, 64, 8):\n",
        "                    cv2.line(img, (0, y), (64, y), pattern['color'], 2)\n",
        "            elif pattern['shape'] == 'checker':\n",
        "                for i in range(0, 64, 16):\n",
        "                    for j in range(0, 64, 16):\n",
        "                        if (i//16 + j//16) % 2 == 0:\n",
        "                            img[i:i+16, j:j+16] = pattern['color']\n",
        "            elif pattern['shape'] == 'diagonal':\n",
        "                cv2.line(img, (0, 0), (64, 64), pattern['color'], 3)\n",
        "                cv2.line(img, (0, 64), (64, 0), pattern['color'], 3)\n",
        "\n",
        "            db_images.append(img)\n",
        "            db_descriptions.append(pattern['desc'])\n",
        "\n",
        "        return db_images, db_descriptions\n",
        "\n",
        "    # 데이터베이스 생성\n",
        "    db_images, db_descriptions = create_image_db()\n",
        "\n",
        "    print(f\"데이터베이스 크기: {len(db_images)}개 이미지\")\n",
        "\n",
        "    # 간단한 특징 추출 함수\n",
        "    def extract_features(image):\n",
        "        # RGB 평균\n",
        "        r_mean = np.mean(image[:, :, 2])\n",
        "        g_mean = np.mean(image[:, :, 1])\n",
        "        b_mean = np.mean(image[:, :, 0])\n",
        "\n",
        "        # 그레이스케일 변환 및 통계\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        brightness = np.mean(gray)\n",
        "        contrast = np.std(gray)\n",
        "\n",
        "        # 에지 특징\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        edge_density = np.sum(edges > 0) / edges.size\n",
        "\n",
        "        # 방향성 특징 (간단한 버전)\n",
        "        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "        horizontal_edges = np.sum(np.abs(sobel_x)) / gray.size\n",
        "        vertical_edges = np.sum(np.abs(sobel_y)) / gray.size\n",
        "\n",
        "        return np.array([r_mean, g_mean, b_mean, brightness, contrast,\n",
        "                        edge_density, horizontal_edges, vertical_edges])\n",
        "\n",
        "    # 모든 DB 이미지의 특징 추출\n",
        "    db_features = []\n",
        "    for img in db_images:\n",
        "        features = extract_features(img)\n",
        "        db_features.append(features)\n",
        "    db_features = np.array(db_features)\n",
        "\n",
        "    # 쿼리 이미지 생성 (파란 원과 유사)\n",
        "    query_img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    cv2.circle(query_img, (32, 32), 14, (200, 50, 50), -1)  # 약간 다른 파란 원\n",
        "\n",
        "    query_features = extract_features(query_img)\n",
        "\n",
        "    # 유사도 계산\n",
        "    similarities = cosine_similarity([query_features], db_features)[0]\n",
        "\n",
        "    # 가장 유사한 이미지들 찾기\n",
        "    top_indices = np.argsort(similarities)[::-1][:3]\n",
        "\n",
        "    print(f\"\\n🎯 검색 결과:\")\n",
        "    print(f\"쿼리: 파란색 원 (약간 다른 크기)\")\n",
        "\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        print(f\"  {i+1}위: {db_descriptions[idx]} (유사도: {similarities[idx]:.4f})\")\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # 쿼리 이미지\n",
        "    plt.subplot(2, 4, 1)\n",
        "    query_rgb = cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(query_rgb)\n",
        "    plt.title('쿼리 이미지')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # 검색 결과\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        plt.subplot(2, 4, i+2)\n",
        "        result_rgb = cv2.cvtColor(db_images[idx], cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(result_rgb)\n",
        "        plt.title(f'{i+1}위: {db_descriptions[idx]}\\n유사도: {similarities[idx]:.3f}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    # 전체 데이터베이스 (하위 4개)\n",
        "    bottom_indices = np.argsort(similarities)[:4]\n",
        "    for i, idx in enumerate(bottom_indices):\n",
        "        plt.subplot(2, 4, i+5)\n",
        "        img_rgb = cv2.cvtColor(db_images[idx], cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.title(f'{db_descriptions[idx]}\\n유사도: {similarities[idx]:.3f}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return db_features, similarities\n",
        "\n",
        "\n",
        "def feature_importance_analysis():\n",
        "    \"\"\"\n",
        "    특징의 중요도 분석\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"📊 특징 중요도 분석\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 다양한 이미지 생성\n",
        "    test_images = []\n",
        "    image_types = []\n",
        "\n",
        "    # 색상이 중요한 이미지들\n",
        "    red_img = np.full((64, 64, 3), [0, 0, 255], dtype=np.uint8)\n",
        "    blue_img = np.full((64, 64, 3), [255, 0, 0], dtype=np.uint8)\n",
        "\n",
        "    # 형태가 중요한 이미지들\n",
        "    circle_img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    cv2.circle(circle_img, (32, 32), 20, (128, 128, 128), -1)\n",
        "\n",
        "    square_img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    cv2.rectangle(square_img, (12, 12), (52, 52), (128, 128, 128), -1)\n",
        "\n",
        "    # 텍스처가 중요한 이미지들\n",
        "    noise_img = np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8)\n",
        "    lines_img = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    for i in range(0, 64, 4):\n",
        "        lines_img[i:i+2, :] = 200\n",
        "\n",
        "    test_images = [red_img, blue_img, circle_img, square_img, noise_img, lines_img]\n",
        "    image_types = ['빨간색', '파란색', '원', '사각형', '노이즈', '선 패턴']\n",
        "\n",
        "    # 특징 추출\n",
        "    def detailed_feature_extraction(image):\n",
        "        # 색상 특징\n",
        "        r_mean = np.mean(image[:, :, 2])\n",
        "        g_mean = np.mean(image[:, :, 1])\n",
        "        b_mean = np.mean(image[:, :, 0])\n",
        "\n",
        "        # 형태 특징\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        edge_density = np.sum(edges > 0) / edges.size\n",
        "\n",
        "        # 둘레 길이 추정 (간단한 방법)\n",
        "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        perimeter = sum(cv2.arcLength(c, True) for c in contours)\n",
        "\n",
        "        # 텍스처 특징\n",
        "        brightness = np.mean(gray)\n",
        "        contrast = np.std(gray)\n",
        "\n",
        "        # 지역적 변화량\n",
        "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "        texture_variation = np.var(laplacian)\n",
        "\n",
        "        return {\n",
        "            '빨간색': r_mean,\n",
        "            '녹색': g_mean,\n",
        "            '파란색': b_mean,\n",
        "            '에지밀도': edge_density,\n",
        "            '둘레': perimeter / 1000,  # 스케일 조정\n",
        "            '밝기': brightness,\n",
        "            '대비': contrast,\n",
        "            '텍스처변화': texture_variation / 1000  # 스케일 조정\n",
        "        }\n",
        "\n",
        "    # 모든 이미지의 특징 추출\n",
        "    all_features = []\n",
        "    feature_names = None\n",
        "\n",
        "    for img in test_images:\n",
        "        features_dict = detailed_feature_extraction(img)\n",
        "        if feature_names is None:\n",
        "            feature_names = list(features_dict.keys())\n",
        "        all_features.append(list(features_dict.values()))\n",
        "\n",
        "    all_features = np.array(all_features)\n",
        "\n",
        "    print(f\"추출된 특징들: {feature_names}\")\n",
        "\n",
        "    # 특징별 분석\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 각 이미지 표시\n",
        "    for i, (img, img_type) in enumerate(zip(test_images, image_types)):\n",
        "        plt.subplot(3, 6, i+1)\n",
        "        if img_type in ['노이즈']:\n",
        "            plt.imshow(img)\n",
        "        else:\n",
        "            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            plt.imshow(rgb_img)\n",
        "        plt.title(img_type)\n",
        "        plt.axis('off')\n",
        "\n",
        "    # 특징별 히트맵\n",
        "    plt.subplot(3, 2, 3)\n",
        "    normalized_features = (all_features - all_features.min(axis=0)) / (all_features.max(axis=0) - all_features.min(axis=0) + 1e-7)\n",
        "\n",
        "    im = plt.imshow(normalized_features.T, cmap='RdYlBu_r', aspect='auto')\n",
        "    plt.colorbar(im)\n",
        "    plt.yticks(range(len(feature_names)), feature_names)\n",
        "    plt.xticks(range(len(image_types)), image_types, rotation=45)\n",
        "    plt.title('정규화된 특징 히트맵')\n",
        "\n",
        "    # 각 특징별 분포\n",
        "    plt.subplot(3, 2, 4)\n",
        "    feature_variances = np.var(normalized_features, axis=0)\n",
        "    bars = plt.bar(range(len(feature_names)), feature_variances,\n",
        "                   color=['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray'])\n",
        "    plt.xticks(range(len(feature_names)), feature_names, rotation=45, ha='right')\n",
        "\n",
        "\n",
        "# 이미지 임베딩 기본 개념 시각화\n",
        "simple_image_embedding_demo()\n"
      ],
      "metadata": {
        "id": "uNlTCqbbF3jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실용적인 이미지 검색 예제\n",
        "practical_image_search_demo()\n"
      ],
      "metadata": {
        "id": "uzSv8JwMalaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i1MV8s2BniDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 특징의 중요도 분석\n",
        "feature_importance_analysis()"
      ],
      "metadata": {
        "id": "I1xrHsU5cUfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_hGl48O7Hqxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 허깅페이스 한국어 임베딩 모델 활용**"
      ],
      "metadata": {
        "id": "EsGjt-zjHrGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 기본 예제"
      ],
      "metadata": {
        "id": "fe-am4AfIoMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class KoreanEmbeddingSystem:\n",
        "    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):\n",
        "        \"\"\"\n",
        "        한국어 임베딩 시스템 초기화\n",
        "\n",
        "        추천 모델들:\n",
        "        - 'jhgan/ko-sroberta-multitask': 한국어 Sentence-RoBERTa (문장 임베딩 특화)\n",
        "        - 'BM-K/KoSimCSE-roberta-multitask': KoSimCSE (의미 유사도 특화)\n",
        "        - 'klue/roberta-base': KLUE RoBERTa (범용)\n",
        "        - 'monologg/kobert': KoBERT (SKT)\n",
        "        \"\"\"\n",
        "        print(f\"모델 로딩 중: {model_name}\")\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"모델 로딩 완료! 디바이스: {self.device}\")\n",
        "\n",
        "    def encode_sentences(self, sentences: List[str], max_length: int = 512) -> np.ndarray:\n",
        "        \"\"\"문장들을 벡터로 인코딩\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sentence in sentences:\n",
        "                # 토큰화\n",
        "                inputs = self.tokenizer(\n",
        "                    sentence,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors='pt'\n",
        "                ).to(self.device)\n",
        "\n",
        "                # 임베딩 생성\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # [CLS] 토큰의 임베딩을 문장 임베딩으로 사용\n",
        "                # 또는 평균 풀링 사용\n",
        "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                    embedding = outputs.pooler_output\n",
        "                else:\n",
        "                    # 평균 풀링\n",
        "                    embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "                embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def calculate_similarity(self, sentences1: List[str], sentences2: List[str] = None) -> np.ndarray:\n",
        "        \"\"\"문장들 간의 코사인 유사도 계산\"\"\"\n",
        "        embeddings1 = self.encode_sentences(sentences1)\n",
        "\n",
        "        if sentences2 is None:\n",
        "            # 자기 자신과의 유사도 매트릭스\n",
        "            similarity_matrix = cosine_similarity(embeddings1)\n",
        "        else:\n",
        "            # 두 그룹 간의 유사도\n",
        "            embeddings2 = self.encode_sentences(sentences2)\n",
        "            similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "        return similarity_matrix\n",
        "\n",
        "    def find_most_similar(self, query: str, candidates: List[str], top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"질의문과 가장 유사한 문장들 찾기\"\"\"\n",
        "        all_sentences = [query] + candidates\n",
        "        embeddings = self.encode_sentences(all_sentences)\n",
        "\n",
        "        query_embedding = embeddings[0:1]\n",
        "        candidate_embeddings = embeddings[1:]\n",
        "\n",
        "        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
        "\n",
        "        # 상위 k개 결과\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'sentence': candidates[idx],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# 메인 실행 함수\n",
        "def main():\n",
        "    print(\"=== 한국어 임베딩 모델 활용 시스템 ===\\n\")\n",
        "\n",
        "    # 시스템 초기화 (여러 모델 중 선택 가능)\n",
        "    embedding_system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # 테스트 문장들\n",
        "    korean_sentences = [\n",
        "        \"오늘 날씨가 정말 좋습니다.\",\n",
        "        \"날씨가 매우 화창하네요.\",\n",
        "        \"비가 많이 내리고 있어요.\",\n",
        "        \"인공지능 기술이 빠르게 발전하고 있다.\",\n",
        "        \"머신러닝은 컴퓨터가 데이터로부터 학습하는 기술이다.\",\n",
        "        \"점심에 뭘 먹을지 고민이에요.\",\n",
        "        \"오늘 저녁 메뉴를 정해야겠어요.\",\n",
        "        \"주식 시장이 불안정합니다.\",\n",
        "        \"경제 상황이 좋지 않아 보입니다.\"\n",
        "    ]\n",
        "\n",
        "    print(\"테스트 문장들:\")\n",
        "    for i, sentence in enumerate(korean_sentences, 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "    print(f\"\\n총 {len(korean_sentences)}개 문장의 임베딩을 생성합니다...\")\n",
        "\n",
        "    # 1. 문장 임베딩 생성\n",
        "    embeddings = embedding_system.encode_sentences(korean_sentences)\n",
        "    print(f\"임베딩 shape: {embeddings.shape}\")\n",
        "    print(f\"각 문장은 {embeddings.shape[1]}차원 벡터로 표현됩니다.\")\n",
        "\n",
        "    # 2. 유사도 매트릭스 계산\n",
        "    print(\"\\n=== 문장 간 유사도 분석 ===\")\n",
        "    similarity_matrix = embedding_system.calculate_similarity(korean_sentences)\n",
        "\n",
        "    # 유사도가 높은 문장 쌍 찾기\n",
        "    print(\"\\n가장 유사한 문장 쌍들 (Top 5):\")\n",
        "    similar_pairs = []\n",
        "\n",
        "    for i in range(len(korean_sentences)):\n",
        "        for j in range(i+1, len(korean_sentences)):\n",
        "            similarity = similarity_matrix[i][j]\n",
        "            similar_pairs.append({\n",
        "                'sentence1': korean_sentences[i],\n",
        "                'sentence2': korean_sentences[j],\n",
        "                'similarity': similarity\n",
        "            })\n",
        "\n",
        "    # 유사도 순으로 정렬\n",
        "    similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "    for i, pair in enumerate(similar_pairs[:5], 1):\n",
        "        print(f\"{i}. 유사도: {pair['similarity']:.4f}\")\n",
        "        print(f\"   문장1: {pair['sentence1']}\")\n",
        "        print(f\"   문장2: {pair['sentence2']}\")\n",
        "        print()\n",
        "\n",
        "    # 3. 질의 기반 유사 문장 검색\n",
        "    print(\"=== 질의 기반 문장 검색 ===\")\n",
        "    query = \"날씨에 관한 이야기\"\n",
        "    print(f\"질의: '{query}'\")\n",
        "\n",
        "    results = embedding_system.find_most_similar(query, korean_sentences, top_k=3)\n",
        "\n",
        "    print(f\"\\n'{query}'와 가장 유사한 문장들:\")\n",
        "    for result in results:\n",
        "        print(f\"{result['rank']}위. {result['sentence']} (유사도: {result['similarity']:.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GzUSU8FhH0pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 모델 성능 비교"
      ],
      "metadata": {
        "id": "7-iK2Ht7IgZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_korean_models():\n",
        "    \"\"\"여러 한국어 모델 성능 비교\"\"\"\n",
        "    print(\"=== 한국어 모델 성능 비교 ===\")\n",
        "\n",
        "    models = [\n",
        "        'jhgan/ko-sroberta-multitask',\n",
        "        'BM-K/KoSimCSE-roberta-multitask',\n",
        "        'klue/roberta-base'\n",
        "    ]\n",
        "\n",
        "    test_sentences = [\n",
        "        \"한국어 자연어처리는 어려운 분야입니다.\",\n",
        "        \"자연어처리 기술은 복잡한 영역이에요.\",\n",
        "        \"오늘은 비가 와서 우울해요.\",\n",
        "        \"날씨가 흐려서 기분이 안 좋네요.\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name in models:\n",
        "        try:\n",
        "            print(f\"\\n{model_name} 테스트 중...\")\n",
        "            system = KoreanEmbeddingSystem(model_name)\n",
        "\n",
        "            # 유사도 계산\n",
        "            similarity_matrix = system.calculate_similarity(test_sentences)\n",
        "\n",
        "            # 관련 문장 쌍의 유사도 측정\n",
        "            related_similarity = (similarity_matrix[0][1] + similarity_matrix[2][3]) / 2\n",
        "            unrelated_similarity = (similarity_matrix[0][2] + similarity_matrix[1][3]) / 2\n",
        "\n",
        "            results[model_name] = {\n",
        "                'related_similarity': related_similarity,\n",
        "                'unrelated_similarity': unrelated_similarity,\n",
        "                'discrimination': related_similarity - unrelated_similarity\n",
        "            }\n",
        "\n",
        "            print(f\"관련 문장 유사도: {related_similarity:.4f}\")\n",
        "            print(f\"무관한 문장 유사도: {unrelated_similarity:.4f}\")\n",
        "            print(f\"구분력: {results[model_name]['discrimination']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "\n",
        "    # 결과 정리\n",
        "    print(\"\\n=== 모델 성능 요약 ===\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model.split('/')[-1]}: 구분력 {metrics['discrimination']:.4f}\")\n",
        "\n",
        "# 예제 실행\n",
        "compare_korean_models()"
      ],
      "metadata": {
        "id": "dW03Vu4JIc6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한국어 감정 분석 임베딩"
      ],
      "metadata": {
        "id": "Xk0oWjGgIvyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_korean_emotions():\n",
        "    \"\"\"한국어 감정 표현 분석\"\"\"\n",
        "    print(\"=== 한국어 감정 표현 분석 ===\")\n",
        "\n",
        "    system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    emotion_sentences = {\n",
        "        '기쁨': [\n",
        "            \"오늘 정말 행복한 하루였어요!\",\n",
        "            \"좋은 소식을 들어서 기분이 날아갈 것 같아요.\",\n",
        "            \"드디어 꿈꾸던 일이 이루어졌습니다!\"\n",
        "        ],\n",
        "        '슬픔': [\n",
        "            \"오늘 너무 우울하고 슬펐어요.\",\n",
        "            \"힘든 일이 있어서 마음이 아픕니다.\",\n",
        "            \"이별의 아픔이 너무 커요.\"\n",
        "        ],\n",
        "        '분노': [\n",
        "            \"정말 화가 나서 참을 수 없어요!\",\n",
        "            \"이런 일이 있을 수 있나요? 너무 억울해요.\",\n",
        "            \"분통이 터질 것 같습니다.\"\n",
        "        ],\n",
        "        '놀라움': [\n",
        "            \"이런 일이! 정말 놀랍네요.\",\n",
        "            \"깜짝 놀랐어요. 예상치 못한 일이에요.\",\n",
        "            \"믿을 수 없어요. 너무 신기합니다.\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 각 감정별 대표 임베딩 계산\n",
        "    emotion_embeddings = {}\n",
        "    all_sentences = []\n",
        "    emotion_labels = []\n",
        "\n",
        "    for emotion, sentences in emotion_sentences.items():\n",
        "        embeddings = system.encode_sentences(sentences)\n",
        "        emotion_embeddings[emotion] = np.mean(embeddings, axis=0)\n",
        "        all_sentences.extend(sentences)\n",
        "        emotion_labels.extend([emotion] * len(sentences))\n",
        "\n",
        "    # 새로운 문장의 감정 예측\n",
        "    test_sentences = [\n",
        "        \"시험에 합격해서 너무 기뻐요!\",\n",
        "        \"친구와 싸워서 마음이 아파요.\",\n",
        "        \"갑자기 비가 와서 당황스러워요.\"\n",
        "    ]\n",
        "\n",
        "    print(\"감정 예측 결과:\")\n",
        "    for test_sentence in test_sentences:\n",
        "        test_embedding = system.encode_sentences([test_sentence])[0]\n",
        "\n",
        "        similarities = {}\n",
        "        for emotion, emotion_emb in emotion_embeddings.items():\n",
        "            similarity = cosine_similarity([test_embedding], [emotion_emb])[0][0]\n",
        "            similarities[emotion] = similarity\n",
        "\n",
        "        predicted_emotion = max(similarities, key=similarities.get)\n",
        "        confidence = similarities[predicted_emotion]\n",
        "\n",
        "        print(f\"\\n문장: '{test_sentence}'\")\n",
        "        print(f\"예측 감정: {predicted_emotion} (신뢰도: {confidence:.4f})\")\n",
        "\n",
        "        # 모든 감정별 점수\n",
        "        print(\"감정별 유사도:\")\n",
        "        for emotion, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {emotion}: {score:.4f}\")\n",
        "\n",
        "# 예제 실행\n",
        "analyze_korean_emotions()"
      ],
      "metadata": {
        "id": "E0PLA4jyI0G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 한국어 문서 클러스터링"
      ],
      "metadata": {
        "id": "UmQFdr_hI3_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_korean_documents():\n",
        "    \"\"\"한국어 문서 클러스터링\"\"\"\n",
        "    print(\"=== 한국어 문서 클러스터링 ===\")\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # 다양한 주제의 한국어 문서\n",
        "    documents = [\n",
        "        # 기술 관련\n",
        "        \"인공지능과 머신러닝 기술이 빠르게 발전하고 있습니다.\",\n",
        "        \"딥러닝 알고리즘은 복잡한 패턴을 학습할 수 있습니다.\",\n",
        "        \"자연어처리 기술로 텍스트를 분석할 수 있습니다.\",\n",
        "\n",
        "        # 요리 관련\n",
        "        \"김치찌개는 한국의 대표적인 음식입니다.\",\n",
        "        \"불고기를 만들 때는 양념이 중요해요.\",\n",
        "        \"비빔밥에는 다양한 나물이 들어갑니다.\",\n",
        "\n",
        "        # 여행 관련\n",
        "        \"제주도는 아름다운 자연경관을 자랑합니다.\",\n",
        "        \"부산의 해운대 바다는 정말 멋져요.\",\n",
        "        \"경주에는 많은 역사적 유적이 있습니다.\",\n",
        "\n",
        "        # 날씨 관련\n",
        "        \"오늘은 맑고 화창한 날씨입니다.\",\n",
        "        \"비가 와서 습도가 높아졌어요.\",\n",
        "        \"겨울이라 날씨가 매우 춥습니다.\"\n",
        "    ]\n",
        "\n",
        "    # 문서 임베딩 생성\n",
        "    embeddings = system.encode_sentences(documents)\n",
        "\n",
        "    # K-means 클러스터링\n",
        "    n_clusters = 4\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"{n_clusters}개 클러스터로 분류 결과:\")\n",
        "    for i in range(n_clusters):\n",
        "        print(f\"\\n클러스터 {i+1}:\")\n",
        "        cluster_docs = [documents[j] for j, label in enumerate(cluster_labels) if label == i]\n",
        "        for doc in cluster_docs:\n",
        "            print(f\"  - {doc}\")\n",
        "\n",
        "    # 시각화 (PCA로 2D 축소)\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.rcParams['font.family'] = 'Malgun Gothic'  # 한글 폰트\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "        for i in range(n_clusters):\n",
        "            mask = cluster_labels == i\n",
        "            plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
        "                       c=colors[i], label=f'클러스터 {i+1}', alpha=0.7)\n",
        "\n",
        "        plt.title('한국어 문서 클러스터링 결과')\n",
        "        plt.xlabel(f'PC1 (설명변량: {pca.explained_variance_ratio_[0]:.2%})')\n",
        "        plt.ylabel(f'PC2 (설명변량: {pca.explained_variance_ratio_[1]:.2%})')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"시각화 오류: {e}\")\n",
        "        print(\"matplotlib 한글 폰트 설정이 필요할 수 있습니다.\")\n",
        "\n",
        "# 예제 실행\n",
        "cluster_korean_documents()"
      ],
      "metadata": {
        "id": "l5PMlTCoI6B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 멀티모달 임베딩**"
      ],
      "metadata": {
        "id": "t8w-MnTSJ8h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    AutoProcessor, AutoModel,\n",
        "    BlipProcessor, BlipModel,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MultimodalEmbeddingSystem:\n",
        "    def __init__(self, model_type='clip'):\n",
        "        \"\"\"\n",
        "        멀티모달 임베딩 시스템 초기화\n",
        "\n",
        "        지원 모델들:\n",
        "        - 'clip': OpenAI CLIP (기본)\n",
        "        - 'korean-clip': 한국어 CLIP\n",
        "        - 'siglip': Google SigLIP\n",
        "        - 'blip': BLIP (Salesforce)\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        print(f\"멀티모달 모델 로딩 중: {model_type}\")\n",
        "        self._load_model()\n",
        "        print(f\"모델 로딩 완료! 디바이스: {self.device}\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"모델별 로딩\"\"\"\n",
        "        if self.model_type == 'clip':\n",
        "            model_name = \"openai/clip-vit-base-patch32\"\n",
        "            self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "            self.model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'korean-clip':\n",
        "            # 한국어 CLIP 모델 (예시)\n",
        "            model_name = \"Bingsu/clip-vit-base-patch32-ko\"\n",
        "            self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "            self.model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'siglip':\n",
        "            model_name = \"google/siglip-base-patch16-224\"\n",
        "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'blip':\n",
        "            model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "            self.processor = BlipProcessor.from_pretrained(model_name)\n",
        "            self.model = BlipModel.from_pretrained(model_name)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def encode_text(self, texts):\n",
        "        \"\"\"텍스트를 벡터로 인코딩\"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.model_type in ['clip', 'korean-clip']:\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                text_features = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(text_features, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'siglip':\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(outputs, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'blip':\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                text_features = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(text_features, p=2, dim=1)\n",
        "\n",
        "        return text_features.cpu().numpy()\n",
        "\n",
        "    def encode_image(self, images):\n",
        "        \"\"\"이미지를 벡터로 인코딩\"\"\"\n",
        "        if not isinstance(images, list):\n",
        "            images = [images]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.model_type in ['clip', 'korean-clip']:\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                image_features = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(image_features, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'siglip':\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(outputs, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'blip':\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                image_features = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(image_features, p=2, dim=1)\n",
        "\n",
        "        return image_features.cpu().numpy()\n",
        "\n",
        "    def calculate_similarity(self, embeddings1, embeddings2=None):\n",
        "        \"\"\"임베딩 간 코사인 유사도 계산\"\"\"\n",
        "        if embeddings2 is None:\n",
        "            return cosine_similarity(embeddings1)\n",
        "        else:\n",
        "            return cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "    def text_image_similarity(self, texts, images):\n",
        "        \"\"\"텍스트와 이미지 간 유사도 계산\"\"\"\n",
        "        text_embeddings = self.encode_text(texts)\n",
        "        image_embeddings = self.encode_image(images)\n",
        "\n",
        "        similarity_matrix = self.calculate_similarity(text_embeddings, image_embeddings)\n",
        "        return similarity_matrix\n",
        "\n",
        "    def cross_modal_search(self, query, candidates, query_type='text', candidate_type='image', top_k=5):\n",
        "        \"\"\"교차 모달 검색\"\"\"\n",
        "        if query_type == 'text':\n",
        "            query_embedding = self.encode_text([query])\n",
        "        else:\n",
        "            query_embedding = self.encode_image([query])\n",
        "\n",
        "        if candidate_type == 'text':\n",
        "            candidate_embeddings = self.encode_text(candidates)\n",
        "        else:\n",
        "            candidate_embeddings = self.encode_image(candidates)\n",
        "\n",
        "        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'candidate': candidates[idx] if candidate_type == 'text' else f\"Image_{idx}\",\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# 이미지 다운로드 유틸리티\n",
        "def download_image(url):\n",
        "    \"\"\"URL에서 이미지 다운로드\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"이미지 다운로드 실패: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_sample_images():\n",
        "    \"\"\"샘플 이미지 URL 목록\"\"\"\n",
        "    return {\n",
        "        'cat': 'https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400',\n",
        "        'dog': 'https://images.unsplash.com/photo-1552053831-71594a27632d?w=400',\n",
        "        'car': 'https://images.unsplash.com/photo-1549924231-f129b911e442?w=400',\n",
        "        'flower': 'https://images.unsplash.com/photo-1441974231531-c6227db76b6e?w=400',\n",
        "        'mountain': 'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400'\n",
        "    }\n",
        "\n",
        "# 메인 실행 함수\n",
        "def main():\n",
        "    print(\"=== 멀티모달 통합 임베딩 시스템 ===\\n\")\n",
        "\n",
        "    # 시스템 초기화\n",
        "    embedding_system = MultimodalEmbeddingSystem('clip')\n",
        "\n",
        "    # 샘플 텍스트\n",
        "    sample_texts = [\n",
        "        \"a cute cat sitting on a chair\",\n",
        "        \"a happy dog running in the park\",\n",
        "        \"a red sports car on the road\",\n",
        "        \"beautiful flowers in the garden\",\n",
        "        \"snow-covered mountain peaks\"\n",
        "    ]\n",
        "\n",
        "    print(\"샘플 텍스트:\")\n",
        "    for i, text in enumerate(sample_texts, 1):\n",
        "        print(f\"{i}. {text}\")\n",
        "\n",
        "    # 샘플 이미지 다운로드\n",
        "    print(\"\\n샘플 이미지 다운로드 중...\")\n",
        "    image_urls = create_sample_images()\n",
        "    sample_images = []\n",
        "    image_names = []\n",
        "\n",
        "    for name, url in image_urls.items():\n",
        "        image = download_image(url)\n",
        "        if image:\n",
        "            sample_images.append(image)\n",
        "            image_names.append(name)\n",
        "            print(f\"✓ {name} 이미지 로드 완료\")\n",
        "\n",
        "    if not sample_images:\n",
        "        print(\"이미지 로드 실패. 로컬 이미지를 사용하거나 URL을 확인해주세요.\")\n",
        "        return\n",
        "\n",
        "    # 1. 텍스트 임베딩 생성\n",
        "    print(f\"\\n=== 텍스트 임베딩 생성 ===\")\n",
        "    text_embeddings = embedding_system.encode_text(sample_texts)\n",
        "    print(f\"텍스트 임베딩 shape: {text_embeddings.shape}\")\n",
        "\n",
        "    # 2. 이미지 임베딩 생성\n",
        "    print(f\"\\n=== 이미지 임베딩 생성 ===\")\n",
        "    image_embeddings = embedding_system.encode_image(sample_images)\n",
        "    print(f\"이미지 임베딩 shape: {image_embeddings.shape}\")\n",
        "\n",
        "    # 3. 교차 모달 유사도 계산\n",
        "    print(f\"\\n=== 텍스트-이미지 교차 모달 유사도 ===\")\n",
        "    cross_modal_similarity = embedding_system.text_image_similarity(sample_texts, sample_images)\n",
        "\n",
        "    print(\"텍스트 → 이미지 유사도 매트릭스:\")\n",
        "    print(\"텍스트 \\\\ 이미지\", end=\"\")\n",
        "    for name in image_names:\n",
        "        print(f\"\\t{name[:8]}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "    for i, text in enumerate(sample_texts):\n",
        "        print(f\"{text[:20]}...\", end=\"\")\n",
        "        for j in range(len(sample_images)):\n",
        "            print(f\"\\t{cross_modal_similarity[i][j]:.3f}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    # 4. 텍스트로 이미지 검색\n",
        "    print(f\"\\n=== 텍스트 쿼리로 이미지 검색 ===\")\n",
        "    text_query = \"cute animal pet\"\n",
        "\n",
        "    results = embedding_system.cross_modal_search(\n",
        "        query=text_query,\n",
        "        candidates=image_names,  # 실제로는 이미지 객체를 사용\n",
        "        query_type='text',\n",
        "        candidate_type='text',  # 결과 표시용으로 이름 사용\n",
        "        top_k=3\n",
        "    )\n",
        "\n",
        "    print(f\"검색어: '{text_query}'\")\n",
        "    for result in results:\n",
        "        print(f\"{result['rank']}. {result['candidate']} (유사도: {result['similarity']:.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lMJblWX3J_2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **사진에 맞는 텍스트 선택하기**\n",
        "\n",
        "- image : 강아지 사진\n",
        "- texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]"
      ],
      "metadata": {
        "id": "-x7CBJU3MS-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "# 1. 모델 로드\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# 2. 샘플 데이터\n",
        "url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\"  # 강아지 사진\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n",
        "\n",
        "# 3. 전처리\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# 4. 예측\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image   # 이미지 vs 텍스트 유사도\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "print(\"텍스트 후보:\", texts)\n",
        "print(\"예측 확률:\", probs.detach().numpy())\n"
      ],
      "metadata": {
        "id": "KnDr3HjMMTT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class CLIPImageTextMatcher:\n",
        "    def __init__(self):\n",
        "        \"\"\"CLIP 모델 초기화\"\"\"\n",
        "        print(\"CLIP 모델 로딩 중...\")\n",
        "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        print(f\"모델 로딩 완료! 디바이스: {self.device}\")\n",
        "\n",
        "    def predict_image_text_match(self, image, texts, show_all_probs=True):\n",
        "        \"\"\"이미지와 텍스트 간 매칭 예측\"\"\"\n",
        "        # 전처리\n",
        "        inputs = self.processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # 예측 수행\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "            probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "        # CPU로 이동 후 numpy 변환\n",
        "        probs_np = probs.cpu().numpy()[0]\n",
        "\n",
        "        # 결과 분석\n",
        "        max_prob_idx = np.argmax(probs_np)\n",
        "        max_prob_text = texts[max_prob_idx]\n",
        "        max_prob_value = probs_np[max_prob_idx]\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"🎯 CLIP 이미지-텍스트 매칭 결과\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if show_all_probs:\n",
        "            print(\"\\n📊 모든 텍스트 후보별 예측 확률:\")\n",
        "            for i, (text, prob) in enumerate(zip(texts, probs_np)):\n",
        "                status = \"✅ 최고 확률\" if i == max_prob_idx else \"  \"\n",
        "                print(f\"{status} {i+1}. '{text}': {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "        print(f\"\\n🏆 가장 높은 확률의 매칭:\")\n",
        "        print(f\"   텍스트: '{max_prob_text}'\")\n",
        "        print(f\"   확률: {max_prob_value:.4f} ({max_prob_value*100:.2f}%)\")\n",
        "        print(f\"   신뢰도: {'높음' if max_prob_value > 0.7 else '보통' if max_prob_value > 0.4 else '낮음'}\")\n",
        "\n",
        "        return {\n",
        "            'best_match': max_prob_text,\n",
        "            'best_probability': max_prob_value,\n",
        "            'all_probabilities': list(zip(texts, probs_np)),\n",
        "            'confidence_level': 'high' if max_prob_value > 0.7 else 'medium' if max_prob_value > 0.4 else 'low'\n",
        "        }\n",
        "\n",
        "def download_and_display_image(url):\n",
        "    \"\"\"이미지 다운로드 및 정보 표시\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        image = Image.open(response.raw).convert('RGB')\n",
        "        print(f\"✅ 이미지 다운로드 성공\")\n",
        "        print(f\"   URL: {url}\")\n",
        "        print(f\"   크기: {image.size}\")\n",
        "        print(f\"   모드: {image.mode}\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 이미지 다운로드 실패: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"메인 실행 함수\"\"\"\n",
        "    print(\"🚀 CLIP 이미지-텍스트 매칭 시스템 시작\")\n",
        "\n",
        "    # 시스템 초기화\n",
        "    matcher = CLIPImageTextMatcher()\n",
        "\n",
        "    # 샘플 이미지 URL들\n",
        "    sample_images = {\n",
        "        'dog': \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",\n",
        "        'cat': \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",\n",
        "        'car': \"https://images.unsplash.com/photo-1549924231-f129b911e442?w=400\",\n",
        "        'flower': \"https://images.unsplash.com/photo-1441974231531-c6227db76b6e?w=400\"\n",
        "    }\n",
        "\n",
        "    # 텍스트 후보들\n",
        "    text_candidates = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a flower\"]\n",
        "\n",
        "    print(f\"\\n📝 텍스트 후보들:\")\n",
        "    for i, text in enumerate(text_candidates, 1):\n",
        "        print(f\"   {i}. {text}\")\n",
        "\n",
        "    # 각 이미지에 대해 예측 수행\n",
        "    results_summary = []\n",
        "\n",
        "    for img_name, img_url in sample_images.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"🖼️  {img_name.upper()} 이미지 분석\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # 이미지 다운로드\n",
        "        image = download_and_display_image(img_url)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        # 예측 수행\n",
        "        result = matcher.predict_image_text_match(image, text_candidates)\n",
        "\n",
        "        # 결과 저장\n",
        "        results_summary.append({\n",
        "            'image_name': img_name,\n",
        "            'predicted_text': result['best_match'],\n",
        "            'probability': result['best_probability'],\n",
        "            'confidence': result['confidence_level']\n",
        "        })\n",
        "\n",
        "    # 전체 결과 요약\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"📋 전체 결과 요약\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for result in results_summary:\n",
        "        confidence_emoji = \"🔥\" if result['confidence'] == 'high' else \"👍\" if result['confidence'] == 'medium' else \"🤔\"\n",
        "        print(f\"{confidence_emoji} {result['image_name'].upper():8} → '{result['predicted_text']}' \"\n",
        "              f\"({result['probability']:.4f}, {result['confidence']})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mNngrcMaP-qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **다양한 언어로 테스트**"
      ],
      "metadata": {
        "id": "WYwNMN30RL4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multilingual_test():\n",
        "    \"\"\"다국어 텍스트로 테스트\"\"\"\n",
        "    print(\"\\n🌍 다국어 텍스트 매칭 테스트\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    matcher = CLIPImageTextMatcher()\n",
        "\n",
        "    # 강아지 이미지\n",
        "    dog_url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\"\n",
        "    image = download_and_display_image(dog_url)\n",
        "\n",
        "    if image:\n",
        "        # 다국어 텍스트 후보\n",
        "        multilingual_texts = [\n",
        "            \"a photo of a dog\",      # 영어\n",
        "            \"un chien\",              # 프랑스어\n",
        "            \"ein Hund\",              # 독일어\n",
        "            \"un perro\",              # 스페인어\n",
        "            \"개 사진\",                # 한국어 (제한적 지원)\n",
        "            \"a cat sleeping\",         # 다른 동물\n",
        "            \"a beautiful landscape\"   # 완전히 다른 개념\n",
        "        ]\n",
        "\n",
        "        result = matcher.predict_image_text_match(image, multilingual_texts)\n",
        "\n",
        "        print(f\"\\n💡 분석: CLIP은 주로 영어 데이터로 학습되어 영어 텍스트에서 가장 좋은 성능을 보입니다.\")\n",
        "\n",
        "multilingual_test()"
      ],
      "metadata": {
        "id": "Q7XJ3-A0RPvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7turu6ayn1B5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mk2jspOcd3r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 인코딩과 디코딩을 이용한 기계번역"
      ],
      "metadata": {
        "id": "tk0AETJAnjPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 텍스트 인코딩과 디코딩을 이용한 기계번역**\n",
        "- RNN / Seq2Seq / Attention / Transformer"
      ],
      "metadata": {
        "id": "xG2OZPWun9-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "id": "MBD3Dqvpn8di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------\n",
        "# Utilities\n",
        "# ------------------------------\n",
        "SRC_SENT = \"나는 점심 식사로 파스타를 먹을 예정입니다.\"\n",
        "EXPECTED = \"I am going to eat pasta for lunch.\"\n",
        "\n",
        "\n",
        "def pretty(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = s[0:1].upper() + s[1:] if s else s\n",
        "    if not s.endswith(\".\"):\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "def timer(func, *args, **kwargs):\n",
        "    t0 = time.perf_counter()\n",
        "    out = func(*args, **kwargs)\n",
        "    t1 = time.perf_counter()\n",
        "    return out, (t1 - t0)\n",
        "\n",
        "# ------------------------------\n",
        "# 1) RNN-style (naive L2R)\n",
        "# ------------------------------\n",
        "def tokenize_ko(text: str):\n",
        "    # Basic whitespace + punctuation split, keep simple for demo\n",
        "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
        "    toks = [t for t in text.split() if t.strip()]\n",
        "    return toks\n",
        "\n",
        "# Minimal dictionary for demo (phrase & token level)\n",
        "PHRASES = {\n",
        "    \"먹을 예정입니다\": \"am going to eat\",\n",
        "    \"점심 식사로\": \"for lunch\",\n",
        "}\n",
        "\n",
        "LEXICON = {\n",
        "    \"나는\": \"i\",\n",
        "    \"점심\": \"lunch\",\n",
        "    \"식사로\": \"for lunch\",\n",
        "    \"파스타를\": \"pasta\",\n",
        "    \"먹을\": \"eat\",\n",
        "    \"예정입니다\": \"going to\",\n",
        "}\n",
        "\n",
        "def translate_rnn_rule(text: str) -> str:\n",
        "    # Left-to-right mapping without reordering (shows RNN baseline weakness on SOV→SVO)\n",
        "    # Apply phrase map first if exact phrase appears\n",
        "    out = text\n",
        "    for k, v in PHRASES.items():\n",
        "        out = out.replace(k, v)\n",
        "\n",
        "    toks = tokenize_ko(out)\n",
        "    eng = []\n",
        "    for tok in toks:\n",
        "        if tok in LEXICON:\n",
        "            eng.append(LEXICON[tok])\n",
        "        elif tok in PHRASES:\n",
        "            eng.append(PHRASES[tok])\n",
        "        elif tok == \"파스타\":\n",
        "            eng.append(\"pasta\")\n",
        "        else:\n",
        "            # leave as-is (unknown token)\n",
        "            eng.append(tok)\n",
        "\n",
        "    # naive L2R join\n",
        "    return pretty(\" \".join(eng))\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Seq2Seq-style (encoder-decoder with reordering rules)\n",
        "# ------------------------------\n",
        "def translate_seq2seq_rule(text: str) -> str:\n",
        "    # Simulate encoder-decoder that learns typical Korean SOV → English SVO reordering\n",
        "    # Pattern: \"나는 X를 먹을 예정입니다\" -> \"i am going to eat X\"\n",
        "    # and attach \"for lunch\" if phrase occurs\n",
        "    s = text\n",
        "\n",
        "    # detect object phrase like \"파스타를\"\n",
        "    obj = None\n",
        "    m = re.search(r\"(.*?)(파스타)를\", s)\n",
        "    if m:\n",
        "        obj = \"pasta\"\n",
        "\n",
        "    # lunch phrase\n",
        "    lunch = \"for lunch\" if \"점심\" in s else None\n",
        "\n",
        "    base = \"i am going to eat\"\n",
        "    if obj:\n",
        "        out = f\"{base} {obj}\"\n",
        "    else:\n",
        "        out = base\n",
        "\n",
        "    if lunch:\n",
        "        out = f\"{out} {lunch}\"\n",
        "    return pretty(out)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Attention-style (alignment-guided + mock attention matrix)\n",
        "# ------------------------------\n",
        "def translate_attention_rule(text: str, return_attention=False):\n",
        "    # Tokenize Korean source\n",
        "    src_tokens = [\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\"]\n",
        "    # Target template tokens (SVO + adjunct)\n",
        "    tgt_tokens = [\"i\", \"am\", \"going\", \"to\", \"eat\", \"pasta\", \"for\", \"lunch\"]\n",
        "\n",
        "    # Create a mock attention alignment matrix (len(tgt) x len(src))\n",
        "    A = np.zeros((len(tgt_tokens), len(src_tokens)), dtype=float)\n",
        "\n",
        "    # Rough alignments\n",
        "    align = {\n",
        "        0: [0],              # i  <- 나는\n",
        "        1: [5],              # am <- 예정입니다\n",
        "        2: [5],              # going\n",
        "        3: [5],              # to\n",
        "        4: [4],              # eat <- 먹을\n",
        "        5: [3],              # pasta <- 파스타를\n",
        "        6: [1,2],            # for <- 점심 식사로\n",
        "        7: [1,2],            # lunch <- 점심 식사로\n",
        "    }\n",
        "    for t_idx, s_list in align.items():\n",
        "        for s_idx in s_list:\n",
        "            A[t_idx, s_idx] = 1.0 / len(s_list)\n",
        "\n",
        "    out = \"i am going to eat pasta for lunch\"\n",
        "    out = pretty(out)\n",
        "\n",
        "    if return_attention:\n",
        "        return out, A, src_tokens, tgt_tokens\n",
        "    return out\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Transformer (real pretrained model if available)\n",
        "# ------------------------------\n",
        "def translate_transformer_hf(text: str) -> str:\n",
        "    try:\n",
        "        from transformers import MarianTokenizer, MarianMTModel\n",
        "        import torch\n",
        "\n",
        "        model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "        batch = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**batch, max_length=64, num_beams=5)\n",
        "        out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "        return pretty(out)\n",
        "    except Exception as e:\n",
        "        # Fallback: use the seq2seq rule-based output\n",
        "        return translate_seq2seq_rule(text)\n",
        "\n",
        "# ------------------------------\n",
        "# Simple scorer: token precision vs expected\n",
        "# ------------------------------\n",
        "def token_precision(pred: str, ref: str) -> float:\n",
        "    p = re.findall(r\"[a-zA-Z']+\", pred.lower())\n",
        "    r = re.findall(r\"[a-zA-Z']+\", ref.lower())\n",
        "    if not p or not r:\n",
        "        return 0.0\n",
        "    hit = sum(1 for w in p if w in r)\n",
        "    return hit / len(p)\n",
        "\n",
        "# ------------------------------\n",
        "# Main comparison\n",
        "# ------------------------------\n",
        "def main():\n",
        "    print(\"Source (KO):\", SRC_SENT)\n",
        "    print(\"Expected (EN):\", EXPECTED)\n",
        "    print(\"=\"*72)\n",
        "\n",
        "    # 1) RNN (rule)\n",
        "    rnn_out, t_rnn = timer(translate_rnn_rule, SRC_SENT)\n",
        "    # 2) Seq2Seq (rule)\n",
        "    s2s_out, t_s2s = timer(translate_seq2seq_rule, SRC_SENT)\n",
        "    # 3) Attention (rule + mock attention)\n",
        "    att_out, t_att = timer(translate_attention_rule, SRC_SENT)\n",
        "    # 4) Transformer (real model if available)\n",
        "    trf_out, t_trf = timer(translate_transformer_hf, SRC_SENT)\n",
        "\n",
        "    rows = [\n",
        "        (\"RNN (rule)\", rnn_out, t_rnn, token_precision(rnn_out, EXPECTED)),\n",
        "        (\"Seq2Seq (rule)\", s2s_out, t_s2s, token_precision(s2s_out, EXPECTED)),\n",
        "        (\"Attention (rule)\", att_out, t_att, token_precision(att_out, EXPECTED)),\n",
        "        (\"Transformer (HF)\", trf_out, t_trf, token_precision(trf_out, EXPECTED)),\n",
        "    ]\n",
        "\n",
        "    print()\n",
        "    print(f\"{'Model':<18} | {'Output':<45} | Time(s) | TokenPrec\")\n",
        "    print(\"-\"*90)\n",
        "    for name, out, t, prec in rows:\n",
        "        print(f\"{name:<18} | {out:<45} | {t:>6.3f} | {prec:>0.3f}\")\n",
        "\n",
        "    # Optional: visualize mock attention with matplotlib if available\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        att_out2, A, src, tgt = translate_attention_rule(SRC_SENT, return_attention=True)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.imshow(A, aspect='auto')\n",
        "        plt.yticks(range(len(tgt)), tgt)\n",
        "        plt.xticks(range(len(src)), src, rotation=45)\n",
        "        plt.title(\"Mock Attention Alignment (tgt x src)\")\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "9bWAaMQqn9Wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}