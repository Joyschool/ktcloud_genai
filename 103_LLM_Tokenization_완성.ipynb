{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPoNVb8s+JmE635QYwdvZDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/ktcloud_genai/blob/main/103_LLM_Tokenization_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ì°¸ê³ ] Tokenization"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [ì‹¤ìŠµ] ì˜ˆì œ 1: í† í¬ë‚˜ì´ì œì´ì…˜ ê³¼ì • ë‹¨ê³„ë³„ í™•ì¸"
      ],
      "metadata": {
        "id": "sbZjTUHSgJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²½ê³  ë©”ì‹œì§€ë§Œ ìˆ¨ê¸°ê¸°\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# 1. ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 2. Hugging Face ì§„í–‰ë¥  í‘œì‹œ ë„ê¸°\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
      ],
      "metadata": {
        "id": "HK9BCOKOg6SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "input_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€\"\n",
        "print(f\"âœ… ì›ë³¸ í…ìŠ¤íŠ¸: '{input_text}'\")\n",
        "print()\n",
        "\n",
        "# 1ë‹¨ê³„: í† í°ìœ¼ë¡œ ë¶„í•  (ë¬¸ìì—´ í˜•íƒœ)\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(f\"1ï¸âƒ£ í† í° ë¶„í•  ê²°ê³¼: {tokens}\")\n",
        "print()\n",
        "\n",
        "# 2ë‹¨ê³„: ê° í† í°ì˜ ID í™•ì¸\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"2ï¸âƒ£ í† í° IDë“¤: {token_ids}\")\n",
        "print()\n",
        "\n",
        "# 3ë‹¨ê³„: encode í•¨ìˆ˜ë¡œ í•œë²ˆì— ì²˜ë¦¬\n",
        "input_ids_list = tokenizer.encode(input_text)\n",
        "print(f\"3ï¸âƒ£ encode ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸): {input_ids_list}\")\n",
        "print()\n",
        "\n",
        "# 4ë‹¨ê³„: í…ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"4ï¸âƒ£ í…ì„œ í˜•íƒœë¡œ ë³€í™˜\")\n",
        "print(f\"â­¢ input_ids (í…ì„œ): {input_ids}\")\n",
        "print(f\"â­¢ input_ids í˜•íƒœ: {input_ids.shape}\")\n",
        "print(f\"â­¢ input_ids íƒ€ì…: {type(input_ids)}\")\n",
        "print()\n",
        "\n",
        "# ì—­ë³€í™˜: IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ\n",
        "decoded_text = tokenizer.decode(input_ids[0])\n",
        "print(f\"âœ… ì—­ë³€í™˜ ê²°ê³¼: '{decoded_text}'\")\n"
      ],
      "metadata": {
        "id": "Gk6peed0gYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [ì‹¤ìŠµ] ì˜ˆì œ 2: ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ"
      ],
      "metadata": {
        "id": "NPnBbz4XgSHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì˜ˆì œ\n",
        "texts = [\n",
        "    \"ì•ˆë…•í•˜ì„¸ìš”\",\n",
        "    \"Hello world\",\n",
        "    \"í”„ë¡œê·¸ë˜ë°\",\n",
        "    \"AIëŠ” ë¯¸ë˜ë‹¤\",\n",
        "    \"123456\",\n",
        "    \"hello@email.com\"\n",
        "]\n",
        "\n",
        "print(\"=== ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ ê²°ê³¼ ===\")\n",
        "for text in texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    print(f\"í…ìŠ¤íŠ¸: '{text}'\")\n",
        "    print(f\"í† í°: {tokens}\")\n",
        "    print(f\"input_ids: {input_ids.tolist()}\")\n",
        "    print(f\"í† í° ê°œìˆ˜: {len(tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "1MBOshvUkB_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [ì‹¤ìŠµ] ì˜ˆì œ 3: í† í° IDì˜ ì‚¬ìš© ê³¼ì •"
      ],
      "metadata": {
        "id": "eU3ZzrJImaQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "input_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€\"\n",
        "print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: '{input_text}'\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì œì´ì…˜\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"input_ids: {input_ids}\")\n",
        "print(f\"ê° IDê°€ ë‚˜íƒ€ë‚´ëŠ” í† í°:\")\n",
        "\n",
        "# ê° IDê°€ ë¬´ìŠ¨ í† í°ì¸ì§€ í™•ì¸\n",
        "for i, token_id in enumerate(input_ids[0]):\n",
        "    token = tokenizer.decode([token_id])\n",
        "    print(f\"  ìœ„ì¹˜ {i}: ID {token_id.item()} â†’ '{token}'\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹¤ìŒ í† í° í™•ë¥  ê³„ì‚°\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    # logits: [ë°°ì¹˜_í¬ê¸°, ì‹œí€€ìŠ¤_ê¸¸ì´, ì–´íœ˜_í¬ê¸°]\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(f\"ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {logits.shape}\")\n",
        "print(f\"ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ í™•ë¥  ë¶„í¬ í¬ê¸°: {logits[0, -1, :].shape}\")\n",
        "\n",
        "# ë‹¤ìŒ í† í°ìœ¼ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìƒìœ„ 5ê°œ í™•ì¸\n",
        "last_token_logits = logits[0, -1, :]\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "top_5_prob, top_5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\në‹¤ìŒ í† í° ì˜ˆì¸¡ ìƒìœ„ 5ê°œ:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_5_prob, top_5_indices)):\n",
        "    token = tokenizer.decode([idx])\n",
        "    print(f\"{i+1}. '{token}' (í™•ë¥ : {prob:.4f})\")"
      ],
      "metadata": {
        "id": "S-nY1StkmiJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [ì£¼ì˜!] ì´ìƒí•œ í† í°ë“¤ì˜ ì •ì²´\n",
        "ì¶œë ¥ ê²°ê³¼ì—ì„œ GPT2ëŠ” í•œê¸€ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ë¶„í•´ ë•Œë¬¸ì— í† í°ì˜ ì´ìƒí•˜ê²Œ ë³´ì—¬ì§ˆ ìˆ˜ ìˆë‹¤**â­¢ì˜ë¯¸ ì†ì‹¤ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\n",
        "- ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•´í•´ì•¼ ì •í™•í•¨\n",
        "    - âœ… ì›ë³¸ í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€'\n",
        "    - 1ï¸âƒ£ í† í° ë¶„í•  ê²°ê³¼: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢', 'Ä Ã¬', 'Å€', 'Â¬', 'Ã«', 'Â¯', 'Â¸', 'Ã¬', 'Å€', 'Äª', 'Ã«Ä­', 'Â¤', '.', 'Ä ', 'Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢']\n",
        "- ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾' ...] ì´ í† í°ë“¤ì˜ ì •ì²´\n",
        "\n",
        "|ì •ì²´|ì„¤ëª…|ë¬¸ì œì |\n",
        "|--- |--- |--- |\n",
        "|UTF-8 ë°”ì´íŠ¸ì˜ ì˜ëª»ëœ í•´ì„ |í•œê¸€ ë°”ì´íŠ¸ë¥¼ Latin-1ë¡œ ë””ì½”ë”©í•œ ê²°ê³¼ |ì˜ë¯¸ ì™„ì „ ì†ì‹¤ |\n",
        "|BPE ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„ |ì˜ì–´ ìœ„ì£¼ í•™ìŠµìœ¼ë¡œ í•œê¸€ íŒ¨í„´ ë¯¸í•™ìŠµ |ë¹„íš¨ìœ¨ì  í† í°í™” |\n",
        "|ì–´íœ˜ì§‘ ë¶€ì¡± |GPT-2 ì–´íœ˜ì§‘ì— í•œê¸€ í† í° ê±°ì˜ ì—†ìŒ |ì•Œ ìˆ˜ ì—†ëŠ” í† í°ìœ¼ë¡œ ì²˜ë¦¬ |"
      ],
      "metadata": {
        "id": "ub9T5C1Sjj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ì˜ˆì œ 1: ë¬¸ì œ ìƒí™© ì •í™•í•œ ë¶„ì„"
      ],
      "metadata": {
        "id": "qqqDtMxRkPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# í•œê¸€ í…ìŠ¤íŠ¸\n",
        "korean_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\"\n",
        "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: '{korean_text}'\")\n",
        "print()\n",
        "\n",
        "# í† í° ë¶„í•  ê²°ê³¼\n",
        "tokens = tokenizer.tokenize(korean_text)\n",
        "print(f\"í† í° ê°œìˆ˜: {len(tokens)}ê°œ\")\n",
        "print(f\"í† í°ë“¤: {tokens[:10]}... (ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ)\")\n",
        "print()\n",
        "\n",
        "# ì´ìƒí•œ ë¬¸ìë“¤ì˜ ì •ì²´ í™•ì¸\n",
        "print(\"=== ì´ìƒí•œ í† í°ë“¤ì˜ ì •ì²´ ===\")\n",
        "for i, token in enumerate(tokens[:5]):\n",
        "    # í† í°ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•´ë³´ê¸°\n",
        "    try:\n",
        "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
        "        print(f\"í† í° {i+1}: '{token}' â†’ ID: {token_id}\")\n",
        "    except:\n",
        "        print(f\"í† í° {i+1}: '{token}' â†’ ë³€í™˜ ë¶ˆê°€\")\n",
        "\n",
        "print()\n",
        "\n",
        "# UTF-8 ë°”ì´íŠ¸ ë¶„ì„\n",
        "print(\"=== UTF-8 ë°”ì´íŠ¸ ë ˆë²¨ ë¶„ì„ ===\")\n",
        "korean_bytes = korean_text.encode('utf-8')\n",
        "print(f\"í•œê¸€ í…ìŠ¤íŠ¸ì˜ UTF-8 ë°”ì´íŠ¸: {korean_bytes}\")\n",
        "print(f\"ë°”ì´íŠ¸ ê°œìˆ˜: {len(korean_bytes)}ê°œ\")\n",
        "\n",
        "# ê° ë°”ì´íŠ¸ë¥¼ ê°œë³„ ë¬¸ìë¡œ ë””ì½”ë”© ì‹œë„\n",
        "print(\"ë°”ì´íŠ¸ë³„ ë¶„ì„:\")\n",
        "for i, byte_val in enumerate(korean_bytes[:10]):\n",
        "    print(f\"ë°”ì´íŠ¸ {i+1}: {byte_val} (0x{byte_val:02x})\")"
      ],
      "metadata": {
        "id": "DjbN4OT2jktt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ì˜ˆì œ 2: ì˜ì–´ì™€ í•œê¸€ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ"
      ],
      "metadata": {
        "id": "MGMcqoTrkRSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ë¹„êµ í…ìŠ¤íŠ¸ë“¤\n",
        "texts = {\n",
        "    \"ì˜ì–´\": \"Programming is fun\",\n",
        "    \"í•œê¸€\": \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\",\n",
        "    \"ìˆ«ì\": \"12345\",\n",
        "    \"íŠ¹ìˆ˜ë¬¸ì\": \"Hello! @#$%\"\n",
        "}\n",
        "\n",
        "print(\"=== ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ ===\")\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_count = len(tokens)\n",
        "    char_count = len(text)\n",
        "\n",
        "    print(f\"\\n{lang}: '{text}'\")\n",
        "    print(f\"  ë¬¸ì ìˆ˜: {char_count}\")\n",
        "    print(f\"  í† í° ìˆ˜: {token_count}\")\n",
        "    print(f\"  íš¨ìœ¨ì„±: {token_count/char_count:.2f} (í† í°/ë¬¸ì)\")\n",
        "    print(f\"  í† í° ì˜ˆì‹œ: {tokens[:5]}...\")"
      ],
      "metadata": {
        "id": "jzaUiWdekRcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ì˜ˆì œ 3: ì˜¬ë°”ë¥¸ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©"
      ],
      "metadata": {
        "id": "h6wnNFbwkRxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ë‹¤êµ­ì–´ ì§€ì› í† í¬ë‚˜ì´ì €ë“¤ ë¹„êµ\n",
        "tokenizer_models = {\n",
        "    \"GPT-2 (ì˜ì–´ ì „ìš©)\": \"gpt2\",\n",
        "    \"mBERT (ë‹¤êµ­ì–´)\": \"bert-base-multilingual-cased\",\n",
        "    \"XLM-RoBERTa (ë‹¤êµ­ì–´)\": \"xlm-roberta-base\"\n",
        "}\n",
        "\n",
        "korean_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\"\n",
        "\n",
        "print(\"=== ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ë¹„êµ ===\")\n",
        "for model_name, model_id in tokenizer_models.items():\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        tokens = tokenizer.tokenize(korean_text)\n",
        "\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  í† í° ìˆ˜: {len(tokens)}\")\n",
        "        print(f\"  í† í°ë“¤: {tokens}\")\n",
        "\n",
        "        # ì—­ë³€í™˜ í™•ì¸\n",
        "        reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
        "        print(f\"  ì—­ë³€í™˜: '{reconstructed}'\")\n",
        "        print(f\"  ì›ë³¸ê³¼ ë™ì¼: {'âœ…' if reconstructed.strip() == korean_text else 'âŒ'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{model_name}: ë¡œë“œ ì‹¤íŒ¨ - {e}\")"
      ],
      "metadata": {
        "id": "tra_GeXrkR5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í•´ê²° ë°©ë²•ê³¼ ê¶Œì¥ì‚¬í•­\n"
      ],
      "metadata": {
        "id": "aHPCJqxdlQjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ì„ íƒ\n",
        "\n",
        "|ìš©ë„ |ê¶Œì¥ ëª¨ë¸ |ì´ìœ \n",
        "|--- |--- |--- |\n",
        "| í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„±| GPT-3.5/4, KoGPT| í•œê¸€ ë°ì´í„°ë¡œ í›ˆë ¨ë¨|\n",
        "| í•œê¸€ ì´í•´/ë¶„ë¥˜| KoBERT, KoELECTRA| í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸|\n",
        "| ë‹¤êµ­ì–´ ì²˜ë¦¬| mBERT, XLM-RoBERTa| ë‹¤êµ­ì–´ ë™ì‹œ ì§€ì›|\n",
        "| ì‹¤ìŠµ/í•™ìŠµìš©| ì˜ì–´ ì˜ˆì œ ì‚¬ìš©| GPT-2 ë³¸ë˜ ì„±ëŠ¥ í™•ì¸|"
      ],
      "metadata": {
        "id": "P0_mx4WXldNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•™ìŠµìš© ê°œì„ ëœ ì˜ˆì œ"
      ],
      "metadata": {
        "id": "Aj62TQvZl7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜¬ë°”ë¥¸ ì ‘ê·¼: ì˜ì–´ë¡œ ì‹¤ìŠµí•˜ê¸°\n",
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# GPT-2ê°€ ì˜ ì²˜ë¦¬í•˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸\n",
        "english_text = \"Programming is fun. Programming is\"\n",
        "print(f\"ì˜ì–´ í…ìŠ¤íŠ¸: '{english_text}'\")\n",
        "\n",
        "tokens = tokenizer.tokenize(english_text)\n",
        "print(f\"í† í°ë“¤: {tokens}\")\n",
        "print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
        "\n",
        "# ê° í† í°ì˜ ì˜ë¯¸ í™•ì¸\n",
        "print(\"\\ní† í°ë³„ ì˜ë¯¸:\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{i+1}. '{token}' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\")"
      ],
      "metadata": {
        "id": "0TeAXjxwnUQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IeP3BFVJf-00"
      }
    }
  ]
}