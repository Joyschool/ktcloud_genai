{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPoNVb8s+JmE635QYwdvZDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joyschool/ktcloud_genai/blob/main/103_LLM_Tokenization_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [참고] Tokenization"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 예제 1: 토크나이제이션 과정 단계별 확인"
      ],
      "metadata": {
        "id": "sbZjTUHSgJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경고 메시지만 숨기기\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# 1. 경고 메시지 비활성화\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 2. Hugging Face 진행률 표시 끄기\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
      ],
      "metadata": {
        "id": "HK9BCOKOg6SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 원본 텍스트\n",
        "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
        "print(f\"✅ 원본 텍스트: '{input_text}'\")\n",
        "print()\n",
        "\n",
        "# 1단계: 토큰으로 분할 (문자열 형태)\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(f\"1️⃣ 토큰 분할 결과: {tokens}\")\n",
        "print()\n",
        "\n",
        "# 2단계: 각 토큰의 ID 확인\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"2️⃣ 토큰 ID들: {token_ids}\")\n",
        "print()\n",
        "\n",
        "# 3단계: encode 함수로 한번에 처리\n",
        "input_ids_list = tokenizer.encode(input_text)\n",
        "print(f\"3️⃣ encode 결과 (리스트): {input_ids_list}\")\n",
        "print()\n",
        "\n",
        "# 4단계: 텐서 형태로 변환\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"4️⃣ 텐서 형태로 변환\")\n",
        "print(f\"⭢ input_ids (텐서): {input_ids}\")\n",
        "print(f\"⭢ input_ids 형태: {input_ids.shape}\")\n",
        "print(f\"⭢ input_ids 타입: {type(input_ids)}\")\n",
        "print()\n",
        "\n",
        "# 역변환: ID를 다시 텍스트로\n",
        "decoded_text = tokenizer.decode(input_ids[0])\n",
        "print(f\"✅ 역변환 결과: '{decoded_text}'\")\n"
      ],
      "metadata": {
        "id": "Gk6peed0gYNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 예제 2: 다양한 텍스트의 토크나이제이션 비교"
      ],
      "metadata": {
        "id": "NPnBbz4XgSHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 다양한 텍스트 예제\n",
        "texts = [\n",
        "    \"안녕하세요\",\n",
        "    \"Hello world\",\n",
        "    \"프로그래밍\",\n",
        "    \"AI는 미래다\",\n",
        "    \"123456\",\n",
        "    \"hello@email.com\"\n",
        "]\n",
        "\n",
        "print(\"=== 다양한 텍스트의 토크나이제이션 결과 ===\")\n",
        "for text in texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    print(f\"텍스트: '{text}'\")\n",
        "    print(f\"토큰: {tokens}\")\n",
        "    print(f\"input_ids: {input_ids.tolist()}\")\n",
        "    print(f\"토큰 개수: {len(tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "1MBOshvUkB_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [실습] 예제 3: 토큰 ID의 사용 과정"
      ],
      "metadata": {
        "id": "eU3ZzrJImaQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 원본 텍스트\n",
        "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
        "print(f\"입력 텍스트: '{input_text}'\")\n",
        "\n",
        "# 토크나이제이션\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"input_ids: {input_ids}\")\n",
        "print(f\"각 ID가 나타내는 토큰:\")\n",
        "\n",
        "# 각 ID가 무슨 토큰인지 확인\n",
        "for i, token_id in enumerate(input_ids[0]):\n",
        "    token = tokenizer.decode([token_id])\n",
        "    print(f\"  위치 {i}: ID {token_id.item()} → '{token}'\")\n",
        "\n",
        "print()\n",
        "\n",
        "# 모델에 입력하여 다음 토큰 확률 계산\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    # logits: [배치_크기, 시퀀스_길이, 어휘_크기]\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(f\"모델 출력 형태: {logits.shape}\")\n",
        "print(f\"마지막 토큰 위치의 확률 분포 크기: {logits[0, -1, :].shape}\")\n",
        "\n",
        "# 다음 토큰으로 가능성이 높은 상위 5개 확인\n",
        "last_token_logits = logits[0, -1, :]\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "top_5_prob, top_5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\n다음 토큰 예측 상위 5개:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_5_prob, top_5_indices)):\n",
        "    token = tokenizer.decode([idx])\n",
        "    print(f\"{i+1}. '{token}' (확률: {prob:.4f})\")"
      ],
      "metadata": {
        "id": "S-nY1StkmiJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [주의!] 이상한 토큰들의 정체\n",
        "출력 결과에서 GPT2는 한글을 바이트 단위로 분해 때문에 토큰의 이상하게 보여질 수 있다**⭢의미 손실 발생할 수 있다.\n",
        "- 의미 단위로 분해해야 정확함\n",
        "    - ✅ 원본 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
        "    - 1️⃣ 토큰 분할 결과: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ']\n",
        "- ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ' ...] 이 토큰들의 정체\n",
        "\n",
        "|정체|설명|문제점|\n",
        "|--- |--- |--- |\n",
        "|UTF-8 바이트의 잘못된 해석 |한글 바이트를 Latin-1로 디코딩한 결과 |의미 완전 손실 |\n",
        "|BPE 알고리즘의 한계 |영어 위주 학습으로 한글 패턴 미학습 |비효율적 토큰화 |\n",
        "|어휘집 부족 |GPT-2 어휘집에 한글 토큰 거의 없음 |알 수 없는 토큰으로 처리 |"
      ],
      "metadata": {
        "id": "ub9T5C1Sjj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 예제 1: 문제 상황 정확한 분석"
      ],
      "metadata": {
        "id": "qqqDtMxRkPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 한글 텍스트\n",
        "korean_text = \"프로그래밍은 재미있다\"\n",
        "print(f\"원본 텍스트: '{korean_text}'\")\n",
        "print()\n",
        "\n",
        "# 토큰 분할 결과\n",
        "tokens = tokenizer.tokenize(korean_text)\n",
        "print(f\"토큰 개수: {len(tokens)}개\")\n",
        "print(f\"토큰들: {tokens[:10]}... (처음 10개만 표시)\")\n",
        "print()\n",
        "\n",
        "# 이상한 문자들의 정체 확인\n",
        "print(\"=== 이상한 토큰들의 정체 ===\")\n",
        "for i, token in enumerate(tokens[:5]):\n",
        "    # 토큰을 바이트로 변환해보기\n",
        "    try:\n",
        "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
        "        print(f\"토큰 {i+1}: '{token}' → ID: {token_id}\")\n",
        "    except:\n",
        "        print(f\"토큰 {i+1}: '{token}' → 변환 불가\")\n",
        "\n",
        "print()\n",
        "\n",
        "# UTF-8 바이트 분석\n",
        "print(\"=== UTF-8 바이트 레벨 분석 ===\")\n",
        "korean_bytes = korean_text.encode('utf-8')\n",
        "print(f\"한글 텍스트의 UTF-8 바이트: {korean_bytes}\")\n",
        "print(f\"바이트 개수: {len(korean_bytes)}개\")\n",
        "\n",
        "# 각 바이트를 개별 문자로 디코딩 시도\n",
        "print(\"바이트별 분석:\")\n",
        "for i, byte_val in enumerate(korean_bytes[:10]):\n",
        "    print(f\"바이트 {i+1}: {byte_val} (0x{byte_val:02x})\")"
      ],
      "metadata": {
        "id": "DjbN4OT2jktt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 예제 2: 영어와 한글 토크나이제이션 비교"
      ],
      "metadata": {
        "id": "MGMcqoTrkRSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 비교 텍스트들\n",
        "texts = {\n",
        "    \"영어\": \"Programming is fun\",\n",
        "    \"한글\": \"프로그래밍은 재미있다\",\n",
        "    \"숫자\": \"12345\",\n",
        "    \"특수문자\": \"Hello! @#$%\"\n",
        "}\n",
        "\n",
        "print(\"=== 언어별 토크나이제이션 비교 ===\")\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_count = len(tokens)\n",
        "    char_count = len(text)\n",
        "\n",
        "    print(f\"\\n{lang}: '{text}'\")\n",
        "    print(f\"  문자 수: {char_count}\")\n",
        "    print(f\"  토큰 수: {token_count}\")\n",
        "    print(f\"  효율성: {token_count/char_count:.2f} (토큰/문자)\")\n",
        "    print(f\"  토큰 예시: {tokens[:5]}...\")"
      ],
      "metadata": {
        "id": "jzaUiWdekRcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 예제 3: 올바른 다국어 토크나이저 사용"
      ],
      "metadata": {
        "id": "h6wnNFbwkRxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 다국어 지원 토크나이저들 비교\n",
        "tokenizer_models = {\n",
        "    \"GPT-2 (영어 전용)\": \"gpt2\",\n",
        "    \"mBERT (다국어)\": \"bert-base-multilingual-cased\",\n",
        "    \"XLM-RoBERTa (다국어)\": \"xlm-roberta-base\"\n",
        "}\n",
        "\n",
        "korean_text = \"프로그래밍은 재미있다\"\n",
        "\n",
        "print(\"=== 다양한 토크나이저 비교 ===\")\n",
        "for model_name, model_id in tokenizer_models.items():\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        tokens = tokenizer.tokenize(korean_text)\n",
        "\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  토큰 수: {len(tokens)}\")\n",
        "        print(f\"  토큰들: {tokens}\")\n",
        "\n",
        "        # 역변환 확인\n",
        "        reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
        "        print(f\"  역변환: '{reconstructed}'\")\n",
        "        print(f\"  원본과 동일: {'✅' if reconstructed.strip() == korean_text else '❌'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{model_name}: 로드 실패 - {e}\")"
      ],
      "metadata": {
        "id": "tra_GeXrkR5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 해결 방법과 권장사항\n"
      ],
      "metadata": {
        "id": "aHPCJqxdlQjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 처리를 위한 올바른 선택\n",
        "\n",
        "|용도 |권장 모델 |이유\n",
        "|--- |--- |--- |\n",
        "| 한글 텍스트 생성| GPT-3.5/4, KoGPT| 한글 데이터로 훈련됨|\n",
        "| 한글 이해/분류| KoBERT, KoELECTRA| 한국어 특화 모델|\n",
        "| 다국어 처리| mBERT, XLM-RoBERTa| 다국어 동시 지원|\n",
        "| 실습/학습용| 영어 예제 사용| GPT-2 본래 성능 확인|"
      ],
      "metadata": {
        "id": "P0_mx4WXldNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습용 개선된 예제"
      ],
      "metadata": {
        "id": "Aj62TQvZl7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 올바른 접근: 영어로 실습하기\n",
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# GPT-2가 잘 처리하는 영어 텍스트\n",
        "english_text = \"Programming is fun. Programming is\"\n",
        "print(f\"영어 텍스트: '{english_text}'\")\n",
        "\n",
        "tokens = tokenizer.tokenize(english_text)\n",
        "print(f\"토큰들: {tokens}\")\n",
        "print(f\"토큰 수: {len(tokens)}\")\n",
        "\n",
        "# 각 토큰의 의미 확인\n",
        "print(\"\\n토큰별 의미:\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{i+1}. '{token}' → 의미있는 단위\")"
      ],
      "metadata": {
        "id": "0TeAXjxwnUQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IeP3BFVJf-00"
      }
    }
  ]
}